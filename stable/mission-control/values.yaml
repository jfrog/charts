# Default values for Mission Control.
# This is a YAML-formatted file.
# Beware when changing values here. You should know what you are doing!
# Access the values with {{ .Values.key.subkey }}

global:
  # imageRegistry:
  # imagePullSecrets:
  #   - myRegistryKeySecretName
  ## Chart.AppVersion can be overidden using global.versions.missionControl or image tags
  ## Note: Order of preference is 1) global.versions 2) image tags 3) Chart.AppVersion
  versions: {}
    # missionControl:
    # router:
  # jfrogUrl:
  # joinKey:
  # masterKey:
  # joinKeySecretName:
  # masterKeySecretName:
  # customVolumes: |

  # customVolumeMounts: |

  # customInitContainersBegin: |

  # customInitContainers: |

  # customSidecarContainers: |

  customCertificates:
    enabled: false
    # certificateSecretName:

## Partially override mission-control.fullname template (eg: <releaseName>-<nameOverride>)
# nameOverride:

## Fully override mission-control.fullname template
# fullnameOverride:

initContainerImage: releases-docker.jfrog.io/alpine:3.14.0

# For supporting pulling from private registries
imagePullSecrets:
  # - myRegistryKeySecretName

## Mission-control systemYaml override
## This is for advanced usecases where users wants to provide their own systemYaml for configuring mission-control
## Refer - https://www.jfrog.com/confluence/display/JFROG/Mission+Control+System+YAML
## Note: This will override existing (default) .Values.missionControl.systemYaml in values.yaml
## Alternatively, systemYaml can be overidden via customInitContainers using external sources like vaults, external repositories etc. Please refer customInitContainer section below for an example.
## Note: Order of preference is 1) customInitContainers 2) systemYamlOverride existingSecret 3) default systemYaml in values.yaml
systemYamlOverride:
## You can use a pre-existing secret by specifying existingSecret
  existingSecret:
## The dataKey should be the name of the secret data key created.
  dataKey:

# Init containers
initContainers:
  resources: {}
#    requests:
#      memory: "64Mi"
#      cpu: "10m"
#    limits:
#      memory: "128Mi"
#      cpu: "250m"

imagePullPolicy: IfNotPresent

# For HA
replicaCount: 1

shared:
  node:
    id:

common:
  uid: 1050
  gid: 1050

  ## Add custom volumes
  customVolumes: |
  #  - name: custom-script
  #    configMap:
  #      name: custom-script

  ## Add custom volumesMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: /scripts/script.sh
  #    subPath: script.sh

  # Add any list of configmaps
  configMaps: |
  #  posthook-start.sh: |-
  #    echo "This is a post start script"
  #  posthook-end.sh: |-
  #    echo "This is a post end script"

  ## Add custom init containers execution before predefined init containers
  customInitContainersBegin: |
  #  - name: "custom-setup"
  #    image: "{{ .Values.initContainerImage }}"
  #    imagePullPolicy: "{{ .Values.imagePullPolicy }}"
  #    command:
  #      - 'sh'
  #      - '-c'
  #      - 'touch {{ .Values.missionControl.persistence.mountPath }}/example-custom-setup'
  #    volumeMounts:
  #      - mountPath: "{{ .Values.missionControl.persistence.mountPath }}"
  #        name: data-volume

  ## Add custom init containers execution after predefined init containers
  customInitContainers: |
  #  - name: "custom-systemyaml-setup"
  #    image: "{{ .Values.initContainerImage }}"
  #    imagePullPolicy: "{{ .Values.imagePullPolicy }}"
  #    command:
  #      - 'sh'
  #      - '-c'
  #      - 'wget -O {{ .Values.missionControl.persistence.mountPath }}/etc/system.yaml https://<repo-url>/systemyaml'
  #    volumeMounts:
  #      - mountPath: "{{ .Values.missionControl.persistence.mountPath }}"
  #        name: data-volume

  ## Add custom sidecar containers
  # - The provided example uses a custom volume (customVolumes)
  customSidecarContainers: |
  #  - name: "sidecar-list-etc"
  #    image: "{{ .Values.initContainerImage }}"
  #    imagePullPolicy: "{{ .Values.imagePullPolicy }}"
  #    securityContext:
  #      allowPrivilegeEscalation: false
  #    command:
  #      - 'sh'
  #      - '-c'
  #      - 'sh /scripts/script.sh'
  #    volumeMounts:
  #      - mountPath: "{{ .Values.xray.persistence.mountPath }}"
  #        name: volume
  #      - mountPath: "/scripts/script.sh"
  #        name: custom-script
  #        subPath: script.sh
  #    resources:
  #      requests:
  #        memory: "32Mi"
  #        cpu: "50m"
  #      limits:
  #        memory: "128Mi"
  #        cpu: "100m"

  # Add custom secrets - secret per file
  customSecrets:
  #  - name: custom-secret
  #    key: custom-secret.yaml
  #    data: >
  #      custom_secret_config:
  #        parameter1: value1
  #        parameter2: value2
  #  - name: custom-secret2
  #    key: custom-secret2.config
  #    data: |
  #      here the custom secret 2 config

## Role Based Access Control
## Ref: https://kubernetes.io/docs/admin/authorization/rbac/
rbac:
  create: true
  role:
    ## Rules to create. It follows the role specification
    rules:
    - apiGroups:
      - ''
      resources:
      - services
      - endpoints
      - pods
      verbs:
      - get
      - watch
      - list

networkpolicy:
  # Allows all ingress and egress
  - name: mission-control
    podSelector:
      matchLabels:
        app: mission-control
    egress:
      - {}
    ingress:
      - {}
  # Uncomment to allow only mission-control pods to communicate with postgresql (if postgresql.enabled is true)
  # - name: postgres
  #   podSelector:
  #     matchLabels:
  #       app: postgresql
  #   ingress:
  #   - from:
  #     - podSelector:
  #         matchLabels:
  #           app: mission-control

## Service Account
## Ref: https://kubernetes.io/docs/admin/service-accounts-admin/
##
serviceAccount:
  create: true
  ## The name of the ServiceAccount to use.
  ## If not set and create is true, a name is generated using the fullname template
  name:

## Details required for initialization/setup of database
dbSetup:
  postgresql:
    image:
      repository: releases-docker.jfrog.io/postgres
      tag: 13.2-alpine
      pullPolicy: IfNotPresent

# PostgreSQL

## Configuration values for the PostgreSQL dependency sub-chart
## ref: https://github.com/kubernetes/charts/blob/master/stable/postgresql/README.md
postgresql:
  enabled: true
  image:
    registry: releases-docker.jfrog.io
    repository: bitnami/postgresql
    tag: 13.2.0-debian-10-r55
  postgresqlUsername: postgres
  postgresqlPassword: ""
  postgresqlDatabase: mission_control
  postgresqlExtendedConf:
    listenAddresses: "*"
    maxConnections: "1500"
  db:
    name: mission_control
    sslmode: "false"
    tablespace: "pg_default"
    ## Use single user and password for all the services
    user: mc
    password: ""
    jfmcSchema: jfmc_server
    jfisSchema: insight_server
    jfscSchema: insight_scheduler
  service:
    port: 5432
  persistence:
    enabled: true
    size: 100Gi
    # existingClaim:
  primary:
    nodeSelector: {}
    affinity: {}
    tolerations: []
  readReplicas:
    nodeSelector: {}
    affinity: {}
    tolerations: []
  resources: {}
  #  requests:
  #    memory: "1Gi"
  #    cpu: "250m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "1"

### If NOT using the PostgreSQL in this chart (postgresql.enabled=false),
## specify custom database details here or leave empty
database:
  type: postgresql
  driver: org.postgresql.Driver
  ## Please make sure these are created under the provided database
  name: mission_control
  jfisSchema: insight_server
  jfmcSchema: jfmc_server
  jfscSchema: insight_scheduler
  ## Use single user and password for all the services
  user:
  password:
  url:
  ## If you have existing Kubernetes secrets containing db credentials, use
  ## these values
  secrets: {}
  #  user:
  #    name: "mc-database-creds"
  #    key: "db-user"
  #  password:
  #    name: "mc-database-creds"
  #    key: "db-password"
  #  url:
  #    name: "mc-database-creds"
  #    key: "db-url"


elasticsearch:
  enabled: true
  ## certificatesSecretName:
  name: elasticsearch
  initContainerImage: releases-docker.jfrog.io/alpine:3.14.0
  configureDockerHost: true
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/elasticsearch-sg
    tag: 7.13.2
    pullPolicy: IfNotPresent
  ## Enter elasticsearch connection details
  ## By default url is set to localhost:8082 (router)
  ## If external elasticsearch is used, provide external elasticsearch url and set elasticsearch.enabled=false
  url: '{{ include "elasticsearch.url" . }}'
  transportPort: 9300
  username: "admin"
  password: "admin"
  env:
    clusterName: "es-cluster"
    networkHost: "0.0.0.0"
    transportHost: "0.0.0.0"
    maxMapCount: 262144

  persistence:
    enabled: true
    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    # existingClaim:

    mountPath: "/usr/share/elasticsearch/data"
    accessMode: ReadWriteOnce
    size: 100Gi
    ## ElasticSearch data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

  ## ElasticSearch xms and xmx should be same!
  javaOpts: {}
  #  xms: "2g"
  #  xmx: "2g"

  resources: {}
  #  requests:
  #    memory: "2Gi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2.5Gi"
  #    cpu: "500m"

  livenessProbe:
    enabled: true
    config: |
      httpGet:
        path: /_cluster/health
        port: 9200
      initialDelaySeconds: {{ if semverCompare "<v1.18.0-0" .Capabilities.KubeVersion.Version }}90{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: 10
      failureThreshold: 3
      successThreshold: 1

  readinessProbe:
    enabled: true
    config: |
      httpGet:
        path: /_cluster/health
        port: 9200
      initialDelaySeconds: {{ if semverCompare "<v1.18.0-0" .Capabilities.KubeVersion.Version }}60{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: 10
      failureThreshold: 10
      successThreshold: 1

  startupProbe:
    enabled: true
    config: |
      httpGet:
        path: /_cluster/health
        port: 9200
      initialDelaySeconds: 30
      failureThreshold: 60
      periodSeconds: 5

podRestartTime:

applicationConfigs: {}

logger:
  image:
    registry: releases-docker.jfrog.io
    repository: busybox
    tag: 1.32.1

missionControl:
  name: mission-control
  ## Note that by default we use appVersion to get image tag/version
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/mission-control
    # tag:
  appContext: mc
  appName: jfmc-server
  home: /var/opt/jfrog/mc
  labels: {}

  # env:

  # Set this to true if you want to override credentials in mission-control.properties on startup
  propertyOverride: true

  ## Add any of the loggers to a sidecar if you want to be able to see them with kubectl logs or a log collector in your k8s cluster
  loggers: []
  # - audit.log
  # - http.log
  # - init-debug.log
  # - init-err.log
  # - init.log
  # - mc-debug.log
  # - mc-err.log
  # - mc.log
  # - monitoring.log

  # Loggers containers resources
  loggersResources: {}
  #  requests:
  #    memory: "64Mi"
  #    cpu: "25m"
  #  limits:
  #    memory: "128Mi"
  #    cpu: "50m"

  ## Mission Control requires a unique master.key
  # This will be used to encript sensitive data to be stored in database
  ## You can generate one with the command:
  ## 'openssl rand -hex 32'
  ## Pass it to helm with '--set missionControl.masterKey=${MASTER_KEY}'
  ## IMPORTANT: You should NOT use the example masterKey for a production deployment!
  masterKey: bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb
  ## Alternatively, you can use a pre-existing secret with a key called master-key by specifying masterKeySecretName
  # masterKeySecretName:

  joinKey: EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE
  ## Alternatively, you can use a pre-existing secret with a key called join-key by specifying joinKeySecretName
  # joinKeySecretName:
  ## If false, all service console logs will not redirect to a common console.log
  consoleLog: false
  ## Artifactory URL . Mandatory
  jfrogUrl:

  customCertificates:
    enabled: false
    # certificateSecretName:

  podRestartTime:
  repository: jfrog-mission-control
  package: mc-docker-installer
  dist: helm
  osVersion: "NA"
  osType: "NA"
  osDist: "NA"

  systemYaml: |
    router:
      topology:
        local:
          {{- if .Values.elasticsearch.enabled }}
          requiredServiceTypes: 'jfesc,jfisc,jfisv,jfmc'
          {{ else }}
          requiredServiceTypes: 'jfisc,jfisv,jfmc'
          {{- end }}
    {{- if .Values.elasticsearch.enabled }}
    elasticsearch:
      app:
        version: {{ .Values.elasticsearch.image.tag }}
    {{- end}}
    shared:
      logging:
        consoleLog:
          enabled: {{ .Values.missionControl.consoleLog }}
      jfrogUrl: "{{ tpl (required "\n\nmissionControl.jfrogUrl or global.jfrogUrl is required! This allows to connect to Artifactory.\nYou can copy the JFrog URL from Admin > Security > Settings" (include "mission-control.jfrogUrl" .)) . }}"
      elasticsearch:
        username: {{ .Values.elasticsearch.username }}
        password: {{ .Values.elasticsearch.password }}
      {{- if and .Values.elasticsearch.enabled  (gt (int .Values.replicaCount) 1)  }}
        clusterSetup: "yes"
      {{- end }}
      {{- if not .Values.elasticsearch.enabled }}
        external: true
        url: {{ tpl .Values.elasticsearch.url . }}
      {{- end }}
      database:
      {{- if .Values.postgresql.enabled }}
        type: postgresql
        driver: org.postgresql.Driver
        url: jdbc:postgresql://{{ .Release.Name }}-postgresql:{{ .Values.postgresql.service.port }}/{{ .Values.postgresql.db.name }}?sslmode=disable
      {{ else }}
        type: "{{ .Values.database.type }}"
        driver: "{{ .Values.database.driver }}"
      {{- end }}

    mc:
    {{- if .Values.postgresql.enabled }}
      database:
        username: {{ .Values.postgresql.db.user }}
        schema: {{ .Values.postgresql.db.jfmcSchema }}
    {{- end }}
      extraJavaOpts: >
      {{- with .Values.missionControl.javaOpts }}
      {{- if .xms }}
        -Xms{{ .xms }}
      {{- end }}
      {{- if .xmx }}
        -Xmx{{ .xmx }}
      {{- end }}
      {{- if .other }}
        {{ .other }}
      {{- end }}
      {{- end }}

    insight-scheduler:
    {{- if .Values.postgresql.enabled }}
      database:
        username: {{ .Values.postgresql.db.user }}
        schema: {{ .Values.postgresql.db.jfscSchema }}
    {{- end }}
      extraJavaOpts: >
      {{- with .Values.insightScheduler.javaOpts }}
      {{- if .xms }}
        -Xms{{ .xms }}
      {{- end }}
      {{- if .xmx }}
        -Xmx{{ .xmx }}
      {{- end }}
      {{- if .other }}
        {{ .other }}
      {{- end }}
      {{- end }}

    insight-server:
    {{- if .Values.postgresql.enabled }}
      database:
        username: {{ .Values.postgresql.db.user }}
        schema: {{ .Values.postgresql.db.jfisSchema }}
    {{- end }}
      clients:
        elasticsearch:
          connectionWaitTimeoutSecs: {{ .Values.insightServer.clients.elasticsearch.connectionWaitTimeoutSecs }}
          searchguard:
            connectionWaitTimeoutSecs: {{ .Values.insightServer.clients.elasticsearch.searchguard.connectionWaitTimeoutSecs }}

  persistence:
    enabled: true
    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    # existingClaim:

    mountPath: "/var/opt/jfrog/mc"
    accessMode: ReadWriteOnce
    size: 100Gi
    ## Mission Control data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

  ## Control Java options (JAVA_OPTIONS)
  ## IMPORTANT: keep resources.limits.memory higher than javaOpts.xmx by 0.5G
  javaOpts: {}
  #  other: "-server -XX:+UseG1GC -Dfile.encoding=UTF8"
  #  xms: "2g"
  #  xmx: "3g"
  resources: {}
  #  requests:
  #    memory: "2Gi"
  #    cpu: "100m"
  #  limits:
  #    memory: "3.5Gi"
  #    cpu: "1"
  nodeSelector: {}

  tolerations: []

  affinity: {}

  service:
    type: ClusterIP
    annotations: {}
  internalPort: 8080
  externalPort: 80

  livenessProbe:
    enabled: true
    config: |
      exec:
        command:
        - curl
        - http://127.0.0.1:{{ .Values.missionControl.internalPort }}/api/v1/system/ping
      initialDelaySeconds: {{ if semverCompare "<v1.18.0-0" .Capabilities.KubeVersion.Version }}90{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
      successThreshold: 1

  readinessProbe:
    enabled: true
    config: |
      exec:
        command:
        - curl
        - http://127.0.0.1:{{ .Values.missionControl.internalPort }}/api/v1/system/ping
      initialDelaySeconds: {{ if semverCompare "<v1.18.0-0" .Capabilities.KubeVersion.Version }}60{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 10
      successThreshold: 1

  startupProbe:
    enabled: true
    config: |
      exec:
        command:
        - curl
        - http://127.0.0.1:{{ .Values.missionControl.internalPort }}/api/v1/system/ping
      initialDelaySeconds: 30
      failureThreshold: 60
      periodSeconds: 5
      timeoutSeconds: 1

  ## Extra pre start command to run extra commands before container starts
  # preStartCommand:
  ## For example, to import trusted keystore ( Need to mount the certificates first )
  # preStartCommand: "/java/jdk-11.0.2+9/bin/keytool -importcert -keystore /java/jdk-11.0.2+9/lib/security/cacerts -storepass changeit -file /tmp/trusted-certificates/root.crt -alias 'newcerts'"

insightServer:
  name: insight-server
  ## Note that by default we use appVersion to get image tag/version
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/insight-server
    # tag:
  appContext: insight
  home: /opt/jfrog
  internalPort: 8087
  ## Timeout in seconds (default 60 seconds) for which insightServer waits for elasticsearch to comeup.
  clients:
    elasticsearch:
      connectionWaitTimeoutSecs: 180
      searchguard:
        connectionWaitTimeoutSecs: 1800
  ## This can be used to whitelist the range of IPs allowed to be served by Insight Server service
  ## The value must follow CIDR format
  allowIP: "0.0.0.0/0"
  resources: {}
  #  requests:
  #    memory: "500Mi"
  #    cpu: "100m"
  #  limits:
  #    memory: "1Gi"
  #    cpu: "1"

  # Persistence for the logs
  persistence:
    enabled: false
    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    # existingClaim:

    mountPath: "/var/opt/jfrog/mc"
    accessMode: ReadWriteOnce
    size: 10Gi
    ## Insight Server logs Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

  ## Add any of the loggers to a sidecar if you want to be able to see them with kubectl logs or a log collector in your k8s cluster
  loggers: []
  # - insight-server.log

  # Loggers containers resources
  loggersResources: {}
  #  requests:
  #    memory: "64Mi"
  #    cpu: "25m"
  #  limits:
  #    memory: "128Mi"
  #    cpu: "50m"

  livenessProbe:
    enabled: true
    config: |
      exec:
        command:
        - curl
        - http://127.0.0.1:{{ .Values.insightServer.internalPort }}/api/v1/system/ping
      initialDelaySeconds: {{ if semverCompare "<v1.18.0-0" .Capabilities.KubeVersion.Version }}90{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
      successThreshold: 1

  readinessProbe:
    enabled: true
    config: |
      exec:
        command:
        - curl
        - http://127.0.0.1:{{ .Values.insightServer.internalPort }}/api/v1/system/ping
      initialDelaySeconds: {{ if semverCompare "<v1.18.0-0" .Capabilities.KubeVersion.Version }}60{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 10
      successThreshold: 1


  startupProbe:
    enabled: true
    config: |
      exec:
        command:
        - curl
        - http://127.0.0.1:{{ .Values.insightServer.internalPort }}/api/v1/system/ping
      initialDelaySeconds: 30
      failureThreshold: 60
      periodSeconds: 5
      timeoutSeconds: 1

insightScheduler:
  name: insight-scheduler
  ## Note that by default we use appVersion to get image tag/version
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/insight-scheduler
    # tag:
  appContext: insight-scheduler
  home: /opt/jfrog
  internalPort: 8085

  ## Control Java options (JFMC_EXTRA_JAVA_OPTS)
  ## IMPORTANT: keep resources.limits.memory higher than javaOpts.xmx by 0.5G
  javaOpts: {}
  #  other:
  #  xms: "500m"
  #  xmx: "1g"
  resources: {}
  #  requests:
  #    memory: "500Mi"
  #    cpu: "100m"
  #  limits:
  #    memory: "1.5Gi"
  #    cpu: "1"

  # Persistence for the logs
  persistence:
    enabled: false
    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    # existingClaim:

    mountPath: "/var/opt/jfrog/mc"
    accessMode: ReadWriteOnce
    size: 10Gi
    ## Insight Scheduler logs Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

  ## Add any of the loggers to a sidecar if you want to be able to see them with kubectl logs or a log collector in your k8s cluster
  loggers: []
  # - insight-scheduler.log
  # - access.log

  # Loggers containers resources
  loggersResources: {}
  #  requests:
  #    memory: "64Mi"
  #    cpu: "25m"
  #  limits:
  #    memory: "128Mi"
  #    cpu: "50m"

  livenessProbe:
    enabled: true
    config: |
      exec:
        command:
        - curl
        - http://127.0.0.1:{{ .Values.insightScheduler.internalPort }}/api/v1/system/ping
      initialDelaySeconds: {{ if semverCompare "<v1.18.0-0" .Capabilities.KubeVersion.Version }}90{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
      successThreshold: 1

  readinessProbe:
    enabled: true
    config: |
      exec:
        command:
        - curl
        - http://127.0.0.1:{{ .Values.insightScheduler.internalPort }}/api/v1/system/ping
      initialDelaySeconds: {{ if semverCompare "<v1.18.0-0" .Capabilities.KubeVersion.Version }}60{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 10
      successThreshold: 1


  startupProbe:
    enabled: true
    config: |
      exec:
        command:
        - curl
        - http://127.0.0.1:{{ .Values.insightScheduler.internalPort }}/api/v1/system/ping
      initialDelaySeconds: 30
      failureThreshold: 60
      periodSeconds: 5
      timeoutSeconds: 1

router:
  name: router
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/router
    tag: 7.21.5
    imagePullPolicy: IfNotPresent
  internalPort: 8082
  externalPort: 8082
  tlsEnabled: false
  resources: {}
  #  requests:
  #    memory: "100Mi"
  #    cpu: "100m"
  #  limits:
  #    memory: "1Gi"
  #    cpu: "1"

  livenessProbe:
    enabled: true
    config: |
      httpGet:
        path: /router/api/v1/system/health
        scheme: {{ include "mission-control.scheme" . | upper }}
        port: {{ .Values.router.internalPort }}
      initialDelaySeconds: {{ if semverCompare "<v1.18.0-0" .Capabilities.KubeVersion.Version }}90{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: 10
      failureThreshold: 3
      successThreshold: 1

  readinessProbe:
    enabled: true
    config: |
      httpGet:
        path: /router/api/v1/system/health
        scheme: {{ include "mission-control.scheme" . | upper }}
        port: {{ .Values.router.internalPort }}
      initialDelaySeconds: {{ if semverCompare "<v1.18.0-0" .Capabilities.KubeVersion.Version }}60{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: 10
      failureThreshold: 10
      successThreshold: 1

  startupProbe:
    enabled: true
    config: |
      httpGet:
        path: /router/api/v1/system/health
        scheme: {{ include "mission-control.scheme" . | upper }}
        port: {{ .Values.router.internalPort }}
      initialDelaySeconds: 30
      failureThreshold: 60
      periodSeconds: 5
      timeoutSeconds: 1

  persistence:
    accessMode: ReadWriteOnce
    mountPath: "/var/opt/jfrog/router"
    size: 5Gi

  # Add any of the loggers to a sidecar if you want to be able to see them with kubectl logs or a log collector in your k8s cluster
  loggers: []

  nodeSelector: {}
  tolerations: []
  affinity: {}

# Filebeat Sidecar container
## The provided filebeat configuration is for Mission-Control logs. It assumes you have a logstash installed and configured properly.
filebeat:
  enabled: false
  name: mission-control-filebeat
  image:
    repository: "docker.elastic.co/beats/filebeat"
    version: 7.9.2
  logstashUrl: "logstash:5044"

  terminationGracePeriod: 10

  livenessProbe:
    exec:
      command:
        - sh
        - -c
        - |
          #!/usr/bin/env bash -e
          curl --fail 127.0.0.1:5066
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5

  readinessProbe:
    exec:
      command:
        - sh
        - -c
        - |
          #!/usr/bin/env bash -e
          filebeat test output
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5

  resources: {}
#    requests:
#      memory: "100Mi"
#      cpu: "100m"
#    limits:
#      memory: "100Mi"
#      cpu: "100m"

  filebeatYml: |
    logging.level: info
    path.data: {{ .Values.missionControl.persistence.mountPath }}/log/filebeat
    name: missionControl-filebeat
    queue.spool: ~
    filebeat.inputs:
    - type: log
      enabled: true
      close_eof: ${CLOSE:false}
      paths:
         - {{ .Values.missionControl.persistence.mountPath }}/log/*.log
      fields:
        service: "jfmc"
        log_type: "missionControl"
    output:
      logstash:
         hosts: ["{{ .Values.filebeat.logstashUrl }}"]

## Allows to add additional kubernetes resources
## Use --- as a separator between multiple resources
additionalResources: |

# Adding entries to a Pod's /etc/hosts file
# For an example, refer - https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases
hostAliases: []
#  - ip: "127.0.0.1"
#    hostnames:
#      - "foo.local"
#      - "bar.local"
#  - ip: "10.1.2.3"
#    hostnames:
#      - "foo.remote"
#      - "bar.remote"
