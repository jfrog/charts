# JFrog Mission-Control Helm Chart

## Prerequisites Details

* Kubernetes 1.12+

## Chart Details
This chart will do the following:

* Deploy PostgreSQL database **NOTE:** For production grade installations it is recommended to use an external PostgreSQL.
* Deploy Elasticsearch.
* Deploy Mission Control.

## Requirements
- A running Kubernetes cluster
- Dynamic storage provisioning enabled
- Default StorageClass set to allow services using the default StorageClass for persistent storage
- A running Artifactory Enterprise
- [Kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) installed and setup to use the cluster
- [Helm](https://helm.sh/) installed and setup to use the cluster (helm init)

### Install Chart

### Add ChartCenter Helm repository

Before installing JFrog helm charts, you need to add the [ChartCenter helm repository](https://chartcenter.io) to your helm client.

```bash
helm repo add center https://repo.chartcenter.io
helm repo update
```

**NOTE:** Check [CHANGELOG.md] for version specific install notes.

#### Special Notes

Mission Control version 4.3.2 is compatible with Artifactory 7.4.1 and above. Refer Mission Control release notes for more details - https://www.jfrog.com/confluence/display/JFROG/Mission+Control+Release+Notes#MissionControlReleaseNotes-MissionControl4.3.2.

#### Artifactory Connection Details
In order to connect Mission Control to your Artifactory installation, you have to use a Join Key, hence it is *MANDATORY* to provide a Join Key and Jfrog Url to your Mission Control installation. Here's how you do that:

Retrieve the connection details of your Artifactory installation, from the UI - https://www.jfrog.com/confluence/display/JFROG/General+Security+Settings#GeneralSecuritySettings-ViewingtheJoinKey. 

#### Initiate Installation
Provide join key and jfrog url as a parameter to the Mission Control chart installation:

```bash
helm upgrade --install mission-control --set missionControl.joinKey=<YOUR_PREVIOUSLY_RETIREVED_JOIN_KEY> \
             --set missionControl.jfrogUrl=<YOUR_PREVIOUSLY_RETIREVED_BASE_URL> --namespace mission-control center/jfrog/mission-control
```
Alternatively, you can create a secret containing the join key manually and pass it to the template at install/upgrade time.
```bash

# Create a secret containing the key. The key in the secret must be named join-key
kubectl create secret generic my-secret --from-literal=join-key=<YOUR_PREVIOUSLY_RETIREVED_JOIN_KEY>

# Pass the created secret to helm
helm upgrade --install mission-control --set missionControl.joinKeySecretName=my-secret --namespace mission-control center/jfrog/mission-control
```
**NOTE:** In either case, make sure to pass the same join key on all future calls to `helm install` and `helm upgrade`! This means always passing `--set missionControl.joinKey=<YOUR_PREVIOUSLY_RETIREVED_JOIN_KEY>`. In the second, this means always passing `--set missionControl.joinKeySecretName=my-secret` and ensuring the contents of the secret remain unchanged.

### Special Upgrade Notes
Mission-control 3.x to 4.x (App Version) upgrade is not currently supported. For manual upgrade, please refer [here](https://github.com/jfrog/charts/blob/master/stable/mission-control/UPGRADE_NOTES.md). If this is an upgrade over an existing Mission Control 4.x, explicitly pass `--set unifiedUpgradeAllowed=true` to upgrade.

### System Configuration
Mission Control uses a common system configuration file - `system.yaml`. See [official documentation](https://www.jfrog.com/confluence/display/JFROG/System+YAML+Configuration+File) on its usage.

### Auto generated passwords

This section is applicable only for deployments with internal postgreSQL.

Internal postgreSQL needs 1 variable to be available on install or upgrade. If it is not set by user, a random 10 character alphanumeric string will be set for the same. It is recommended for the user to set this explicitly during install and upgrade.
```bash
...
--set postgresql.postgresqlPassword=<value> \
...
```
The values should remain same between upgrades.

If this was autogenerated during `helm install`, the same password will have to be passed on future upgrades.

Following can be used to read current set password,(refer [decoding-a-secret](https://kubernetes.io/docs/concepts/configuration/secret/#decoding-a-secret) for more info on reading a sceret value)

POSTGRES_PASSWORD=$(kubectl get secret -n <namespace> <release_name>-postgresql -o jsonpath="{.data.postgresql-password}" | base64 --decode)

Following parameter can be set during upgrade,
```bash
...
--set postgresql.postgresqlPassword=${POSTGRES_PASSWORD} \
...
```

### Deploying JFMC for small/medium/large instllations
In the chart directory, we have added three values files, one for each installation type - small/medium/large. These values files are recommendations for setting resources requests and limits for your installation. The values are derived from the following [documentation](https://www.jfrog.com/confluence/display/EP/Installing+on+Kubernetes#InstallingonKubernetes-Systemrequirements). You can find them in the corresponding chart directory -  values-small.yaml, values-medium.yaml and values-large.yaml

### Create a unique Master Key
Mission Control HA cluster uses a unique master key. By default the chart has one set in values.yaml (`missionControl.masterKey`).

**This key is for demo purpose and should not be used in a production environment!**

You should generate a unique one and pass it to the template at install/upgrade time.
```bash
# Create a key
export MASTER_KEY=$(openssl rand -hex 32)
echo ${MASTER_KEY}

# Pass the created master key to helm
helm upgrade --install mission-control --set missionControl.masterKey=${MASTER_KEY} --namespace mission-control center/jfrog/mission-control
```

Alternatively, you can create a secret containing the master key manually and pass it to the template at install/upgrade time.
```bash

# Create a secret containing the key. The key in the secret must be named master-key
kubectl create secret generic my-secret --from-literal=master-key=${MASTER_KEY}

# Pass the created secret to helm
helm upgrade --install mission-control --namespace mission-control --set missionControl.masterKeySecretName=my-secret center/jfrog/mission-control
```
**NOTE:** In either case, make sure to pass the same master key on all future calls to `helm install` and `helm upgrade`! In the first case, this means always passing `--set missionControl.masterKey=${MASTER_KEY}`. In the second, this means always passing `--set missionControl.masterKeySecretName=my-secret` and ensuring the contents of the secret remain unchanged.


## Upgrade
Once you have a new chart version, you can update your deployment with
```
helm upgrade mission-control center/jfrog/mission-control
```

**NOTE:** Check for any version specific upgrade notes in [CHANGELOG.md]

### Non compatible upgrades
In cases where a new version is not compatible with existing deployed version (look in CHANGELOG.md) you should
* Deploy new version along side old version (set a new release name)
* Copy configurations and data from old deployment to new one (The following instructions were tested for chart migration from 0.9.4 (3.4.3) to 1.0.0 (3.5.0))
  * Copy data and config from old deployment to local filesystem
    ```
    kubectl cp <elasticsearch-pod>:/usr/share/elasticsearch/data                                   /<local_disk_path>/mission-control-data/elastic_data               -n <old_namespace>
    kubectl cp <postgres-pod>:/var/lib/postgresql/data                                             /<local_disk_path>/mission-control-data/postgres_data              -n <old_namespace>
    kubectl cp <mission-control-pod>:/var/opt/jfrog/mission-control/etc/mission-control.properties /<local_disk_path>/mission-control-data/mission-control.properties -n <old_namespace> -c mission-control
    kubectl cp <mission-control-pod>:/var/opt/jfrog/mission-control/data/security/mc.key           /<local_disk_path>/mission-control-data/mc.key                     -n <old_namespace> -c mission-control
    ```
  * This point applies only if you have used autogenerated password for postgres in your previous deploy or in your new deployement.
    * Get the postgres password from previous deploy, (refer [decoding-a-secret](https://kubernetes.io/docs/concepts/configuration/secret/#decoding-a-secret) for more info on reading a sceret value)
      ```
      POSTGRES_PASSWORD=$(kubectl get secret -n <old_namespace> <old_release_name>-postgresql -o jsonpath="{.data.postgresql-password}" | base64 --decode)
      ```
      **NOTE** This needs to be passed with every `helm --set postgresql.postgresqlPassword=${POSTGRES_PASSWORD} install` and `helm --set postgresql.postgresqlPassword=${POSTGRES_PASSWORD} upgrade` 
  * Copy data and config from local filesystem to new deployment
    ```bash
    kubectl cp /<local_disk_path>/mission-control-data/mc.key                     <mission-control-pod>:/var/opt/jfrog/mission-control/data/security/mc.key            -n <new_namespace> -c mission-control
    # Note : This mission-control.properties has to be copied to all the replicas if you plan to scale to more replicas in future
    kubectl cp /<local_disk_path>/mission-control-data/mission-control.properties <mission-control-pod>:/var/opt/jfrog/mission-control/etc/mission-control.properties  -n <new_namespace> -c mission-control
    kubectl cp /<local_disk_path>/mission-control-data/elastic_data               <mission-control-pod>:/usr/share/elasticsearch                                       -n <new_namespace> -c elasticsearch
    kubectl cp /<local_disk_path>/mission-control-data/postgres_data              <postgres-pod>:/var/lib/postgresql                                                   -n <new_namespace>

    kubectl exec -it <postgres-pod> -n <new_namespace> -- bash
        rm -fr /var/lib/postgresql/data
        cp -fr /var/lib/postgresql/postgres_data/* /var/lib/postgresql/data/
        rm -fr /var/lib/postgresql/postgres_data
    kubectl exec -it <mission-control-pod> -n <new_namespace> -c elasticsearch -- bash
        rm -fr /usr/share/elasticsearch/data
        cp -fr /usr/share/elasticsearch/elastic_data/* /usr/share/elasticsearch/data
        rm -fr /usr/share/elasticsearch/elastic_data
    ```
* Restart the new deployment
  ```bash
  kubectl scale deployment <postgres-deployment> --replicas=0 -n <new_namespace>
  kubectl scale statefulset <mission-control-statefulset> --replicas=0 -n <new_namespace>

  kubectl scale deployment <postgres-deployment> --replicas=1 -n <new_namespace>
  kubectl scale statefulset <mission-control-statefulset> --replicas=1 -n <new_namespace>

  # if you are using autogenerated password for postgres, set the postgres password from previous deploy by running an upgrade
  # helm --set postgresql.postgresqlPassword=${POSTGRES_PASSWORD} upgrade ...
  ```
* A new mc.key will be generated after this upgrade, save a copy of this key. **NOTE**: This should be passed on all future calls to `helm install` and `helm upgrade`!
```bash
export MC_KEY=$(kubectl exec -it <mission-control-pod> -n <new_namespace> -c mission-control -- cat /var/opt/jfrog/mission-control/data/security/mc.key )
```
* Remove old release

### Use external Database

**For production grade installations it is recommended to use an external PostgreSQL with a static password**

#### PostgreSQL
There are cases where you will want to use an external **PostgreSQL** and not the enclosed **PostgreSQL**.
See more details on [configuring the database](https://www.jfrog.com/confluence/display/MC/Using+External+Databases#UsingExternalDatabases-ExternalizingPostgreSQL)

This can be done with the following parameters
```bash
...
--set postgresql.enabled=false \
--set database.url=${DB_URL} \
--set database.user=${DB_USER} \
--set database.password=${DB_PASSWORD} \
...
```
**NOTE:** You must set `postgresql.enabled=false` in order for the chart to use the `database.*` parameters. Without it, they will be ignored!

##### Use existing secrets for PostgreSQL connection details
You can use already existing secrets for managing the database connection details.

Pass them to the install command with the following parameters
```bash
export POSTGRES_USERNAME_SECRET_NAME=
export POSTGRES_USERNAME_SECRET_KEY=
export POSTGRES_PASSWORD_SECRET_NAME=
export POSTGRES_PASSWORD_SECRET_KEY=
...
    --set database.secrets.user.name=${POSTGRES_USERNAME_SECRET_NAME} \
    --set database.secrets.user.key=${POSTGRES_USERNAME_SECRET_KEY} \
    --set database.secrets.password.name=${POSTGRES_PASSWORD_SECRET_NAME} \
    --set database.secrets.password.key=${POSTGRES_PASSWORD_SECRET_KEY} \
...
```

#### Elasticsearch

By default, this HELM chart deploys elasticsearch pod. It also configures docker host kernel parameters
using a privileged initContainer. In some installations, you would not be allowed to run privileged 
containers, in which case you can disable docker host configuration by configuring following parameter:

```
--set elasticsearch.configureDockerHost=false
```

There are cases where you will want to use an external **Elasticsearch** and not the enclosed **Elasticsearch**.

This can be done with the following parameters
```bash
...
--set elasticsearch.enabled=false \
--set elasticsearch.url=${ES_URL} \
--set elasticsearch.username=${ES_USERNAME} \
--set elasticsearch.password=${ES_PASSWORD} \
...
```

### Logger sidecars
This chart provides the option to add sidecars to tail various logs from Mission Control containers. See the available values in `values.yaml`

Get list of containers in the pod
```bash
kubectl get pods -n <NAMESPACE> <POD_NAME> -o jsonpath='{.spec.containers[*].name}' | tr ' ' '\n'
```

View specific log
```bash
kubectl logs -n <NAMESPACE> <POD_NAME> -c <LOG_CONTAINER_NAME>
```

### Custom sidecar containers
There are cases where an extra sidecar container is needed. For example monitoring agents or log collection.

For this, there is a section for writing a custom sidecar container in the [values.yaml](values.yaml). By default it's commented out
```yaml
common:
  ## Add custom sidecar containers
  customSidecarContainers: |
    ## Sidecar containers template goes here ##
```

### Establishing TLS and Adding certificates
Create trust between the nodes by copying the ca.crt from the Artifactory server under $JFROG_HOME/artifactory/var/etc/access/keys to of the nodes you would like to set trust with under $JFROG_HOME/<product>/var/etc/security/keys/trusted. For more details, Please refer [here](https://www.jfrog.com/confluence/display/JFROG/Managing+TLS+Certificates).


To add this certificate to mission control, Create a configmaps.yaml file with the following content:

```yaml
common:
  configMaps: |
    ca.crt: |
      -----BEGIN CERTIFICATE-----
        <certificate content>
      -----END CERTIFICATE-----
  customVolumeMounts: |
    - name: mission-control-configmaps
      mountPath: /tmp/ca.crt
      subPath: ca.crt
missionControl:
  preStartCommand: "mkdir -p {{ .Values.missionControl.persistence.mountPath }}/etc/security/keys/trusted && cp -fv /tmp/ca.crt {{ .Values.missionControl.persistence.mountPath }}/etc/security/keys/trusted/ca.crt"
router:
  tlsEnabled: true  
```

and use it with your helm install/upgrade:
```bash
helm upgrade --install mission-control -f configmaps.yaml --namespace mission-control center/jfrog/xray
```

This will, in turn:
* Create a configMap with the files you specified above
* Create a volume pointing to the configMap with the name `mission-control-configmaps`
* Mount said configMap onto `/tmp` using a `customVolumeMounts`
* Using preStartCommand copy the `ca.crt` file to distribution trusted keys folder `/etc/security/keys/trusted/ca.crt`
* `router.tlsEnabled` is set to true to add HTTPS scheme in liveness and readiness probes.

### Custom volumes

If you need to use a custom volume, you can use this option.

For this, there is a section for defining custom volumes in the [values.yaml](values.yaml). By default it's commented out

```yaml
common:
  ## Add custom volumes
  customVolumes: |
    ## Custom volume comes here ##
```

### Custom init containers
There are cases where a special, unsupported init processes is needed like checking something on the file system or testing something before spinning up the main container.

For this, there is a section for writing a custom init container in the [values.yaml](values.yaml). By default it's commented out
```
common:
  ## Add custom init containers
  customInitContainers: |
    ## Init containers template goes here ##
```

### Custom volumes
There are also cases where you'd like custom files or for your init container to make changes to the file system the mission control container will see.

For this, there is a section for defining custom volumes in the [vaules.yaml](values.yaml).  By default they are left empty.
```
common:
  ## Add custom volumes
  customVolumes: |
  #  - name: custom-script
  #    configMap:
  #      name: custom-script

  ## Add custom volumeMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: "/scripts/script.sh"
  #    subPath: script.sh
```

### Custom secrets
If you need to add a custom secret in a custom init or any common container, you can use this option.

For this, there is a section for defining custom secrets in the [values.yaml](values.yaml). By default it's commented out
```yaml
common:
  # Add custom secrets - secret per file
    customSecrets:
      - name: custom-secret
        key: custom-secret.yaml
        data: >
          secret data
```

To use a custom secret, need to define a custom volume.
```yaml
common:
  ## Add custom volumes
  customVolumes: |
    - name: custom-secret
      secret:
        secretName: custom-secret
```

To use a volume, need to define a volume mount as part of a custom init or sidecar container.
```yaml
common:
  customVolumeMounts:
    - name: custom-secret
      mountPath: /opt/custom-secret.yaml
      subPath: custom-secret.yaml
      readOnly: true
```

## Configuration
The following table lists the configurable parameters of the mission-control chart and their default values.

|         Parameter                            |           Description                           |          Default                      |
|----------------------------------------------|-------------------------------------------------|---------------------------------------|
| `unifiedUpgradeAllowed`                      | Set this flag to `true` for unifiedupgrades     |                                       |
| `initContainerImage`                         | Init Container Image                            | `docker.bintray.io/alpine:3.12`       |
| `initContainers.resources.requests.memory`   | Init containers initial memory request          |                                       |
| `initContainers.resources.requests.cpu`      | Init containers initial cpu request             |                                       |
| `initContainers.resources.limits.memory`     | Init containers memory limit                    |                                       |
| `initContainers.resources.limits.cpu`        | Init containers cpu limit                       |                                       |
| `imagePullPolicy`                            | Container pull policy                           | `IfNotPresent`                        |
| `imagePullSecrets`                           | Docker registry pull secret                     |                                       |
| `replicaCount`                               | Number of replicas                              | `1`                                   |
| `serviceAccount.create`                      | Specifies whether a ServiceAccount should be created | `true`                           |
| `serviceAccount.name`                        | The name of the ServiceAccount to create        | Generated using the fullname template |
| `rbac.create`                                | Specifies whether RBAC resources should be created   | `true`                           |
| `rbac.role.rules`                            | Rules to create                                 | `[]`                                  |
| `postgresql.enabled`                         | Enable PostgreSQL                               | `true`                                |
| `postgresql.image.registry`                  | PostgreSQL Docker registry                      | `docker.bintray.io`                   |
| `postgresql.image.repository`                | PostgreSQL Repository name                      | `bitnami/postgresql`                  |
| `postgresql.image.tag`                       | PostgreSQL docker image tag                     | `10.13.0-debian-10-r38`               |
| `postgresql.image.pullPolicy`                | PostgreSQL Container pull policy                | `IfNotPresent`                        |
| `postgresql.persistence.enabled`             | PostgreSQL persistence volume enabled           | `true`                                |
| `postgresql.persistence.existingClaim`       | Use an existing PVC to persist data             | `nil`                                 |
| `postgresql.persistence.size`                | PostgreSQL persistence volume size              | `50Gi`                                |
| `postgresql.postgresqlUsername`              | PostgreSQL admin username                       | `postgres`                            |
| `postgresql.postgresqlPassword`              | PostgreSQL admin password                       | ` `                                   |
| `postgresql.postgresqlExtendedConf.listenAddresses` | PostgreSQL listen address                | `"'*'"`                               |
| `postgresql.postgresqlExtendedConf.maxConnections`  | PostgreSQL max_connections parameter     | `1500`                                |
| `postgresql.db.name`                         | PostgreSQL Database name                        | `mission_control`                     |
| `postgresql.db.sslmode`                      | PostgreSQL Database SSL Mode                    | `false`                               |
| `postgresql.db.tablespace`                   | PostgreSQL Database Tablespace                  | `pg_default`                          |
| `postgresql.db.user`                         | PostgreSQL Database User                        | `mc`                                  |
| `postgresql.db.password`                     | PostgreSQL Database Password                    | `random 10 char alphanumeric string`  |
| `postgresql.db.jfmcSchema`                   | PostgreSQL Database mission control Schema      | `jfmc_server`                         |
| `postgresql.db.jfisSchema`                   | PostgreSQL Database insight server Schema       | `insight_server`                      |
| `postgresql.db.jfscSchema`                   | PostgreSQL Database insight scheduler Schema    | `insight_scheduler`                   |
| `postgresql.service.port`                    | PostgreSQL Database Port                        | `5432`                                |
| `postgresql.resources.requests.memory`       | PostgreSQL initial memory request               |                                       |
| `postgresql.resources.requests.cpu`          | PostgreSQL initial cpu request                  |                                       |
| `postgresql.resources.limits.memory`         | PostgreSQL memory limit                         |                                       |
| `postgresql.resources.limits.cpu`            | PostgreSQL cpu limit                            |                                       |
| `postgresql.master.nodeSelector`             | PostgreSQL master node selector                 | `{}`                                  |
| `postgresql.master.affinity`                 | PostgreSQL master node affinity                 | `{}`                                  |
| `postgresql.master.tolerations`              | PostgreSQL master node tolerations              | `[]`                                  |
| `postgresql.slave.nodeSelector`              | PostgreSQL slave node selector                  | `{}`                                  |
| `postgresql.slave.affinity`                  | PostgreSQL slave node affinity                  | `{}`                                  |
| `postgresql.slave.tolerations`               | PostgreSQL slave node tolerations               | `[]`                                  |
| `common.uid`                                 | user id                                         | `1050`                                |
| `common.gid`                                 | group id                                        | `1050`                                |
| `common.customInitContainers`                | Custom init containers                          | see [values.yaml](values.yaml)        |
| `common.customVolumes`                       | Custom Volumes                                  | see [values.yaml](values.yaml)        |
| `common.customVolumeMounts`                  | Custom Volume Mounts                            | see [values.yaml](values.yaml)        |
| `common.configMaps`                          | Custom configMaps                               | see [values.yaml](values.yaml)        |
| `common.customSidecarContainers`             | Custom SidecarContainers                        | see [values.yaml](values.yaml)        |
| `common.customSecrets`                       | Custom secrets                                  | see [values.yaml](values.yaml)        |
| `database.type`                              | External database type (`postgresql`)           | `postgresql`                          |
| `database.driver`                            | External database driver                        | `org.postgresql.Driver`               |
| `database.name`                              | External database name                          | `mission_control`                     |
| `database.url`                               | External database url                           | ``                                    |
| `database.user`                              | External database user                          | ` `                                   |
| `database.password`                          | External database password                      | ` `                                   |
| `database.jfmcSchema`                        | External database mission control Schema        | `jfmc_server`                         |
| `database.jfisSchema`                        | External database insight server Schema         | `insight_server`                      |
| `database.jfscSchema`                        | External database insight scheduler Schema      | `insight_scheduler`                   |
| `database.secrets.user.name`                 | External database username `Secret` name        |                                       |
| `database.secrets.user.key`                  | External database username `Secret` key         |                                       |
| `database.secrets.password.name`             | External database password `Secret` name        |                                       |
| `database.secrets.password.key`              | External database password `Secret` key         |                                       |
| `database.secrets.url.name`                  | External database url `Secret` name             |                                       |
| `database.secrets.url.key`                   | External database url `Secret` key              |                                       |
| `elasticsearch.enabled`                      | Enable Elasticsearch                            | `true`                                |
| `elasticsearch.configureDockerHost`          | Use privileged init container to configure Docker Host | `true`                         |
| `elasticsearch.persistence.enabled`          | Elasticsearch persistence volume enabled        | `true`                                |
| `elasticsearch.persistence.existingClaim`    | Use an existing PVC to persist data             | `nil`                                 |
| `elasticsearch.persistence.storageClass`     | Storage class of backing PVC                    | `generic`                             |
| `elasticsearch.persistence.size`             | Elasticsearch persistence volume size           | `50Gi`                                |
| `elasticsearch.javaOpts.xms`                 | Elasticsearch ES_JAVA_OPTS -Xms                 | ` `                                   |
| `elasticsearch.javaOpts.xmx`                 | Elasticsearch ES_JAVA_OPTS -Xmx                 | ` `                                   |
| `elasticsearch.resources.requests.memory`    | Elasticsearch initial memory request            |                                       |
| `elasticsearch.resources.requests.cpu`       | Elasticsearch initial cpu request               |                                       |
| `elasticsearch.resources.limits.memory`      | Elasticsearch memory limit                      |                                       |
| `elasticsearch.resources.limits.cpu`         | Elasticsearch cpu limit                         |                                       |
| `elasticsearch.env.clusterName`              | Elasticsearch Cluster Name                      | `es-cluster`                          |
| `logger.image.repository`                    | repository for logger image                     | `docker.bintray.io/busybox`           |
| `logger.image.tag`                           | tag for logger image                            | `1.31.1`                              |
| `missionControl.name`                        | Mission Control name                            | `mission-control`                     |
| `missionControl.image.repository`            | Container image                                 | `docker.bintray.io/jfrog/mission-control`     |
| `missionControl.image.version`               | Container image tag                             | `.Chart.AppVersion`                   |
| `missionControl.masterkey`                   | Mission Control Master Key . Can be generated with `openssl rand -hex 32` |``|
| `missionControl.jfrogUrl`                    | Main Artifactory URL, without the `/artifactory` prefix . Mandatory| ` `                                   |
| `missionControl.joinKey`                     | MissionControl join Key . Mandatory             | ` `                                   |
| `missionControl.masterKeySecretName`         | MissionControl Master Key secret name           |                                       |
| `missionControl.joinKeySecretName`           | MissionControl Join Key secret name             |                                       |
| `missionControl.service.annotations`         | Mission Control service annotations             | `{}`                                  |
| `missionControl.service.type`                | Mission Control service type                    | `ClusterIP`                           |
| `missionControl.externalPort`                | Mission Control service external port           | `80`                                  |
| `missionControl.internalPort`                | Mission Control service internal port           | `8080`                                |
| `missionControl.persistence.mountPath`       | Mission Control persistence volume mount path   | `"/var/opt/jfrog/mission-control"`    |
| `missionControl.persistence.storageClass`    | Storage class of backing PVC                    | `nil (uses alpha storage class annotation)` |
| `missionControl.persistence.existingClaim`   | Provide an existing PersistentVolumeClaim       | `nil`                                 |
| `missionControl.persistence.enabled`         | Mission Control persistence volume enabled      | `true`                                |
| `missionControl.persistence.accessMode`      | Mission Control persistence volume access mode  | `ReadWriteOnce`                       |
| `missionControl.persistence.size`            | Mission Control persistence volume size         | `100Gi`                               |
| `missionControl.preStartCommand`             | Command to run before mission control app starts| ``                                    |
| `missionControl.labels`                      | Mission Control labels                          | `{}`                                  |
| `missionControl.javaOpts.other`              | Mission Control JAVA_OPTIONS                    | `-server -XX:+UseG1GC -Dfile.encoding=UTF8` |
| `missionControl.javaOpts.xms`                | Mission Control JAVA_OPTIONS -Xms               | ` `                                   |
| `missionControl.resources.requests.memory`   | Mission Control initial memory request          |                                       |
| `missionControl.resources.requests.cpu`      | Mission Control initial cpu request             |                                       |
| `missionControl.resources.limits.memory`     | Mission Control memory limit                    |                                       |
| `missionControl.resources.limits.cpu`        | Mission Control cpu limit                       |                                       |
| `missionControl.propertyOverride`            | Force write of properties on mc startup         | ` `                                   |
| `missionControl.loggers`                     | Mission Control loggers (see values.yaml for possible values)          | ` `            |
| `missionControl.loggersResources.requests.memory`  | Mission Control loggers initial memory request                             |                                                                    |
| `missionControl.loggersResources.requests.cpu`     | Mission Control loggers initial cpu request                                |                                                                    |
| `missionControl.loggersResources.limits.memory`    | Mission Control loggers memory limit                                       |                                                                    |
| `missionControl.loggersResources.limits.cpu`       | Mission Control loggers cpu limit                                          |                                                                    |
| `missionControl.systemYaml`                  | Mission Control system configuration (`system.yaml`)  | `see values.yaml`               |
| `insightServer.name`                         | Insight Server name                             | `insight-server`                      |
| `insightServer.image.repository`             | Container image                                 | `docker.bintray.io/jfrog/insight-server`|
| `insightServer.image.version`                | Container image tag                             | `.Chart.AppVersion`                   |
| `insightServer.externalHttpPort`             | Insight Server service external port            | `8082`                                |
| `insightServer.internalHttpPort`             | Insight Server service internal port            | `8082`                                |
| `insightServer.allowIP`                      | Range of IPs allowed to be served by Insight Server service  | `"0.0.0.0/0"`            |
| `insightServer.resources.requests.memory`    | Insight Server initial memory request           |                                        |
| `insightServer.resources.requests.cpu`       | Insight Server initial cpu request              |                                        |
| `insightServer.resources.limits.memory`      | Insight Server memory limit                     |                                        |
| `insightServer.resources.limits.cpu`         | Insight Server cpu limit                        |                                        |
| `insightServer.loggers`                      | Insight Server loggers (see values.yaml for possible values)           | ` `            |
| `insightServer.loggersResources.requests.memory`  | Insight Server loggers initial memory request                             |                                                                    |
| `insightServer.loggersResources.requests.cpu`     | Insight Server loggers initial cpu request                                |                                                                    |
| `insightServer.loggersResources.limits.memory`    | Insight Server loggers memory limit                                       |                                                                    |
| `insightServer.loggersResources.limits.cpu`       | Insight Server loggers cpu limit                                          |                                                                    |
| `insightScheduler.name`                      | Insight Scheduler name                          | `insight-scheduler`                   |
| `insightScheduler.image.repository`          | Container image                                 | `docker.bintray.io/jfrog/insight-scheduler`  |
| `insightScheduler.image.version`             | Container image tag                             | `.Chart.AppVersion`                   |
| `insightScheduler.externalPort`              | Insight Scheduler service external port         | `8080`                                |
| `insightScheduler.internalPort`              | Insight Scheduler service internal port         | `8080`                                |
| `insightScheduler.javaOpts.other`            | Insight Scheduler JFMC_EXTRA_JAVA_OPTS          | ``                                    |
| `insightScheduler.javaOpts.xms`              | Insight Scheduler JFMC_EXTRA_JAVA_OPTS -Xms     | ``                                    |
| `insightScheduler.javaOpts.xmx`              | Insight Scheduler JFMC_EXTRA_JAVA_OPTS -Xmx     | ``                                    |
| `insightScheduler.resources.requests.memory` | Insight Scheduler initial memory request        |                                       |
| `insightScheduler.resources.requests.cpu`    | Insight Scheduler initial cpu request           |                                       |
| `insightScheduler.resources.limits.memory`   | Insight Scheduler memory limit                  |                                       |
| `insightScheduler.resources.limits.cpu`      | Insight Scheduler cpu limit                     |                                       |
| `insightScheduler.loggers`                   | Insight Scheduler loggers (see values.yaml for possible values)           | ` `            |
| `insightScheduler.loggersResources.requests.memory`  | Insight Scheduler loggers initial memory request |                              |
| `insightScheduler.loggersResources.requests.cpu`     | Insight Scheduler loggers initial cpu request    |                              |
| `insightScheduler.loggersResources.limits.memory`    | Insight Scheduler loggers memory limit           |                              |
| `insightScheduler.loggersResources.limits.cpu`       | Insight Scheduler loggers cpu limit              |                              |
| `router.name`                                | Router name                                            | `router`                        |
| `router.image.pullPolicy`                    | Container pull policy                                  | `IfNotPresent`                  |
| `router.image.repository`                    | Container image                                        | `docker.bintray.io/jfrog/router`|
| `router.image.version`                       | Container image tag                                    | `.Chart.AppVersion`             |
| `router.tlsEnabled`                          | Enable TLS connection                                  | `false`                         |

Specify each parameter using the `--set key=value[,key=value]` argument to `helm install`.


## Useful links
- https://www.jfrog.com
- https://www.jfrog.com/confluence/
- https://www.jfrog.com/confluence/display/EP/Getting+Started
