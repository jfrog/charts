# Default values for worker.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

global:
  ## Image registry to pull docker images from
  imageRegistry: ""
  ## Secrets used to pull images from private registries
  imagePullSecrets: []

  ## Chart.AppVersion can be overridden using global.versions.worker or image tags
  ## Note: Order of preference is 1) global.versions 2) image tags 3) Chart.AppVersion
  versions: {}

  ## Artifactory URL
  # jfrogUrl: ""
  ## Worker service requires a unique master key.
  ## You can generate one with the command: "openssl rand -hex 32"
  ## An initial one is auto generated on first startup.
  # masterKey: ""

  ## Alternatively, you can use a pre-existing secret with a key
  ## called "master-key" by specifying "masterKeySecretName"
  # masterKeySecretName: ""

  ## Join Key to connect other services to Artifactory
  ## IMPORTANT: Setting this value overrides the existing joinKey
  # joinKey: ""

  ## Alternatively, you can use a pre-existing secret with a key
  ## called "join-key" by specifying "joinKeySecretName"
  # joinKeySecretName: ""

  ## Note: tags customInitContainers,customVolumes,customVolumeMounts,customSidecarContainers can be used both from global and application level simultaneously
  # customVolumes: |

  # customVolumeMounts: |

  # customInitContainers: |

  # customSidecarContainers: |

  ## certificates added to this secret will be copied to $JFROG_HOME/worker/var/data/router/keys/trusted directory
  customCertificates:
    enabled: false
    # certificateSecretName:

  ## Applies to Worker pods
  nodeSelector: {}

## Artifactory URL. Mandatory
jfrogUrl:

## Number of replicas used when autoscaling is disabled
replicaCount: 1

## Worker image
image:
  registry: releases-docker.jfrog.io
  repository: jfrog/worker
  pullPolicy: IfNotPresent

## String to partially override worker.name template (will maintain the release name)
nameOverride: ""
## String to fully override worker.fullname template
fullnameOverride: ""

## Specify common probes parameters
probes:
  timeoutSeconds: 120

## Service Account
## Ref: https://kubernetes.io/docs/admin/service-accounts-admin/
##
serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

## Annotations added to Worker pods
podAnnotations: {}

## Disable podSecurityContext for openshift
podSecurityContext:
  enabled: true
  fsGroup: 1131
  runAsUser: 1131

## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
## Disable securityContext for openshift
securityContext:
  enabled: true
  runAsNonRoot: true
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - NET_RAW

## Set sleep in order to keep service responding while k8s is removing it from network
lifecycle:
  preStopSleepSeconds: 15

## Worker service configuration
service:
  type: ClusterIP
  port: 8098
  grpcPort: 8099
  ## If the type is NodePort you can set a fixed port
  # nodePort: 30000
  productHome: /opt/jfrog/worker

## Worker service ingress configuration
ingress:
  enabled: false
  className: ""
  grpcReadTimeout: 43200s
  grpcSendTimeout: 43200s
  clientBodyTimeout: 43200s
  backendTls: false
  backendPortName: http-router


## Worker container resources
resources:
  limits:
    cpu: 2
    memory: 1536Mi
  requests:
    cpu: 2
    memory: 1536Mi

## Autoscaling configuration for Worker deployment
autoscaling:
  ## Enable autoscaling for Worker deployment
  enabled: true
  ## Type of autoscaler. Possible values:
  ## - "keda"
  ##    ref: https://keda.sh/
  ## - "hpa"
  ##    ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  kind: "hpa"
  minReplicas: 1
  maxReplicas: 10
  pollingInterval: 30
  cooldownPeriod: 300
  # targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80
  keda:
    threshold: '3'
    query: sum(avg_over_time(jfwks_execution_processing_count{service="{{ .Release.Name }}", namespace="{{ .Release.Namespace }}"}[1m]))
    metricType: AverageValue
    prometheusServerAddress: http://prometheus-server.monitoring.svc.cluster.local:9090

## @param serviceMonitor Declaratively specifies how groups of Kubernetes services should be monitored.
## Ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/getting-started.md
serviceMonitor:
  observabilityEnabled: false
  scheme: http
  targetPort: 8082
  enabled: false
  interval: 60s
  scrapeTimeout: 15s
  kubePromApp: ""
  kubePromFullName: ""
  namespaceSelector:
    any: true

## Applies to worker pods
nodeSelector: {}

## Applies to worker pods
tolerations: []

## Applies to worker pods
affinity: {}

## Extra environment variables that can be used to tune worker service to your needs.
## Uncomment and set value as needed
extraEnvironmentVariables:
#  - name: MY_ENV_VAR
#    value: ""


router:
  name: router
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/router
    tag: 7.149.0
    imagePullPolicy: IfNotPresent
  serviceRegistry:
    ## Service registry (Access) TLS verification skipped if enabled
    insecure: false
    ## Service registry URL and gRPC URL use jfrogUrl by default. Override if needed
    # url: https://workersvc-artifactory:8082
    # grpcUrl: workersvc-artifactory:8082
  internalPort: 8046
  externalPort: 8082
  tlsEnabled: false

  ## Extra environment variables that can be used to tune router to your needs.
  ## Uncomment and set value as needed
  extraEnvironmentVariables:
  # - name: MY_ENV_VAR
  #   value: ""

  resources:
    requests: {}
  #    memory: "100Mi"
  #    cpu: "100m"
    limits: {}
  #    memory: "1Gi"
  #    cpu: "1"

  ## Add custom volumesMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: /scripts/script.sh
  #    subPath: script.sh

  # customLivenessProbe:
  #   exec:
  #     command:
  #       - sh
  #       - -c
  #       - curl -s -k --fail --max-time 120 http://localhost:8046/router/api/v1/system/liveness
  #   initialDelaySeconds: 0
  #   periodSeconds: 5
  #   timeoutSeconds: 120
  #   failureThreshold: 10
  #   successThreshold: 1

  livenessProbe:
    enabled: true
    periodSeconds: 5
    failureThreshold: 10
    successThreshold: 1

  # customReadinessProbe:
  #   exec:
  #     command:
  #       - sh
  #       - -c
  #       - curl -s -k --fail --max-time 120 http://localhost:8046/router/api/v1/system/readiness
  #   initialDelaySeconds: 0
  #   periodSeconds: 5
  #   timeoutSeconds: 120
  #   failureThreshold: 10
  #   successThreshold: 1

  readinessProbe:
    enabled: true
    periodSeconds: 5
    failureThreshold: 10
    successThreshold: 1

  # customStartupProbe:
  #   exec:
  #     command:
  #       - sh
  #       - -c
  #       - curl -s -k --fail --max-time 120 http://localhost:8046/router/api/v1/system/readiness
  #   initialDelaySeconds: 0
  #   periodSeconds: 5
  #   timeoutSeconds: 120
  #   failureThreshold: 10
  #   successThreshold: 1

  startupProbe:
    enabled: true
    initialDelaySeconds: 0
    failureThreshold: 60
    periodSeconds: 5
  persistence:
    mountPath: "/var/opt/jfrog/router"

## @section Init Container Parameters
##

## Init Container parameters

initContainers:
  ## @param volumePermissions.enabled Enable init container that changes the owner and group of the persistent volume(s) mountpoint to `runAsUser:fsGroup`
  ##
  ## @param initContainers.image.registry Init container volume-permissions image registry
  ## @param initContainers.image.repository Init container volume-permissions image repository
  ## @param initContainers.image.tag Init container volume-permissions image tag
  ## @param initContainers.image.pullPolicy Init container volume-permissions image pull policy
  ## @param initContainers.image.pullSecrets Specify docker-registry secret names as an array
  ##
  image:
    registry: releases-docker.jfrog.io
    repository: ubi9/ubi-minimal
    tag: 9.4.949.1716471857
    ## Specify a imagePullPolicy
    ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
    ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images
    ##
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## Example:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
  ## Init Container resource requests and limits
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ## We usually recommend not to specify default resources and to leave this as a conscious
  ## choice for the user. This also increases chances charts run on environments with little
  ## resources, such as Minikube. If you do want to specify resources, uncomment the following
  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  ## @param volumePermissions.resources.limits Init container volume-permissions resource limits
  ## @param volumePermissions.resources.requests Init container volume-permissions resource requests
  ##
  resources:
    ## Example:
    ## limits:
    ##    cpu: 100m
    ##    memory: 128Mi
    ##
    limits: {}
    ## Examples:
    ## requests:
    ##    cpu: 100m
    ##    memory: 128Mi
    ##
    requests: {}

## a unique master key.
## You can generate one with the command: "openssl rand -hex 32"
## An initial one is auto generated by Artifactory on first startup.
masterKey: FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
## Alternatively, you can use a pre-existing secret with a key called master-key by specifying masterKeySecretName
masterKeySecretName: ""

## Join Key to connect other services to Artifactory
## IMPORTANT: Setting this value overrides the existing joinKey
## IMPORTANT: You should NOT use the example joinKey for a production deployment!
joinKey: EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE
## Alternatively, you can use a pre-existing secret with a key called join-key by specifying joinKeySecretName
joinKeySecretName: ""

## System YAML entries now reside under files/system.yaml.
## You can provide the specific values that you want to add or override under 'extraSystemYaml'.
## For example:
## extraSystemYaml:
##   shared:
##     node:
##       id: my-instance
## The entries provided under 'extraSystemYaml' are merged with files/system.yaml to create the final system.yaml.
## If you have already provided system.yaml under, 'systemYaml', the values in that entry take precedence over files/system.yaml
## You can modify specific entries with your own value under `extraSystemYaml`, The values under extraSystemYaml overrides the values under 'systemYaml' and files/system.yaml
extraSystemYaml: {}

## systemYaml is intentionally commented and the previous content has been moved under files/system.yaml.
## You have to add the all entries of the system.yaml file here, and it overrides the values in files/system.yaml.
# systemYaml:

## worker systemYaml override
## This is for advanced usecases where users wants to provide their own systemYaml for configuring worker
## Note: This will override existing (default) .Values.systemYaml in values.yaml
## Alternatively, systemYaml can be overidden via customInitContainers using external sources like vaults, external repositories etc. Please refer customInitContainer section below for an example.
## Note: Order of preference is 1) customInitContainers 2) systemYamlOverride existingSecret 3) default systemYaml in values.yaml
systemYamlOverride:
  ## You can use a pre-existing secret by specifying existingSecret
  existingSecret: ""
  ## The dataKey should be the name of the secret data key created.
  dataKey:

## @param commonAnnotations Annotations to add to all deployed objects
##
commonAnnotations: {}

## @param podLabels integration Pod labels. Evaluated as a template
## Ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
##
podLabels: {}

## Worker deployment labels
labels: {}

common:
  ## Spread Worker pods evenly across your nodes or some other topology
  topologySpreadConstraints: []
    # - maxSkew: 1
    #   topologyKey: kubernetes.io/hostname
    #   whenUnsatisfiable: DoNotSchedule
    #   labelSelector:
    #     matchLabels:
    #       app: '{{ template "worker.name" . }}'
    #       role: '{{ template "worker.name" . }}'
  #       release: "{{ .Release.Name }}"

  ## Custom command to run before microservice startup. Runs BEFORE any microservice-specific preStartCommand
  preStartCommand:

  ## Add custom volumes
  customVolumes: |
    # - name: custom-script
    #   configMap:
    #     name: custom-script

  ## Add custom volumesMounts
  customVolumeMounts: |
    # - name: custom-script
    #   mountPath: /scripts/script.sh
    #   subPath: script.sh

  ## Add any list of configmaps to worker
  configMaps: |
    # posthook-start.sh: |-
    #   echo "This is a post start script"
    # posthook-end.sh: |-
    #   echo "This is a post end script"

  ## Add custom init containers execution after predefined init containers
  customInitContainers: |
    # - name: "custom-systemyaml-setup"
    #   image: {{ template "initContainers.image" $ }}
    #   imagePullPolicy: {{ $.Values.initContainers.image.pullPolicy | quote }}
    #   securityContext:
    #     runAsNonRoot: true
    #     allowPrivilegeEscalation: false
    #     capabilities:
    #       drop:
    #         - NET_RAW
    #   command:
    #     - 'sh'
    #     - '-c'
    #     - 'curl -o /opt/jfrog/worker/var/etc/system.yaml https://<repo-url>/systemyaml'
    #   volumeMounts:
    #     - mountPath: "/opt/jfrog/worker/var"
    #       name: data

  ## Add custom sidecar containers
  ## - The provided example uses a custom volume (customVolumes)
  customSidecarContainers: |
    # - name: "sidecar-list-etc"
    #   image: {{ template "initContainers.image" $ }}
    #   imagePullPolicy: {{ $.Values.initContainers.image.pullPolicy | quote }}
    #   securityContext:
    #     runAsNonRoot: true
    #     allowPrivilegeEscalation: false
    #     capabilities:
    #       drop:
    #         - NET_RAW
    #   command:
    #     - 'sh'
    #     - '-c'
    #     - 'sh /scripts/script.sh'
    #   volumeMounts:
    #     - mountPath: "/opt/jfrog/worker/var"
    #       name: data
    #     - mountPath: "/scripts/script.sh"
    #       name: custom-script
    #       subPath: script.sh
    #   resources:
    #     requests:
    #       memory: "32Mi"
    #       cpu: "50m"
    #     limits:
    #       memory: "128Mi"
    #       cpu: "100m"

## certificates added to this secret will be copied to $JFROG_HOME/worker/var/data/router/keys/trusted directory
customCertificates:
  enabled: false
  # certificateSecretName:

## Adding entries to a Pod's /etc/hosts file
## For an example, refer - https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases
hostAliases: []
  # - ip: "127.0.0.1"
  #   hostnames:
  #     - "foo.local"
  #     - "bar.local"
  # - ip: "10.1.2.3"
  #   hostnames:
  #     - "foo.remote"
  #     - "bar.remote"

## Worker liveness probe configuration
livenessProbe:
  enabled: true
  periodSeconds: 5
  failureThreshold: 10
  successThreshold: 1

## Worker custom liveness probe configuration
# customLivenessProbe:
#   httpGet:
#     path: /api/v1/system/liveness
#     port: 8098
#   initialDelaySeconds: 0
#   periodSeconds: 5
#   timeoutSeconds: 120
#   failureThreshold: 10
#   successThreshold: 1


## Worker readiness probe configuration
readinessProbe:
  enabled: true
  periodSeconds: 5
  failureThreshold: 10
  successThreshold: 1

## Worker custom readiness probe configuration
# customReadinessProbe:
#   httpGet:
#     path: /api/v1/system/readiness
#     port: 8098
#   initialDelaySeconds: 0
#   periodSeconds: 5
#   timeoutSeconds: 120
#   failureThreshold: 10
#   successThreshold: 1

## Worker startup probe configuration
startupProbe:
  enabled: true
  failureThreshold: 60
  periodSeconds: 5

## Worker custom startup probe configuration
# customStartupProbe:
#   httpGet:
#     path: /api/v1/system/readiness
#     port: 8098
#   initialDelaySeconds: 0
#   periodSeconds: 5
#   timeoutSeconds: 120
#   failureThreshold: 10
#   successThreshold: 1

## Add any of the loggers to a sidecar if you want to be able to see them with kubectl logs or a log collector in your k8s cluster
loggers: []
  # - router-request.log
  # - router-service.log
  # - router-traefik.log
  # - worker-service.log
  # - worker-request.log

## Loggers containers resources
loggersResources: {}
  # requests:
  #   memory: "64Mi"
  #   cpu: "25m"
  # limits:
  #   memory: "128Mi"
  #   cpu: "50m"

## Filebeat Sidecar container
## The provided filebeat configuration is for Worker logs. It assumes you have a logstash installed and configured properly.
filebeat:
  enabled: false
  name: worker-filebeat
  image:
    repository: docker.elastic.co/beats/filebeat
    version: 7.17.26
    pullPolicy: IfNotPresent
  logstashUrl: "logstash:5044"

  terminationGracePeriod: 10

  livenessProbe:
    exec:
      command:
        - sh
        - -c
        - |
          #!/usr/bin/env bash -e
          curl --fail 127.0.0.1:5066
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5

  readinessProbe:
    exec:
      command:
        - sh
        - -c
        - |
          #!/usr/bin/env bash -e
          filebeat test output
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5

  resources: {}
    # requests:
    #   memory: "100Mi"
    #   cpu: "100m"
    # limits:
    #   memory: "100Mi"
    #   cpu: "100m"

  filebeatYml: |
    logging.level: info
    path.data: /opt/jfrog/worker/var/log/filebeat
    name: worker-filebeat
    queue.spool:
      file:
        permissions: 0760
    filebeat.inputs:
    - type: log
      enabled: true
      close_eof: ${CLOSE:false}
      paths:
         - /opt/jfrog/worker/var/log/*.log
      fields:
        service: "jfwks"
        log_type: "worker"
    output:
      logstash:
         hosts: ["{{ .Values.filebeat.logstashUrl }}"]

observability:
  enabled: true
  name: observability
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/observability
    tag: 1.31.11
    pullPolicy: IfNotPresent
  internalPort: 8036
  resources: {}
    # requests:
    #   memory: "100Mi"
    #   cpu: "100m"
    # limits:
    #   memory: "1Gi"
    #   cpu: "1"

  ## Add lifecycle hooks for the observability pod
  lifecycle: {}
    # postStart:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the observability postStart handler > /usr/share/message"]
    # preStop:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the observability preStart handler > /usr/share/message"]

  # customLivenessProbe:
  #   exec:
  #     command:
  #       - sh
  #       - -c
  #       - curl --fail --max-time 120 http://localhost:8036/api/v1/system/liveness
  #   initialDelaySeconds: 0
  #   failureThreshold: 5
  #   timeoutSeconds: 120
  #   periodSeconds: 10
  #   successThreshold: 1

  livenessProbe:
    enabled: true
    failureThreshold: 5
    periodSeconds: 10
    successThreshold: 1

  # customStartupProbe:
  #   exec:
  #     command:
  #       - sh
  #       - -c
  #       - curl --fail --max-time 120 http://localhost:8036/api/v1/system/readiness
  #   initialDelaySeconds: 30
  #   failureThreshold: 90
  #   periodSeconds: 5
  #   timeoutSeconds: 120

  startupProbe:
    enabled: true
    initialDelaySeconds: 30
    failureThreshold: 90
    periodSeconds: 5

  persistence:
    mountPath: "/var/opt/jfrog/observability"

## Pod Disruption Budget configuration
## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
##
pdb:
  ## @param pdb.create Enable/disable a Pod Disruption Budget creation
  ##
  create: false
  ## @param pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
  ##
  minAvailable: 1
  ## @param pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable
  ##
  maxUnavailable: ""
