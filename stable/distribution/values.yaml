# Default values for distribution.
# This is a YAML-formatted file.
# Beware when changing values here. You should know what you are doing!
# Access the values with {{ .Values.key.subkey }}

# Common
initContainerImage: docker.bintray.io/alpine:3.12

# For supporting pulling from private registries
imagePullSecrets:

# For HA
replicaCount: 1

## Role Based Access Control
## Ref: https://kubernetes.io/docs/admin/authorization/rbac/
rbac:
  create: true
  role:
    ## Rules to create. It follows the role specification
    rules:
    - apiGroups:
      - ''
      resources:
      - services
      - endpoints
      - pods
      verbs:
      - get
      - watch
      - list

networkpolicy:
  # Allows all ingress and egress
  - name: distribution
    podSelector:
      matchLabels:
        app: distribution
    egress:
      - {}
    ingress:
      - {}
  # Uncomment to allow only distribution pods to communicate with postgresql (if postgresql.enabled is true)
  # - name: postgres
  #   podSelector:
  #     matchLabels:
  #       app: postgresql
  #   ingress:
  #   - from:
  #     - podSelector:
  #         matchLabels:
  #           app: distribution
  # Uncomment to allow only distribution pods to communicate with redis
  # - name: redis
  #   podSelector:
  #     matchLabels:
  #       app: redis
  #   ingress:
  #   - from:
  #     - podSelector:
  #         matchLabels:
  #           app: distribution

## Service Account
## Ref: https://kubernetes.io/docs/admin/service-accounts-admin/
##
serviceAccount:
  create: true
  ## The name of the ServiceAccount to use.
  ## If not set and create is true, a name is generated using the fullname template
  name:

# PostgreSQL
## Configuration values for the PostgreSQL dependency sub-chart
## ref: https://github.com/bitnami/charts/blob/master/bitnami/postgresql/README.md
postgresql:
  enabled: true
  image:
    registry: docker.bintray.io
    repository: bitnami/postgresql
    tag: 10.13.0-debian-10-r38
  postgresqlUsername: distribution
  postgresqlPassword: ""
  postgresqlDatabase: distribution
  postgresqlExtendedConf:
    listenAddresses: "'*'"
    maxConnections: "1500"
  persistence:
    enabled: true
    size: 50Gi
  service:
    port: 5432
  resources: {}
  #  requests:
  #    memory: "512Mi"
  #    cpu: "100m"
  #  limits:
  #    memory: "1Gi"
  #    cpu: "1"
  nodeSelector: {}
  tolerations: []
  affinity: {}

## If NOT using the PostgreSQL in this chart (postgresql.enabled=false),
## specify custom/external database details here
database:
  type: postgresql
  driver: org.postgresql.Driver
  ## If you would like this chart to create the secret containing the db
  ## password, use these values
  url:
  user:
  password:
  ## If you have existing Kubernetes secrets containing db credentials, use
  ## these values
  secrets: {}
  #  user:
  #    name: "dis-database-creds"
  #    key: "db-user"
  #  password:
  #    name: "dis-database-creds"
  #    key: "db-password"
  #  url:
  #    name: "dis-database-creds"
  #    key: "db-url"


## Configuration values for the redis dependency
## ref: https://github.com/helm/charts/blob/master/stable/redis/README.md
##
redis:
  image:
    repository: docker.bintray.io/bitnami/redis
    tag: 5.0.9-debian-10-r31
    pullPolicy: IfNotPresent
  port: 6379
  password: ""
  ## Alternatively, you can use a pre-existing secret with a key called redis-password by specifying existingSecret
  # existingSecret: <name-of-existing-secret>
  uid: 1001
  serviceAccount:
    create: true
  disableCommands: "FLUSHDB,FLUSHALL"
  persistence:
    enabled: true
    path: /bitnami/redis/data
    size: 10Gi
    ## A manually managed Persistent Volume and Claim
    ## Requires redis.persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    # existingClaim:
    accessMode: ReadWriteOnce
  resources: {}
  #  requests:
  #    memory: "256Mi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "250m"

  nodeSelector: {}
  tolerations: []
  affinity: {}

common:
  uid: 1020
  gid: 1020

  ## Add custom volumes
  customVolumes: |
  #  - name: custom-script
  #    configMap:
  #      name: custom-script

logger:
  image:
    repository: docker.bintray.io/busybox
    tag: 1.31.1

distribution:
  name: distribution
  ## Note that by default we use appVersion to get image tag/version
  image:
    repository: docker.bintray.io/jfrog/distribution-distribution
    # version:
    imagePullPolicy: IfNotPresent
  internalPort: 8080
  externalPort: 80
  ## Distribution requires a unique master key
  ## You can generate one with the command:
  ## 'openssl rand -hex 32'
  ## Pass it to helm with '--set distribution.masterKey=${MASTER_KEY}'
  ## IMPORTANT: You should NOT use the example masterKey for a production deployment!
  masterKey: BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB
  ## Alternatively, you can use a pre-existing secret with a key called master-key by specifying masterKeySecretName
  # masterKeySecretName:

  joinKey: EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE
  ## Alternatively, you can use a pre-existing secret with a key called join-key by specifying joinKeySecretName
  # joinKeySecretName:

  ## If false, all service console logs will not redirect to a common console.log
  consoleLog: false
  ## Artifactory URL . Mandatory
  jfrogUrl:

  serviceId:

  ## Extra environment variables that can be used to tune Distribution to your needs.
  ## Uncomment and set value as needed
  extraEnvironmentVariables:
  # - name: JF_DISTRIBUTION_DATABASE_URL
  #   value: "jdbc:postgresql://localhost:5432/distribution"

  systemYaml: |
    shared:
      logging:
        consoleLog:
          enabled: {{ .Values.distribution.consoleLog }}
      jfrogUrl: "{{ required "\n\ndistribution.jfrogUrl is required! This allows to connect to Artifactory.\n(You can copy the JFrog URL from Admin > Security > Settings)" .Values.distribution.jfrogUrl }}"
      database:
      {{- if .Values.postgresql.enabled }}
        type: "postgresql"
        driver: "org.postgresql.Driver"
        username: "{{ .Values.postgresql.postgresqlUsername }}"
        url: "postgresql://{{ .Release.Name }}-postgresql:{{ .Values.postgresql.service.port }}/{{ .Values.postgresql.postgresqlDatabase }}"
      {{ else }}
        type: "{{ .Values.database.type }}"
        driver: "{{ .Values.database.driver }}"
      {{- end }}
    distribution:
      extraJavaOpts: >
      {{- with .Values.distribution.javaOpts }}
      {{- if .xms }}
        -Xms{{ .xms }}
      {{- end }}
      {{- if .xmx }}
        -Xmx{{ .xmx }}
      {{- end }}
      {{- if .other }}
        {{ .other }}
      {{- end }}
      {{- end }}
    distributor:
      extraJavaOpts: >
      {{- with .Values.distributor.javaOpts }}
      {{- if .xms }}
        -Xms{{ .xms }}
      {{- end }}
      {{- if .xmx }}
        -Xmx{{ .xmx }}
      {{- end }}
      {{- if .other }}
        {{ .other }}
      {{- end }}
      {{- end }}

  service:
    type: ClusterIP

  ## Add custom init containers
  customInitContainers: |
  #  - name: "custom-setup"
  #    image: "{{ .Values.initContainerImage }}"
  #    imagePullPolicy: "{{ .Values.distribution.image.pullPolicy }}"
  #    command:
  #      - 'sh'
  #      - '-c'
  #      - 'touch {{ .Values.distribution.persistence.mountPath }}/example-custom-setup'
  #    volumeMounts:
  #      - mountPath: "{{ .Values.distribution.persistence.mountPath }}"
  #        name: distribution-data

  ## Add custom volumeMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: "/scripts/script.sh"
  #    subPath: script.sh

  resources: {}
  #  requests:
  #    memory: "2Gi"
  #    cpu: "500m"
  #  limits:
  #    memory: "4Gi"
  #    cpu: "2"

  livenessProbe:
    enabled: true
    config: |
      httpGet:
        path: /api/v1/system/ping
        port: {{ .Values.distribution.internalPort }}
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
      successThreshold: 1

  readinessProbe:
    enabled: true
    config: |
      httpGet:
        path: /api/v1/system/ping
        port: {{ .Values.distribution.internalPort }}
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 10
      successThreshold: 1

  ## Control Java options (JAVA_OPTIONS)
  ## IMPORTANT: keep javaOpts.xmx no higher than resources.limits.memory
  javaOpts:
    # xms: "1g"
    # xmx: "2g"
    # other: ""

  persistence:
    enabled: true
    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    # existingClaim:

    accessMode: ReadWriteOnce
    mountPath: "/var/opt/jfrog/distribution"
    size: 50Gi
    ## distribution data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner. (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

  ## Custom command to run before Distribution startup.
  preStartCommand:
  ## For example, to import trusted keystore ( Need to mount the certificates first )
  # preStartCommand: "/java/jdk-11.0.2+9/bin/keytool -importcert -keystore /java/jdk-11.0.2+9/lib/security/cacerts -storepass changeit -file /tmp/trusted-certificates/root.crt -alias 'newcerts'"

  # Add any of the loggers to a sidecar if you want to be able to see them with kubectl logs or a log collector in your k8s cluster
  loggers: []
  # - access.log
  # - distribution.log
  # - request.log

  # Loggers containers resources
  loggersResources: {}
  #  requests:
  #    memory: "64Mi"
  #    cpu: "25m"
  #  limits:
  #    memory: "128Mi"
  #    cpu: "50m"

  nodeSelector: {}
  tolerations: []
  affinity: {}

distributor:
  name: distributor
  ## Note that by default we use appVersion to get image tag/version
  image:
    repository: docker.bintray.io/jfrog/distribution-distributor
    # version:
    imagePullPolicy: IfNotPresent

  ## Extra environment variables that can be used to tune Distributor to your needs.
  ## Uncomment and set value as needed
  extraEnvironmentVariables:
  # - name: JF_DISTRIBUTION_FOREMAN_INACTIVITYTHRESHOLDSECS
  #   value: "300"

  resources: {}
  #  requests:
  #    memory: "2Gi"
  #    cpu: "500m"
  #  limits:
  #    memory: "4Gi"
  #    cpu: "2"

  ## Control Java options (JAVA_OPTIONS)
  ## IMPORTANT: keep javaOpts.xmx no higher than resources.limits.memory
  javaOpts:
    # xms: "1g"
    # xmx: "2g"
    # other: ""

  persistence:
    enabled: true
    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    # existingClaim:

    accessMode: ReadWriteOnce
    mountPath: "/var/opt/jfrog/distribution"
    size: 50Gi
    ## distribution data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

  ## Add custom volumeMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: "/scripts/script.sh"
  #    subPath: script.sh

  ## Custom command to run before Distribution startup.
  preStartCommand:
  ## For example, to import trusted keystore ( Need to mount the certificates first )
  # preStartCommand: "/java/jdk-11.0.2+9/bin/keytool -importcert -keystore /java/jdk-11.0.2+9/lib/security/cacerts -storepass changeit -file /tmp/trusted-certificates/root.crt -alias 'newcerts'"

  # Add any of the loggers to a sidecar if you want to be able to see them with kubectl logs or a log collector in your k8s cluster
  loggers: []
  # - distributor.log
  # - foreman.log

  # Loggers containers resources
  loggersResources: {}
  #  requests:
  #    memory: "64Mi"
  #    cpu: "25m"
  #  limits:
  #    memory: "128Mi"
  #    cpu: "50m"

  nodeSelector: {}
  tolerations: []
  affinity: {}
  foreman:
    inactivityThresholdSecs:

router:
  name: router
  image:
    repository: docker.bintray.io/jfrog/router
    version: 1.4.0
    imagePullPolicy: IfNotPresent
  internalPort: 8082
  externalPort: 8082
  resources: {}
  #  requests:
  #    memory: "100Mi"
  #    cpu: "100m"
  #  limits:
  #    memory: "1Gi"
  #    cpu: "1"

  livenessProbe:
    enabled: true
    config: |
      httpGet:
        path: /router/api/v1/system/health
        port: {{ .Values.router.internalPort }}
      initialDelaySeconds: 90
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
      successThreshold: 1

  readinessProbe:
    enabled: true
    config: |
      httpGet:
        path: /router/api/v1/system/health
        port: {{ .Values.router.internalPort }}
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 10
      successThreshold: 1

  persistence:
    accessMode: ReadWriteOnce
    mountPath: "/var/opt/jfrog/router"
    size: 5Gi

  # Add any of the loggers to a sidecar if you want to be able to see them with kubectl logs or a log collector in your k8s cluster
  loggers: []

  nodeSelector: {}
  tolerations: []
  affinity: {}

shared:
  node:
    id:

# Filebeat Sidecar container
## The provided filebeat configuration is for Distribution logs. It assumes you have a logstash installed and configured properly.
filebeat:
  enabled: false
  name: distribution-filebeat
  image:
    repository: docker.elastic.co/beats/filebeat
    version: 7.5.1
  logstashUrl: "logstash:5044"

  terminationGracePeriod: 10

  livenessProbe:
    exec:
      command:
        - sh
        - -c
        - |
          #!/usr/bin/env bash -e
          curl --fail 127.0.0.1:5066
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5

  readinessProbe:
    exec:
      command:
        - sh
        - -c
        - |
          #!/usr/bin/env bash -e
          filebeat test output
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5

  resources: {}
#    requests:
#      memory: "100Mi"
#      cpu: "100m"
#    limits:
#      memory: "100Mi"
#      cpu: "100m"

  filebeatYml: |
    logging.level: info
    path.data: {{ .Values.distribution.persistence.mountPath }}/log/filebeat
    name: artifactory-filebeat
    queue.spool: ~
    filebeat.inputs:
    - type: log
      enabled: true
      close_eof: ${CLOSE:false}
      paths:
         - {{ .Values.distribution.persistence.mountPath }}/log/*.log
      fields:
        service: "jfds"
        log_type: "distribution"
    output:
      logstash:
         hosts: ["{{ .Values.filebeat.logstashUrl }}"]
