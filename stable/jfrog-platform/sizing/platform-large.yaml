artifactory:
  ##############################################################################################
  # The large sizing
  # This size is intended for large organizations. It can be increased with adding replicas or moving to the xlarge sizing
  # [WARNING] Some of the configuration mentioned in this file are taken inside system.yaml
  # hence this configuration will be overridden when enabling systemYamlOverride
  ##############################################################################################
  splitServicesToContainers: true
  artifactory:
    # Enterprise and above licenses are required for setting replicaCount greater than 1.
    # Count should be equal or above the total number of licenses available for artifactory.
    replicaCount: 3
  
    # Require multiple Artifactory pods to run on separate nodes
    podAntiAffinity:
      type: "hard"
  
    resources:
      requests:
        cpu: "2"
        memory: 10Gi
      limits:
        # cpu: "14"
        memory: 12Gi
  
    javaOpts:
      other: >
        -XX:InitialRAMPercentage=40
        -XX:MaxRAMPercentage=65
        -Dartifactory.async.corePoolSize=80
        -Dartifactory.async.poolMaxQueueSize=20000
        -Dartifactory.http.client.max.total.connections=100
        -Dartifactory.http.client.max.connections.per.route=100
        -Dartifactory.access.client.max.connections=125
        -Dartifactory.metadata.event.operator.threads=4
        -XX:MaxMetaspaceSize=512m
        -Djdk.nio.maxCachedBufferSize=524288
        -XX:MaxDirectMemorySize=512m
    tomcat:
      connector:
        maxThreads: 500
        extraConfig: 'acceptCount="800" acceptorThreadCount="2" compression="off" connectionLinger="-1" connectionTimeout="120000" enableLookups="false"'
  
    database:
      maxOpenConnections: 100
  
    extraEnvironmentVariables:
      - name: MALLOC_ARENA_MAX
        value: "8"
      - name: SKIP_WAIT_FOR_EXTERNAL_DB
        value: "true"
  
  access:
    tomcat:
      connector:
        maxThreads: 125
    javaOpts:
      other: >
        -XX:InitialRAMPercentage=20
        -XX:MaxRAMPercentage=60
    database:
      maxOpenConnections: 100
    resources:
      requests:
        cpu: 1
        memory: 1.5Gi
      limits:
        # cpu: 1
        memory: 2Gi
    extraEnvironmentVariables:
      - name: MALLOC_ARENA_MAX
        value: "8"
      - name: SKIP_WAIT_FOR_EXTERNAL_DB
        value: "true"
  
  
  router:
    resources:
      requests:
        cpu: 400m
        memory: 800Mi
      limits:
        # cpu: "8"
        memory: 2Gi
  
  frontend:
    resources:
      requests:
        cpu: 200m
        memory: 300Mi
      limits:
        # cpu: "3"
        memory: 1Gi
  
  metadata:
    database:
      maxOpenConnections: 100
    resources:
      requests:
        cpu: 200m
        memory: 200Mi
      limits:
        # cpu: "4"
        memory: 1Gi
  
  onemodel:
    resources:
      requests:
        cpu: 200m
        memory: 160Mi
      limits:
        # cpu: "1"
        memory: 250Mi
        
  event:
    resources:
      requests:
        cpu: 100m
        memory: 50Mi
      limits:
        # cpu: 500m
        memory: 250Mi
  
  observability:
    resources:
      requests:
        cpu: 100m
        memory: 50Mi
      limits:
        # cpu: 500m
        memory: 250Mi
  
  jfconnect:
    resources:
      requests:
        cpu: 50m
        memory: 100Mi
      limits:
        # cpu: 500m
        memory: 250Mi
  
  jfconfig:
    resources:
      requests:
        cpu: 100m
        memory: 1000Mi
      limits:
        # cpu: 500m
        memory: 1500Mi
    extraEnvironmentVariables:
      - name: MALLOC_ARENA_MAX
        value: "8"      
  
  topology:
    resources:
      requests:
        cpu: 100m
        memory: 1000Mi
      limits:
        memory: 1500Mi
    extraEnvironmentVariables:
      - name: MALLOC_ARENA_MAX
        value: "8"
  
  nginx:
    replicaCount: 2
    disableProxyBuffering: true
    resources:
      requests:
        cpu: "1"
        memory: "500Mi"
      limits:
        # cpu: "4"
        memory: "1Gi"
  
  postgresql:
    primary:
      extendedConfiguration: |
        max_connections = 600
      affinity:
        # Require PostgreSQL pod to run on a different node than Artifactory pods
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - artifactory
              topologyKey: kubernetes.io/hostname
      resources:
        requests:
          memory: 64Gi
          cpu: "16"
        limits:
          memory: 64Gi
          # cpu: "32"
  
  evidence:
    resources:
      requests:
        cpu: 500m
        memory: 800Mi
      limits:
        # cpu: "2"
        memory: 2Gi
  
  rtfs:
    resources:
      requests:
        cpu: 500m
        memory: 3500Mi
      limits:
        # cpu: "2"
        memory: 3500Mi
xray:
  ##############################################################
  # The large sizing
  # This size is intended for large organizations. It can be increased with adding replicas or moving to the xlarge sizing
  ##############################################################
  
  replicaCount: 2
  databaseUpgradeReady: true
  waitForDatabase: true
  unifiedUpgradeAllowed: true
  
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 6
    targetCPUUtilizationPercentage: 200
    targetMemoryUtilizationPercentage: 800
  
  xray:
    podAntiAffinity:
      ## Valid values are "soft" or "hard"; any other value indicates no anti-affinity
      type: "hard"
      topologyKey: "kubernetes.io/hostname"
  
  analysis:
    resources:
      requests:
        cpu: "100m"
        memory: 250Mi
      limits:
        # cpu: "4"
        memory: 10Gi
  
  indexer:
    resources:
      requests:
        cpu: "300m"
        memory: 550Mi
      limits:
        # cpu: "6"
        memory: 8Gi
  
  persist:
    resources:
      requests:
        cpu: "100m"
        memory: 250Mi
      limits:
        # cpu: "4"
        memory: 8Gi
  
  server:
    resources:
      requests:
        cpu: "200m"
        memory: 500Mi
      limits:
        # cpu: "4"
        memory: 6Gi
  
  router:
    resources:
      requests:
        cpu: "60m"
        memory: 100Mi
      limits:
        # cpu: "1"
        memory: 1Gi
  
  observability:
    resources:
      requests:
        cpu: "10m"
        memory: 25Mi
      limits:
        # cpu: "1"
        memory: 250Mi
  
  panoramic:
    enabled: false
    resources:
      requests:
        cpu: "100m"
        memory: 250Mi
      limits:
        # cpu: "4"
        memory: 8Gi
  
  sbom:
    enabled: false
    resources:
      requests:
        cpu: "100m"
        memory: 250Mi
      limits:
        # cpu: "4"
        memory: 10Gi
  
  policyenforcer:
    resources:
      requests:
        memory: "250Mi"
        cpu: "60m"
      limits:
        memory: "8Gi"
        cpu: "4"
  
  # PostgreSQL
  ## Configuration values for the postgresql dependency
  ## ref: https://github.com/kubernetes/charts/blob/master/stable/postgresql/README.md
  ##
  postgresql:
    resources:
      requests:
        memory: 32Gi
        cpu: "16"
      limits:
        memory: 32Gi
        # cpu: "32"
    primary:
      extendedConfiguration: |
        max_connections = 600
        listen_addresses = '*'
      affinity:
        # Require PostgreSQL pod to run on a different node than Xray pods
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - xray
              topologyKey: kubernetes.io/hostname
  
  rabbitmq:
    extraConfiguration: |-
      vm_memory_high_watermark.absolute = 3G
      {{- if not .Values.global.xray.rabbitmq.haQuorum.enabled }}
      raft.wal_max_size_bytes = 1048576
      {{- end }}
    resources:
      requests:
        cpu: "200m"
        memory: 500Mi
      limits:
        # cpu: "2"
        memory: 4Gi
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
                - key: app
                  operator: In
                  values:
                    - xray
            topologyKey: kubernetes.io/hostname
distribution:
  ##############################################################
  # The large sizing
  # This is identical to the medium sizing, but with an extra Distribution replica
  ##############################################################
  
  unifiedUpgradeAllowed: true
  databaseUpgradeReady: true
  
  replicaCount: 2
  
  distribution:
    resources:
      requests:
        cpu: 100m
        memory: 700Mi
      limits:
        # cpu: "3"
        memory: 1200Mi
  
    extraEnvironmentVariables:
      - name: MALLOC_ARENA_MAX
        value: "2"
  
    javaOpts:
      other: >
        -XX:InitialRAMPercentage=30
        -XX:MaxRAMPercentage=60
        -XX:+UseStringDeduplication
        -XX:MaxMetaspaceSize=300m
        -Djdk.nio.maxCachedBufferSize=262144
        -XX:MaxDirectMemorySize=256m
  
  router:
    resources:
      requests:
        cpu: 50m
        memory: 100Mi
      limits:
        # cpu: "1"
        memory: 1Gi
  
  observability:
    resources:
      requests:
        cpu: 30m
        memory: 30Mi
      limits:
        # cpu: "1"
        memory: 50Mi
  
  redis:
    resources:
      requests:
        cpu: 30m
        memory: 50Mi
      limits:
        # cpu: "1"
        memory: 150Mi
  
  postgresql:
    primary:
      extendedConfiguration: |
        max_connections = 100
      affinity:
        # Require PostgreSQL pod to run on a different node than distribution pods
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - distribution
              topologyKey: kubernetes.io/hostname
      resources:
        requests:
          cpu: "100m"
          memory: 250Mi
        limits:
          # cpu: "2"
          memory: 1Gi

