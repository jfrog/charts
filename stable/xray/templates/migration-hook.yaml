{{- if .Values.rabbitmq.enabled }}
{{- if and (not .Values.rabbitmq.migration.enabled) (not .Values.rabbitmq.rabbitmqUpgradeReady)  }}
    {{- fail "Rabbitmq migration flag is disabled. Please enable the rabbitmq.rabbitmqUpgradeReady flag after manually enabling the feature flags in rabbitmq" }}
{{- end }}
{{- if eq (include "xray.rabbitmq.migration.isHookRegistered" .) "true" }}
{{- if .Values.rabbitmq.migration.serviceAccount.create }}
apiVersion: v1
kind: ServiceAccount
metadata:
    labels:
        app: {{ template "xray.name" . }}
        chart: {{ template "xray.chart" . }}
        release: {{ .Release.Name | quote }}
        heritage: {{ .Release.Service | quote }}
    name: {{ template "xray.rabbitmq.migration.serviceAccountName" . }}
    annotations:
        helm.sh/hook: "pre-upgrade"
        helm.sh/hook-weight: "-10"
{{- with .Values.rabbitmq.migration.serviceAccount.annotations }}
{{ toYaml . | indent 8 }}
{{- end }}
automountServiceAccountToken: {{ .Values.rabbitmq.migration.serviceAccount.automountServiceAccountToken }}
{{- end }}
{{- end }}
{{- end }}
---
{{- if .Values.rabbitmq.enabled }}
{{- if eq (include "xray.rabbitmq.migration.isHookRegistered" .) "true" }}
{{- if .Values.rabbitmq.migration.serviceAccount.rbac.create }}
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
    labels:
        app: {{ template "xray.name" . }}
        chart: {{ template "xray.chart" . }}
        release: {{ .Release.Name | quote }}
        heritage: {{ .Release.Service | quote }}
    name: {{ template "xray.rabbitmq.migration.fullname" . }}
    annotations:
        helm.sh/hook: "pre-upgrade"
        helm.sh/hook-weight: "-10"
rules:
{{ toYaml .Values.rabbitmq.migration.serviceAccount.rbac.role.rules }}
{{- end }}
{{- end }}
{{- end }}
---
{{- if .Values.rabbitmq.enabled }}
{{- if eq (include "xray.rabbitmq.migration.isHookRegistered" .) "true" }}
{{- if .Values.rabbitmq.migration.serviceAccount.rbac.create }}
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
    labels:
        app: {{ template "xray.name" . }}
        chart: {{ template "xray.chart" . }}
        release: {{ .Release.Name | quote }}
        heritage: {{ .Release.Service | quote }}
    name: {{ template "xray.rabbitmq.migration.fullname" . }}
    annotations:
        helm.sh/hook: "pre-upgrade"
        helm.sh/hook-weight: "-10"
subjects:
    - kind: ServiceAccount
      name: {{ template "xray.rabbitmq.migration.serviceAccountName" . }}
roleRef:
    kind: Role
    apiGroup: rbac.authorization.k8s.io
    name: {{ template "xray.rabbitmq.migration.fullname" . }}
{{- end }}
{{- end }}
{{- end }}
---
{{- if .Values.rabbitmq.enabled }}
{{- if eq (include "xray.rabbitmq.migration.isHookRegistered" .) "true" }}
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    app: {{ template "xray.name" . }}
    chart: {{ template "xray.chart" . }}
    heritage: {{ .Release.Service }}
    release: {{ .Release.Name }}
  name: {{ template "xray.fullname" . }}-pre-upgrade-hook
  annotations:
    "helm.sh/hook": "pre-upgrade"
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
spec:
  template:
    metadata:
      labels:
        app: {{ template "xray.name" . }}
        chart: {{ template "xray.chart" . }}
        heritage: {{ .Release.Service }}
        release: {{ .Release.Name }}
    spec:
      {{- if .Values.rabbitmq.podSecurityContext.enabled }}
      securityContext: {{- omit .Values.rabbitmq.podSecurityContext "enabled" | toYaml | nindent 8 }}
      {{- end }}
      {{- if or .Values.imagePullSecrets .Values.global.imagePullSecrets }}
      {{- include "xray.imagePullSecrets" . | indent 6 }}
      {{- end }}
      serviceAccountName: {{ template "xray.rabbitmq.migration.serviceAccountName" . }}
      {{- if .Values.rabbitmq.affinity }}
      affinity: {{- include "common.tplvalues.render" (dict "value" .Values.rabbitmq.affinity "context" .) | nindent 8 }}
      {{- end }}
      {{- if .Values.rabbitmq.nodeSelector }}
      nodeSelector: {{- include "common.tplvalues.render" (dict "value" .Values.rabbitmq.nodeSelector "context" .) | nindent 8 }}
      {{- end }}
      {{- if .Values.rabbitmq.tolerations }}
      tolerations: {{- include "common.tplvalues.render" (dict "value" .Values.rabbitmq.tolerations "context" .) | nindent 8 }}
      {{- end }}
      containers:
        - name: pre-upgrade-container
          image: "{{ include "xray.getRegistryByService" (list . "migrationHook") }}/{{ .Values.rabbitmq.migration.image.repository }}:{{ .Values.rabbitmq.migration.image.tag }}"
          imagePullPolicy: IfNotPresent
          {{- if .Values.rabbitmq.resources }}
          resources:
            {{- toYaml .Values.rabbitmq.resources | nindent 12 }}
          {{- end }}
          {{- if .Values.rabbitmq.containerSecurityContext.enabled }}
          securityContext: {{- omit .Values.rabbitmq.containerSecurityContext "enabled" | toYaml | nindent 12 }}
          {{- end }}
          command:
          - bash
          - -c
          - |
            #!/bin/bash
            rabbitMqZeroPodName="{{ .Release.Name }}-{{ template "rabbitmq.name" . }}-0"
            rabbitMqZeroPodStatus=$(kubectl get pods $rabbitMqZeroPodName -n {{ .Release.Namespace }} -o jsonpath='{..status.conditions[?(@.type=="Ready")].status}')
            
            {{- if and .Values.global.xray.rabbitmq.haQuorum.enabled .Values.rabbitmq.migration.removeHaPolicyOnMigrationToHaQuorum.enabled }}
            for (( i=1; i<=6; i++ ))
            do 
              if [ "$rabbitMqZeroPodStatus" = "True" ]; then
                break
              fi
              echo "Waiting for Rabbitmq zero pod $rabbitMqZeroPodName to be in Ready state - iteration $i"
              sleep 5
              rabbitMqZeroPodStatus=$(kubectl get pods $rabbitMqZeroPodName -n {{ .Release.Namespace }} -o jsonpath='{..status.conditions[?(@.type=="Ready")].status}')
            done
            if [ "$rabbitMqZeroPodStatus" != "True" ]; then
              echo "Rabbitmq zero pod $rabbitMqZeroPodName is not in Ready state. Failed to remove mirroring policy 'ha-all'"
              exit 1
            fi
            policyExists=$(kubectl exec -i $rabbitMqZeroPodName -n {{ .Release.Namespace }} -- bash -c "rabbitmqctl list_policies --formatter json | grep -o "'"\"name\":\"ha-all\""'" | wc -l | tr -d '[:space:]'")
            if [ "$?" -ne 0 ]; then
              echo "Failed to check if policy ha-all exists on default vhost" 
              exit 1
            fi
            echo "Policy ha-all exists: $policyExists"
            if [ $policyExists -gt 0 ]; then
              kubectl exec -i $rabbitMqZeroPodName -n {{ .Release.Namespace }} -- rabbitmqctl clear_policy ha-all
              if [ "$?" -ne 0 ]; then
                echo "Failed to delete policy ha-all on default vhost"
                exit 1
              else
                echo "Deleted ha-all policy successfully on default vhost"
              fi
            fi
            {{- end }}
            
            {{- if .Values.rabbitmq.migration.enabled }}
            if [ "$rabbitMqZeroPodStatus" = "True" ]; then
                kubectl exec -i $rabbitMqZeroPodName -n {{ .Release.Namespace }} -- rabbitmqctl enable_feature_flag all
                if [ "$?" -ne 0 ]; then
                    echo "Failed to perform the migration. Please make sure to enable the feature flag in rabbitmq manually [rabbitmqctl enable_feature_flag all] " 
                    exit 1
                else
                    echo Feature flags executed successfully!
                fi
            else
                echo "Rabbitmq zero pod is not in running state. Ignoring feature flag migration for rabbitmq"
            fi
            {{- end }}
            
            {{- if .Values.rabbitmq.migration.deleteStatefulSetToAllowFieldUpdate.enabled }}
            # Checking whether RMQ sts exist as we delete it with --cascade=orphan during upgrade
            # If it exists proceed with the deletion and update the podManagementPolicy field
            # else do nothing as the sts will be automatically created with updated podManagementPolicy during upgrade
            rabbitMqStatefulSetsName=$(kubectl get statefulsets -n {{ .Release.Namespace }} -l "app.kubernetes.io/name={{ template "rabbitmq.name" . }},app.kubernetes.io/instance={{ .Release.Name }}" -o=jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}')
            if [ $? -ne 0 ]; then
              echo "Failed to get rabbitmq statefulset name"
              exit 1
            fi
            rabbitMqStatefulSetName=$(echo $rabbitMqStatefulSetsName | head -n 1)
            if [ -z "$rabbitMqStatefulSetName" ]; then
              echo "Rabbitmq statefulset does not exist"
            fi
            if [ -n "$rabbitMqStatefulSetName" ] && [ -n "{{ .Values.rabbitmq.podManagementPolicy }}" ]; then
              currPodManagementPolicy=$(kubectl get statefulset $rabbitMqStatefulSetName -n {{ .Release.Namespace }} -o=jsonpath='{.spec.podManagementPolicy}')
              if [ $? -ne 0 ]; then
                echo "Failed to get current pod management policy definition"
                exit 1
              fi
              if [ "$currPodManagementPolicy" != "{{ .Values.rabbitmq.podManagementPolicy }}" ]; then
                kubectl delete statefulset $rabbitMqStatefulSetName --cascade=orphan -n {{ .Release.Namespace }}
                if [ $? -ne 0 ]; then
                  echo "Failed to delete statefulset $rabbitMqStatefulSetName to allow update of podManagementDefinition field: [kubectl delete statefulset STATEFULSET_NAME --cascade=orphan]"
                  exit 1
                fi
                echo "Deleted statefulset $rabbitMqStatefulSetName successfully"
              else
                echo "Field podManagementPolicy of statefulset $rabbitMqStatefulSetName has not changed"
              fi
            else
              echo "rabbitmq.podManagementPolicy is not set"
            fi
            {{- end }}
      restartPolicy: Never
      terminationGracePeriodSeconds: 0
{{- end }}
---
{{- if and (eq (include "xray.rabbitmq.migration.isHookRegistered" .) "true") (.Values.global.xray.rabbitmq.haQuorum.enabled) }}
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    app: {{ template "xray.name" . }}
    chart: {{ template "xray.chart" . }}
    heritage: {{ .Release.Service }}
    release: {{ .Release.Name }}
  name: {{ template "xray.fullname" . }}-post-upgrade-hook
  annotations:
    "helm.sh/hook": "post-upgrade"
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
spec:
  template:
    metadata:
      labels:
        app: {{ template "xray.name" . }}
        chart: {{ template "xray.chart" . }}
        heritage: {{ .Release.Service }}
        release: {{ .Release.Name }}
    spec:
      {{- if .Values.rabbitmq.podSecurityContext.enabled }}
      securityContext: {{- omit .Values.rabbitmq.podSecurityContext "enabled" | toYaml | nindent 8 }}
      {{- end }}
      {{- if or .Values.imagePullSecrets .Values.global.imagePullSecrets }}
      {{- include "xray.imagePullSecrets" . | indent 6 }}
      {{- end }}
      serviceAccountName: {{ template "xray.rabbitmq.migration.serviceAccountName" . }}
      {{- if .Values.rabbitmq.affinity }}
      affinity: {{- include "common.tplvalues.render" (dict "value" .Values.rabbitmq.affinity "context" .) | nindent 8 }}
      {{- end }}
      {{- if .Values.rabbitmq.nodeSelector }}
      nodeSelector: {{- include "common.tplvalues.render" (dict "value" .Values.rabbitmq.nodeSelector "context" .) | nindent 8 }}
      {{- end }}
      {{- if .Values.rabbitmq.tolerations }}
      tolerations: {{- include "common.tplvalues.render" (dict "value" .Values.rabbitmq.tolerations "context" .) | nindent 8 }}
      {{- end }}
      containers:
        - name: post-upgrade-container
          image: "{{ include "xray.getRegistryByService" (list . "migrationHook") }}/{{ .Values.rabbitmq.migration.image.repository }}:{{ .Values.rabbitmq.migration.image.tag }}"
          imagePullPolicy: IfNotPresent
          {{- if .Values.rabbitmq.resources }}
          resources:
            {{- toYaml .Values.rabbitmq.resources | nindent 12 }}
          {{- end }}
          {{- if .Values.rabbitmq.containerSecurityContext.enabled }}
          securityContext: {{- omit .Values.rabbitmq.containerSecurityContext "enabled" | toYaml | nindent 12 }}
          {{- end }}
          command:
            - bash
            - -c
            - |
              #!/bin/bash
              rabbitMqZeroPodName="{{ .Release.Name }}-{{ template "rabbitmq.name" . }}-0"

              # Sleep for 2 minutes to ensure that all pods are scheduled so that they don't get missed out while checking the pod status.
              # This is needed as the RabbitMQ cluster needs to be up and running before we can perform any operations on it.
              # Based on extensive experimentation, it can take some time before all pods are up and running.
              echo "Sleeping for 2 minutes to ensure all pods are scheduled"
              sleep 120

              maxRetries=10

              for (( retryCount=0; retryCount<maxRetries; retryCount++ )); do
                podStatuses=$(kubectl get pods -l app.kubernetes.io/name=rabbitmq-ha -l app.kubernetes.io/instance={{ .Release.Namespace }}-xray -o json | jq '[.items[].status.phase]')
                podStatuses=($(echo "$podStatuses" | jq -r '.[]'))

                numPods=${#podStatuses[@]}

                if [ $numPods -eq 0 ]; then
                  echo "No pods found. Exiting with status code 1"
                  exit 1
                fi

                allRunning=true
                for status in "${podStatuses[@]}"; do
                  if [[ "$status" != "Running" ]]; then
                    allRunning=false
                    break
                  fi
                done

                if $allRunning; then
                  echo "All pods are running. Exiting loop."
                  break
                fi

                echo "Not all pods are 'Running'. Retrying in 1 minute..."
                sleep 60
              done

              if [[ $retryCount -eq $maxRetries ]]; then
                  echo "Max retries reached while waiting for all pods to run."
                  exit 1
              fi

              # runningNodeCount returns the number of RabbitMQ nodes running in the cluster
              runningNodeCount=$(kubectl exec -i $rabbitMqZeroPodName -n {{ .Release.Namespace }} -c rabbitmq -- bash -c "rabbitmqctl cluster_status --formatter=json" | jq .running_nodes | jq '. | length')
              if [ $? -ne 0 ]; then
                echo "Failed to get the number of running nodes in the cluster"
                exit 1
              fi

              echo $runningNodeCount

              # queueReplicaCount returns the number of replicas for the queue 'indexRegular'. 'indexRegular' is used as it is a permanent queue.
              # This is used to determine the number of replicas for all the queues. Based on this number and the runningNodeCount,
              # the script will either shrink or grow the number of replicas for the queues.
              queueReplicaCount=$(kubectl exec -i $rabbitMqZeroPodName -n {{ .Release.Namespace }} -c rabbitmq -- bash -c "rabbitmqctl list_queues name,type,leader,members --vhost={{ .Values.global.xray.rabbitmq.haQuorum.vhost }} --formatter=json" | jq '.[] | select(.name | test("^indexRegular$")) | .members | length')
              if [ $? -ne 0 ]; then
                echo "Failed to get the number of running replicas for the queues"
                exit 1
              fi
              echo $queueReplicaCount

              if [ "$queueReplicaCount" -gt "$runningNodeCount" ]; then
                echo "Needed to shrink quorum queues before scaling down nodes. Please scale up the nodes, shrink the queues and then scale down the nodes"
              elif [ "$queueReplicaCount" -lt "$runningNodeCount" ]; then
                echo "Need to grow quorum queues on scaled up nodes"

                for ((i = (queueReplicaCount + 1); i <= runningNodeCount; i++)); do
                  echo "Growing node rabbit@{{ .Release.Namespace }}-xray-rabbitmq-ha-$(($i-1))"
                  kubectl exec -i $rabbitMqZeroPodName -n {{ .Release.Namespace }} -c rabbitmq -- bash -c "rabbitmq-queues grow rabbit@{{ .Release.Namespace }}-xray-rabbitmq-ha-$(($i-1)).{{ .Release.Namespace }}-xray-rabbitmq-ha-headless.{{ .Release.Namespace }}.svc.cluster.local all --vhost-pattern={{ .Values.global.xray.rabbitmq.haQuorum.vhost }} --queue-pattern \"(.*)\""
                  if [ $? -ne 0 ]; then
                    echo "Failed to grow queues on node rabbit@{{ .Release.Namespace }}-xray-rabbitmq-ha-$(($i-1))"
                    exit 1
                  fi
                done
              else
                echo "Growing or shrinking of queues not needed as queue replica count matches the number of nodes"
              fi
      restartPolicy: Never
      terminationGracePeriodSeconds: 0
{{- end }}
{{- end }}
