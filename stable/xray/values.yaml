# Default values for Xray HA.
# This is a YAML-formatted file.
# Beware when changing values here. You should know what you are doing!
# Access the values with {{ .Values.key.subkey }}

global: {}
  # imagePullSecrets:
  #   - myRegistryKeySecretName
  # joinKey:
  # masterKey:
  # joinKeySecretName:
  # masterKeySecretName:

  # customInitContainersBegin: |

  # customInitContainers: |

  # customVolumes: |

  # customVolumeMounts: |

  # customSidecarContainers: |


initContainerImage: docker.bintray.io/alpine:3.12
imagePullPolicy: IfNotPresent

# Init containers
initContainers:
  resources: {}
#    requests:
#      memory: "64Mi"
#      cpu: "10m"
#    limits:
#      memory: "128Mi"
#      cpu: "250m"

# For supporting pulling from private registries
imagePullSecrets:
  # - myRegistryKeySecretName

replicaCount: 1

xray:
  name: xray
  labels: {}
  persistence:
    mountPath: /var/opt/jfrog/xray

  ## Xray requires a unique master key
  ## You can generate one with the command:
  ## 'openssl rand -hex 32'
  ## Pass it to helm with '--set xray.masterKey=${MASTER_KEY}'
  ## IMPORTANT: You should NOT use the example masterKey for a production deployment!
  masterKey: FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
  ## Alternatively, you can use a pre-existing secret with a key called master-key by specifying masterKeySecretName
  # masterKeySecretName:

  ## Join Key to connect to main Artifactory. Mandatory
  joinKey: EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE
  ## Alternatively, you can use a pre-existing secret with a key called join-key by specifying joinKeySecretName
  # joinKeySecretName:
  ## If false, all service console logs will not redirect to a common console.log
  consoleLog: false
  ## Artifactory URL . Mandatory
  jfrogUrl:

  ## Mongo details are used only for Manual migration of data from Mongo to Postgres in Xray 2.x to 3.x
  # mongoUrl:
  # mongoUsername:
  # mongoPassword:

  systemYaml: |
    configVersion: 1
    shared:
      logging:
        consoleLog:
          enabled: {{ .Values.xray.consoleLog }}
      jfrogUrl: "{{ tpl (required "\n\nxray.jfrogUrl or global.jfrogUrl is required! This allows to connect to Artifactory.\nYou can copy the JFrog URL from Admin > Security > Settings" (include "xray.jfrogUrl" .)) . }}"
      database:
      {{- if .Values.postgresql.enabled }}
        type: "postgresql"
        driver: "org.postgresql.Driver"
        username: "{{ .Values.postgresql.postgresqlUsername }}"
        url: "postgres://{{ .Release.Name }}-postgresql:{{ .Values.postgresql.service.port }}/{{ .Values.postgresql.postgresqlDatabase }}?sslmode=disable"
      {{- else }}
        type: {{ .Values.database.type }}
        driver: {{ .Values.database.driver }}
      {{- end }}
      {{- if and (not .Values.rabbitmq.enabled) (not .Values.common.rabbitmq.connectionConfigFromEnvironment) }}
      rabbitMq:
        url: "{{ tpl .Values.rabbitmq.external.url . }}"
        username: "{{ .Values.rabbitmq.external.username }}"
        password: "{{ .Values.rabbitmq.external.password }}"
        erlangCookie:
          value: "{{ .Values.rabbitmq.external.erlangCookie }}"
      {{- end }}
      {{- if .Values.xray.mongoUrl }}
      mongo:
        url: "{{ .Values.xray.mongoUrl }}"
        username: "{{ .Values.xray.mongoUsername }}"
        password: "{{ .Values.xray.mongoPassword }}"
      {{- end }}
    server:
      mailServer: "{{ .Values.server.mailServer }}"
      indexAllBuilds: "{{ .Values.server.indexAllBuilds }}"

## Role Based Access
## Ref: https://kubernetes.io/docs/admin/authorization/rbac/
rbac:
  create: true
  role:
    ## Rules to create. It follows the role specification
    rules:
    - apiGroups:
      - ''
      resources:
      - services
      - endpoints
      - pods
      verbs:
      - get
      - watch
      - list

networkpolicy:
  # Allows all ingress and egress
  - name: xray
    podSelector:
      matchLabels:
        app: xray
    egress:
      - {}
    ingress:
      - {}
  # Uncomment to allow only xray pods to communicate with postgresql (if postgresql.enabled is true)
  # - name: postgres
  #   podSelector:
  #     matchLabels:
  #       app: postgresql
  #   ingress:
  #   - from:
  #     - podSelector:
  #         matchLabels:
  #           app: xray
  # Uncomment to allow only xray pods to communicate with rabbitmq (if rabbitmq.enabled is true)
  # - name: rabbitmq
  #   podSelector:
  #     matchLabels:
  #       app: rabbitmq
  #   ingress:
  #   - from:
  #     - podSelector:
  #         matchLabels:
  #           app: xray
  # Uncomment to allow only xray pods to communicate with rabbitmq-ha (if rabbitmq-ha.enabled is true)
  # - name: rabbitmq-ha
  #   podSelector:
  #     matchLabels:
  #       app: rabbitmq-ha
  #   ingress:
  #   - from:
  #     - podSelector:
  #         matchLabels:
  #           app: xray

## Affinity rules
nodeSelector: {}
affinity: {}
tolerations: []

## Apply horizontal pod auto scaling on Xray pods
## Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 3
  targetCPUUtilizationPercentage: 70

## Service Account
## Ref: https://kubernetes.io/docs/admin/service-accounts-admin/
##
serviceAccount:
  create: true
  ## The name of the ServiceAccount to use.
  ## If not set and create is true, a name is generated using the fullname template
  name:

# PostgreSQL
## Configuration values for the postgresql dependency
## ref: https://github.com/kubernetes/charts/blob/master/stable/postgresql/README.md
##
postgresql:
  enabled: true
  image:
    registry: docker.bintray.io
    repository: bitnami/postgresql
    tag: 12.3.0-debian-10-r71
  postgresqlUsername: xray
  postgresqlPassword: ""
  postgresqlDatabase: xraydb
  postgresqlExtendedConf:
    listenAddresses: "'*'"
    maxConnections: "500"
  service:
    port: 5432
  persistence:
    enabled: true
    size: 50Gi
    existingClaim:
  master:
    nodeSelector: {}
    affinity: {}
    tolerations: []
  slave:
    nodeSelector: {}
    affinity: {}
    tolerations: []
  resources: {}
  #  requests:
  #    memory: "1Gi"
  #    cpu: "250m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "1"

## If NOT using the PostgreSQL in this chart (postgresql.enabled=false),
database:
  type: "postgresql"
  driver: "org.postgresql.Driver"
  ## If you would like this chart to create the secret containing the database url, user, password - use these below values
  url:
  user:
  password:
  ## If you have existing Kubernetes secrets containing db credentials, use
  ## these values
  secrets: {}
  #  user:
  #    name: "xray-database-creds"
  #    key: "db-user"
  #  password:
  #    name: "xray-database-creds"
  #    key: "db-password"
  #  url:
  #    name: "xray-database-creds"
  #    key: "db-url"

# RabbitMQ HA
## Configuration values for the rabbitmq-ha dependency
## ref: https://github.com/kubernetes/charts/blob/master/stable/rabbitmq-ha/README.md
##
rabbitmq-ha:
  enabled: true
  replicaCount: 1
  image:
    repository: docker.bintray.io/rabbitmq
    tag: 3.8.7-alpine
  rabbitmqUsername: guest
  rabbitmqPassword: ""
  ## Alternatively, you can use a pre-existing secret with a key called rabbitmq-password by specifying existingSecret
  # existingSecret: <name-of-existing-secret>
  rabbitmqErlangCookie: XRAYRABBITMQCLUSTER
  rabbitmqMemoryHighWatermark: 500MB
  rabbitmqNodePort: 5672
  definitions:
    policies: |-
      {
        "name": "ha-all",
        "apply-to": "all",
        "pattern": ".*",
        "vhost": "/",
        "definition": {
          "ha-mode": "all",
          "ha-sync-mode": "automatic",
        }
      }
  resources: {}
  #  requests:
  #    memory: "250Mi"
  #    cpu: "100m"
  #  limits:
  #    memory: "550Mi"
  #    cpu: "200m"
  persistentVolume:
    enabled: true
    size: 20Gi
  rbac:
    create: true
  nodeSelector: {}
  tolerations: []
  ## Configuration values for the prometheus
  prometheus:
    operator:
      enabled: false

# RabbitMQ
## Configuration values for the rabbitmq dependency
## ref: https://github.com/bitnami/charts/blob/master/bitnami/rabbitmq/README.md
##
rabbitmq:
  enabled: false
  replicaCount: 1
  rbac:
    create: true
  image:
    registry: docker.bintray.io
    repository: bitnami/rabbitmq
    tag: 3.8.7-debian-10-r3
  auth:
    username: guest
    password: ""
    ## Alternatively, you can use a pre-existing secret with a key called rabbitmq-password by specifying existingPasswordSecret
    # existingPasswordSecret: <name-of-existing-secret>
    erlangCookie: XRAYRABBITMQCLUSTER
    # existingErlangSecret: <name-of-existing-secret>
  maxAvailableSchedulers: null
  onlineSchedulers: null
  ## Additional environment variables to set
  ## ref: https://github.com/bitnami/charts/tree/master/bitnami/rabbitmq#adding-extra-environment-variables
  extraEnvVars:
    - name: RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS
      value: "+S 2:2 +sbwt none +sbwtdcpu none +sbwtdio none"
  service:
    port: 5672
  external:
    username:
    password:
    url:
    erlangCookie:
  persistence:
    enabled: true
    accessMode: ReadWriteOnce
    size: 20Gi
  ## Load Definitions - https://www.rabbitmq.com/management.html#load-definitions
  # ref : https://github.com/bitnami/charts/tree/master/bitnami/rabbitmq#load-definitions
  extraSecrets:
    load-definition:
      load_definition.json: |
        {
          "vhosts": [
            {
              "name": "/"
            }
          ],
          "policies": [
            {
              "name": "ha-all",
              "apply-to": "all",
              "pattern": ".*",
              "vhost": "/",
              "definition": {
                "ha-mode": "all",
                "ha-sync-mode": "automatic",
              }
            }
          ]
        }
  loadDefinition:
    enabled: true
    existingSecret: load-definition
  extraConfiguration: |
    management.load_definitions = /app/load_definition.json
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Common Xray settings
common:
  ## Note that by default we use appVersion to get image tag
  # xrayVersion:
  xrayUserId: 1035
  xrayGroupId: 1035

  # Xray configuration to be written to xray_config.yaml
  xrayConfig:
    stdOutEnabled: true
    indexAllBuilds: false
    support-router: true

  # Use rabbitmq connection config from environment variables.
  # If false, then connection details should be set directly in system.yaml (systemYaml section).
  # When using external rabbitmq, set this to false
  rabbitmq:
    connectionConfigFromEnvironment: true

  ## Custom command to run before Xray startup. Runs BEFORE any microservice-specific preStartCommand
  preStartCommand:

  ## Add custom volumes
  customVolumes: |
  #  - name: custom-script
  #    configMap:
  #      name: custom-script

  ## Add custom volumesMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: /scripts/script.sh
  #    subPath: script.sh

  # Add any list of configmaps to Xray
  configMaps: |
  #  posthook-start.sh: |-
  #    echo "This is a post start script"
  #  posthook-end.sh: |-
  #    echo "This is a post end script"

  ## Add custom init containers execution before predefined init containers
  customInitContainersBegin: |
  #  - name: "custom-setup"
  #    image: "{{ .Values.initContainerImage }}"
  #    imagePullPolicy: "{{ .Values.imagePullPolicy }}"
  #    command:
  #      - 'sh'
  #      - '-c'
  #      - 'touch {{ .Values.common.xrayConfigPath }}/example-custom-setup'
  #    volumeMounts:
  #      - mountPath: "{{ .Values.common.xrayConfigPath }}"
  #        name: data-volume

  ## Add custom init containers execution after predefined init containers
  customInitContainers: |
  #  - name: "custom-setup"
  #    image: "{{ .Values.initContainerImage }}"
  #    imagePullPolicy: "{{ .Values.imagePullPolicy }}"
  #    command:
  #      - 'sh'
  #      - '-c'
  #      - 'touch {{ .Values.xray.persistence.mountPath }}/example-custom-setup'
  #    volumeMounts:
  #      - mountPath: "{{ .Values.xray.persistence.mountPath }}"
  #        name: data-volume

  ## Add custom sidecar containers
  # - The provided example uses a custom volume (customVolumes)
  # - The provided example shows running container as root (id 0)
  customSidecarContainers: |
  #  - name: "sidecar-list-etc"
  #    image: "{{ .Values.initContainerImage }}"
  #    imagePullPolicy: "{{ .Values.imagePullPolicy }}"
  #    securityContext:
  #      allowPrivilegeEscalation: false
  #    command:
  #      - 'sh'
  #      - '-c'
  #      - 'sh /scripts/script.sh'
  #    volumeMounts:
  #      - mountPath: "{{ .Values.xray.persistence.mountPath }}"
  #        name: volume
  #      - mountPath: "/scripts/script.sh"
  #        name: custom-script
  #        subPath: script.sh
  #    resources:
  #      requests:
  #        memory: "32Mi"
  #        cpu: "50m"
  #      limits:
  #        memory: "128Mi"
  #        cpu: "100m"

  persistence:
    enabled: true
    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    # existingClaim:

    accessMode: ReadWriteOnce
    ## Container storage limit if persistence.enabled: false
    ## Otherwise PVC size
    size: 50Gi
    ## server data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"


analysis:
  name: xray-analysis
  ## Note that by default we use appVersion to get image tag/version
  image:
    registry: docker.bintray.io
    repository: jfrog/xray-analysis
    # tag:
  internalPort: 7000
  externalPort: 7000
  annotations: {}

  ## Add custom volumesMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: /scripts/script.sh
  #    subPath: script.sh

  livenessProbe:
    enabled: true
    config: |
      httpGet:
        path: /api/v1/system/ping
        port: {{ .Values.analysis.internalPort }}
      initialDelaySeconds: 90
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
      successThreshold: 1

  readinessProbe:
    enabled: true
    config: |
      httpGet:
        path: /api/v1/system/ping
        port: {{ .Values.analysis.internalPort }}
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 10
      successThreshold: 1

  ## Container storage limit
  persistence:
    size: 10Gi

  ## Custom command to run before Xray Analysis startup. Runs AFTER the common.preStartCommand
  preStartCommand:

  resources: {}
  #  requests:
  #    memory: "1Gi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "1"

indexer:
  name: xray-indexer
  ## Note that by default we use appVersion to get image tag/version
  image:
    registry: docker.bintray.io
    repository: jfrog/xray-indexer
    # tag:
  internalPort: 7002
  externalPort: 7002
  annotations: {}

  ## Add custom volumesMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: /scripts/script.sh
  #    subPath: script.sh

  livenessProbe:
    enabled: true
    config: |
      httpGet:
        path: /api/v1/system/ping
        port: {{ .Values.indexer.internalPort }}
      initialDelaySeconds: 90
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
      successThreshold: 1

  readinessProbe:
    enabled: true
    config: |
      httpGet:
        path: /api/v1/system/ping
        port: {{ .Values.indexer.internalPort }}
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 10
      successThreshold: 1

  ## Custom command to run before Xray Indexer startup. Runs AFTER the common.preStartCommand
  preStartCommand:

  resources: {}
  #  requests:
  #    memory: "1Gi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "1"

persist:
  name: xray-persist
  ## Note that by default we use appVersion to get image tag/version
  image:
    registry: docker.bintray.io
    repository: jfrog/xray-persist
    # tag:
  internalPort: 7003
  externalPort: 7003
  annotations: {}

  ## Add custom volumesMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: /scripts/script.sh
  #    subPath: script.sh

  livenessProbe:
    enabled: true
    config: |
      httpGet:
        path: /api/v1/system/ping
        port: {{ .Values.persist.internalPort }}
      initialDelaySeconds: 90
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
      successThreshold: 1

  readinessProbe:
    enabled: true
    config: |
      httpGet:
        path: /api/v1/system/ping
        port: {{ .Values.persist.internalPort }}
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 10
      successThreshold: 1

  ## Container storage limit
  persistence:
    size: 10Gi

  ## Custom command to run before Xray Persist startup. Runs AFTER the common.preStartCommand
  preStartCommand:

  resources: {}
  #  requests:
  #    memory: "1Gi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "1"

server:
  name: xray-server
  ## Note that by default we use appVersion to get image tag/version
  image:
    registry: docker.bintray.io
    repository: jfrog/xray-server
    # tag:
  internalPort: 8000
  externalPort: 80
  annotations: {}

  ## Add custom volumesMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: /scripts/script.sh
  #    subPath: script.sh

  service:
    type: ClusterIP
    name: xray
    annotations: {}

  livenessProbe:
    enabled: true
    config: |
      httpGet:
        path: /api/v1/system/ping
        port: {{ .Values.server.internalPort }}
      initialDelaySeconds: 90
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
      successThreshold: 1

  readinessProbe:
    enabled: true
    config: |
      httpGet:
        path: /api/v1/system/ping
        port: {{ .Values.server.internalPort }}
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 10
      successThreshold: 1

  ## Custom command to run before Xray Server startup. Runs AFTER the common.preStartCommand
  preStartCommand:

  resources: {}
  #  requests:
  #    memory: "1Gi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "1"

router:
  name: router
  image:
    registry: docker.bintray.io
    repository: jfrog/router
    tag: 1.4.4
    imagePullPolicy: IfNotPresent
  internalPort: 8082
  externalPort: 8082
  tlsEnabled: false
  resources: {}
  #  requests:
  #    memory: "100Mi"
  #    cpu: "100m"
  #  limits:
  #    memory: "1Gi"
  #    cpu: "1"

  annotations: {}

  ## Add custom volumesMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: /scripts/script.sh
  #    subPath: script.sh

  livenessProbe:
    enabled: true
    config: |
      httpGet:
        path: /router/api/v1/system/health
        scheme: {{ include "xray.scheme" . | upper }}
        port: {{ .Values.router.internalPort }}
      initialDelaySeconds: 90
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
      successThreshold: 1

  readinessProbe:
    enabled: true
    config: |
      httpGet:
        path: /router/api/v1/system/health
        scheme: {{ include "xray.scheme" . | upper }}
        port: {{ .Values.router.internalPort }}
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 10
      successThreshold: 1

  persistence:
    accessMode: ReadWriteOnce
    mountPath: "/var/opt/jfrog/router"
    size: 5Gi

  # Add any of the loggers to a sidecar if you want to be able to see them with kubectl logs or a log collector in your k8s cluster
  loggers: []

# Filebeat Sidecar container
## The provided filebeat configuration is for Xray logs. It assumes you have a logstash installed and configured properly.
filebeat:
  enabled: false
  name: xray-filebeat
  image:
    repository: "docker.elastic.co/beats/filebeat"
    version: 7.9.2
  logstashUrl: "logstash:5044"

  annotations: {}

  terminationGracePeriod: 10

  livenessProbe:
    exec:
      command:
        - sh
        - -c
        - |
          #!/usr/bin/env bash -e
          curl --fail 127.0.0.1:5066
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5

  readinessProbe:
    exec:
      command:
        - sh
        - -c
        - |
          #!/usr/bin/env bash -e
          filebeat test output
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5

  resources: {}
  #  requests:
  #    memory: "100Mi"
  #    cpu: "100m"
  #  limits:
  #    memory: "100Mi"
  #    cpu: "100m"

  filebeatYml: |
    logging.level: info
    path.data: {{ .Values.xray.persistence.mountPath }}/log/filebeat
    name: xray-filebeat
    queue.spool: ~
    filebeat.inputs:
    - type: log
      enabled: true
      close_eof: ${CLOSE:false}
      paths:
         - {{ .Values.xray.persistence.mountPath }}/log/*.log
      fields:
        service: "jfxr"
        log_type: "xray"
    output:
      logstash:
         hosts: ["{{ .Values.filebeat.logstashUrl }}"]
