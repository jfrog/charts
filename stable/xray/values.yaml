# Default values for Xray HA.
# This is a YAML-formatted file.
# Beware when changing values here. You should know what you are doing!
# Access the values with {{ .Values.key.subkey }}

# General
imagePullPolicy: IfNotPresent
initContainerImage: "alpine:3.8"
imagePullSecrets:

## Role Based Access
## Ref: https://kubernetes.io/docs/admin/authorization/rbac/
rbac:
  create: true
  role:
    ## Rules to create. It follows the role specification
    rules:
    - apiGroups:
      - ''
      resources:
      - services
      - endpoints
      - pods
      verbs:
      - get
      - watch
      - list

ingress:
  enabled: false
  defaultBackend:
    enabled: true
  # Used to create an Ingress record.
  hosts:
    - xray.domain.example
  annotations:
  # kubernetes.io/ingress.class: nginx
  # kubernetes.io/tls-acme: "true"
  tls:
  # Secrets must be manually created in the namespace.
  # - secretName: chart-example-tls
  #   hosts:
  #     - xray.domain.example
# labels:
  # traffic-type: external
  # traffic-type: internal

## Service Account
## Ref: https://kubernetes.io/docs/admin/service-accounts-admin/
##
serviceAccount:
  create: true
  ## The name of the ServiceAccount to use.
  ## If not set and create is true, a name is generated using the fullname template
  name:

# PostgreSQL
## Configuration values for the postgresql dependency
## ref: https://github.com/kubernetes/charts/blob/master/stable/postgresql/README.md
##
postgresql:
  enabled: true
  imageTag: "9.6.11"
  postgresDatabase: "xraydb"
  postgresUser: "xray"
  postgresPassword:
  postgresConfig:
    maxConnections: "500"
  service:
    port: 5432
  persistence:
    enabled: true
    size: 50Gi
    existingClaim:
  resources: {}
  #  requests:
  #    memory: "1Gi"
  #    cpu: "250m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "1"
  nodeSelector: {}
  affinity: {}
  tolerations: []

## Configuration values for the mongodb dependency
## ref: https://github.com/kubernetes/charts/blob/master/stable/mongodb/README.md
##
mongodb:
  enabled: true
  image:
    tag: 3.6.3
    pullPolicy: IfNotPresent
  persistence:
    size: 50Gi
  resources: {}
  #  requests:
  #    memory: "12Gi"
  #    cpu: "200m"
  #  limits:
  #    memory: "12Gi"
  #    cpu: "2"
  ## Make sure the limits.memory value is the same as the requests.memory value!
  ## Make sure the --wiredTigerCacheSizeGB is no more than half the memory limit!
  ## This is critical to protect against OOMKill by Kubernetes!
  mongodbExtraFlags:
  - "--wiredTigerCacheSizeGB=1"
  mongodbRootPassword:
  mongodbUsername: xray
  mongodbPassword:
  mongodbDatabase: xray
#  livenessProbe:
#    initialDelaySeconds: 60
#    periodSeconds: 10
#  readinessProbe:
#    initialDelaySeconds: 30
#    periodSeconds: 30
  nodeSelector: {}
  affinity: {}
  tolerations: []

# RabbitMQ HA
## Configuration values for the rabbitmq-ha dependency
## ref: https://github.com/kubernetes/charts/blob/master/stable/rabbitmq-ha/README.md
##
rabbitmq-ha:
  enabled: true
  replicaCount: 1
  rabbitmqUsername: guest
  rabbitmqPassword:
  rabbitmqErlangCookie: XRAYRABBITMQCLUSTER
  rabbitmqMemoryHighWatermark: 500MB
  rabbitmqNodePort: 5672
  policies: |-
    {
      "name": "ha-all",
      "apply-to": "all",
      "pattern": ".*",
      "vhost": "/",
      "definition": {
        "ha-mode": "all",
        "ha-sync-mode": "automatic",
      }
    }
  resources: {}
  #  requests:
  #    memory: "250Mi"
  #    cpu: "100m"
  #  limits:
  #    memory: "550Mi"
  #    cpu: "200m"
  persistentVolume:
    enabled: true
    size: 20Gi
  rbac:
    create: true
  nodeSelector: {}
  tolerations: []

# RabbitMQ
## Configuration values for the rabbitmq dependency
## ref: https://github.com/kubernetes/charts/blob/master/stable/rabbitmq/README.md
##
rabbitmq:
  enabled: false
  rabbitmqErlangCookie: XRAYRABBITMQCLUSTER
  rabbitmqMemoryHighWatermark: 500MB
  rabbitmqNodePort: 5672
  rabbitmqUsername: user
  rabbitmqPassword:
  persistentVolume:
    enabled: true
    size: 20Gi
  rbac:
    create: true

# Logger containers
logger:
  image:
    repository: busybox
    tag: 1.30

# Common Xray settings
common:
  ## Note that by default we use appVersion to get image tag
  # xrayVersion:
  xrayConfigPath: /var/opt/jfrog/xray/data
  xrayUserId: 1035
  xrayGroupId: 1035
  stdOutEnabled: true
  ## Xray requires a unique master key
  ## You can generate one with the command:
  ## 'openssl rand -hex 32'
  ## Pass it to helm with '--set common.masterKey=${MASTER_KEY}'
  ## IMPORTANT: You should NOT use the example masterKey for a production deployment!
  masterKey: FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

  # Configure Xray to index all the builds in artifactory
  indexAllBuilds: false

  ## Add custom init containers
  customInitContainers: |
  #  - name: "custom-setup"
  #    image: "{{ .Values.initContainerImage }}"
  #    imagePullPolicy: "{{ .Values.imagePullPolicy }}"
  #    command:
  #      - 'sh'
  #      - '-c'
  #      - 'touch {{ .Values.common.xrayConfigPath }}/example-custom-setup'
  #    volumeMounts:
  #      - mountPath: "{{ .Values.common.xrayConfigPath }}"
  #        name: data-volume

# For setting up external services, must pass the connection URL for them
global:
  mongoUrl:
  postgresqlUrl:

analysis:
  name: xray-analysis
  image: docker.bintray.io/jfrog/xray-analysis
  updateStrategy: RollingUpdate
  podManagementPolicy: Parallel
  replicaCount: 1
  internalPort: 7000
  externalPort: 7000
  service:
    type: ClusterIP
  ## Container storage limit
  storage:
    sizeLimit: 10Gi
  # Add any of the loggers to a sidecar if you want to be able to see them with kubectl logs or a log collector in your k8s cluster
  loggers: []
  # - xray_analysis.log
  resources: {}
  #  requests:
  #    memory: "1Gi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "1"
  nodeSelector: {}
  affinity: {}
  tolerations: []

indexer:
  name: xray-indexer
  image: docker.bintray.io/jfrog/xray-indexer
  updateStrategy: RollingUpdate
  podManagementPolicy: Parallel
  replicaCount: 1
  internalPort: 7002
  externalPort: 7002
  service:
    type: ClusterIP
  ## Container storage limit
  storage:
    sizeLimit: 10Gi
  # Add any of the loggers to a sidecar if you want to be able to see them with kubectl logs or a log collector in your k8s cluster
  loggers: []
  # - xray_indexer.log
  resources: {}
  #  requests:
  #    memory: "1Gi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "1"
  nodeSelector: {}
  affinity: {}
  tolerations: []

persist:
  name: xray-persist
  image: docker.bintray.io/jfrog/xray-persist
  updateStrategy: RollingUpdate
  podManagementPolicy: Parallel
  replicaCount: 1
  internalPort: 7003
  externalPort: 7003
  service:
    type: ClusterIP
  ## Container storage limit
  storage:
    sizeLimit: 10Gi
  # Add any of the loggers to a sidecar if you want to be able to see them with kubectl logs or a log collector in your k8s cluster
  loggers: []
  # - xray_persist.log
  resources: {}
  #  requests:
  #    memory: "1Gi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "1"
  nodeSelector: {}
  affinity: {}
  tolerations: []

server:
  name: xray-server
  image: docker.bintray.io/jfrog/xray-server
  updateStrategy: RollingUpdate
  podManagementPolicy: Parallel
  replicaCount: 1
  internalPort: 8000
  externalPort: 80
  service:
    type: LoadBalancer
    name: xray
  ## Container storage limit
  storage:
    sizeLimit: 10Gi
  # Whitelist IPs allowed to LoadBalancer type services
  # Example: loadBalancerSourceRanges={82.82.190.51/32,141.141.8.8/32}
  loadBalancerSourceRanges: []
  # Add any of the loggers to a sidecar if you want to be able to see them with kubectl logs or a log collector in your k8s cluster
  loggers: []
  # - xray_server.log
  resources: {}
  #  requests:
  #    memory: "1Gi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "1"
  nodeSelector: {}
  affinity: {}
  tolerations: []
