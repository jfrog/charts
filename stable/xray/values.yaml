# Default values for Xray HA.
# This is a YAML-formatted file.
# Beware when changing values here. You should know what you are doing!
# Access the values with {{ .Values.key.subkey }}
global:
  # imageRegistry: releases-docker.jfrog.io
  # imagePullSecrets:
  #   - myRegistryKeySecretName
  ## Chart.AppVersion can be overidden using global.versions.xray, common.xrayVersion or image tags
  ## Note: Order of preference is 1) global.versions 2) common.xrayVersion 3) image tags 4) Chart.AppVersion
  versions: {}
    # xray:
    # initContainers:
    # router:
  # joinKey:
  # masterKey:
  # joinKeySecretName:
  # masterKeySecretName:
  # executionServiceAesKey:
  # executionServiceAesKeySecretName:

  ## Note: tags customInitContainersBegin,customInitContainers,customVolumes,customVolumeMounts,customSidecarContainers can be used both from global and application level simultaneously
  # customInitContainersBegin: |

  # customInitContainers: |

  # customVolumes: |

  # customVolumeMounts: |

  # customSidecarContainers: |

  ## certificates added to this secret will be copied to $JFROG_HOME/xray/var/etc/security/keys/trusted directory
  customCertificates:
    enabled: false
    # certificateSecretName:

  ## Applies to xray pods
  nodeSelector: {}

  ## Applies to platform charts.
  rabbitmq:
    auth:
      tls:
        enabled:
        autoGenerated:

  xray:
    # Rabbitmq settings that are specific to Xray
    rabbitmq:
      replicaCount: 1
      haQuorum:
        enabled: false
        waitForPreviousPodsOnInitialStartup: true
        vhost: xray_haq

  autoscaling:
    keda:
      annotations: {}

deployment:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0

## String to partially override xray.fullname template (will maintain the release name)
##
# nameOverride:

## String to fully override xray.fullname template
##
# fullnameOverride:

imagePullPolicy: IfNotPresent

# Init containers
initContainers:
  image:
    registry: releases-docker.jfrog.io
    repository: ubi9/ubi-minimal
    tag: 9.5.1736404155
    pullPolicy: IfNotPresent
  resources:
    requests:
      memory: "50Mi"
      cpu: "10m"
    limits:
      memory: "1Gi"
      cpu: "1"

# For supporting pulling from private registries
imagePullSecrets:
  # - myRegistryKeySecretName

## Xray systemYaml override
## This is for advanced usecases where users wants to provide their own systemYaml for configuring xray
## Refer - https://www.jfrog.com/confluence/display/JFROG/Xray+System+YAML
## Note: This will override existing (default) .Values.xray.systemYaml in values.yaml
## Alternatively, systemYaml can be overidden via customInitContainers using external sources like vaults, external repositories etc. Please refer customInitContainer section below for an example.
## Note: Order of preference is 1) customInitContainers 2) systemYamlOverride existingSecret 3) default systemYaml in values.yaml
systemYamlOverride:
## You can use a pre-existing secret by specifying existingSecret
  existingSecret:
## The dataKey should be the name of the secret data key created.
  dataKey:

replicaCount: 1

## Database configurations
## Use the wait-for-db init container. Set to false to skip
waitForDatabase: true

xray:
  name: xray
  labels: {}
  persistence:
    mountPath: /var/opt/jfrog/xray

  # adding minAvailable for Xray Pod Disruption Budget
  # minAvailable: 1

  # unifiedSecretInstallation flag enables single unified secret holding all the xray internal(chart) secrets, It won't be affecting external secrets.
  ## Note: unifiedSecretInstallation flag is enabled by true by default from chart version 103.91.x, Users can switch to false to continue with the old way of secret creation.
  unifiedSecretInstallation: true

  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  schedulerName:

  # Create a priority class for the Xray pod or use an existing one
  # NOTE - Maximum allowed value of a user defined priority is 1000000000
  priorityClass:
    create: false
    value: 1000000000
    ## Override default name
    # name:
    ## Use an existing priority class
    # existingPriorityClass:

  ## certificates added to this secret will be copied to $JFROG_HOME/xray/var/etc/security/keys/trusted directory
  customCertificates:
    enabled: false
    # certificateSecretName:

  ## Add custom annotations for xray pods
  annotations: {}

  ## Xray requires a unique master key
  ## You can generate one with the command:
  ## 'openssl rand -hex 32'
  ## Pass it to helm with '--set xray.masterKey=${MASTER_KEY}'
  ## IMPORTANT: You should NOT use the example masterKey for a production deployment!
  masterKey: FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
  ## Alternatively, you can use a pre-existing secret with a key called master-key by specifying masterKeySecretName
  # masterKeySecretName:

  ## Join Key to connect to main Artifactory. Mandatory
  joinKey: EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE

  ## Xray AES key used by execution server to the xray server and analysis containers.
  ## You can generate one with the command:
  ## 'openssl rand -hex 16'
  # executionServiceAesKey:
  ## Alternatively, you can use a pre-existing secret with a key called execution-service-aes-key by specifying executionServiceAesKeySecretName
  # executionServiceAesKeySecretName:

  ## Alternatively, you can use a pre-existing secret with a key called join-key by specifying joinKeySecretName
  # joinKeySecretName:
  ## If false, all service console logs will not redirect to a common console.log
  consoleLog: false
  ## Artifactory URL . Mandatory
  jfrogUrl:

  ## Mongo details are used only for Manual migration of data from Mongo to Postgres in Xray 2.x to 3.x
  # mongoUrl:
  # mongoUsername:
  # mongoPassword:

  ## Only used if "affinity" is empty
  podAntiAffinity:
    ## Valid values are "soft" or "hard"; any other value indicates no anti-affinity
    type: "soft"
    topologyKey: "kubernetes.io/hostname"

  # To enable set `.Values.xray.openMetrics.enabled` to `true`
  # Refer - https://www.jfrog.com/confluence/display/JFROG/Open+Metrics
  openMetrics:
    enabled: false
    ## Settings for pushing metrics to Insight - enable filebeat to true
    filebeat:
      enabled: false
      log:
        enabled: false
        ## Log level for filebeat. Possible values: debug, info, warning, or error.
        level: "info"
      ## Elasticsearch details for filebeat to connect
      elasticsearch:
        url: "Elasticsearch url where JFrog Insight is installed For example, http://<ip_address>:8082"
        username: ""
        password: ""

  ## System YAML entries now reside under files/system.yaml.
  ## You can provide the specific values that you want to add or override under 'xray.extraSystemYaml'.
  ## For example:
  ## extraSystemYaml:
  ##  shared:
  ##    logging:
  ##      consoleLog:
  ##        enabled: true
  ## The entries provided under 'xray.extraSystemYaml' are merged with files/system.yaml to create the final system.yaml.
  ## If you have already provided system.yaml under, 'xray.systemYaml', the values in that entry take precedence over files/system.yaml
  ## You can modify specific entries with your own value under `xray.extraSystemYaml`, The values under extraSystemYaml overrides the values under 'xray.systemYaml' and files/system.yaml

  extraSystemYaml: {}
      
  ## systemYaml is intentionally commented and the previous content has been moved under files/system.yaml.
  ## You have to add the all entries of the system.yaml file here, and it overrides the values in files/system.yaml.
  # systemYaml:

  # Sidecar containers for tailing Xray logs
  loggers: []
  # - router-request.log
  # - router-service.log
  # - router-traefik.log
  # - xray-request.log
  # - xray-analysis-service.log
  # - xray-analysis-metrics.log
  # - xray-server-service.log
  # - xray-server-metrics.log
  # - xray-indexer-service.log
  # - xray-indexer-metrics.log
  # - xray-analysis-stack.log
  # - xray-indexer-stack.log
  # - xray-persist-stack.log
  # - xray-persist-metrics.log
  # - xray-server-stack.log
  # - observability-metrics.log
  # - observability-request.log
  # - observability-service.log

  # Loggers containers resources
  loggersResources: {}
  #   requests:
  #     memory: "64Mi"
  #     cpu: "25m"
  #   limits:
  #     memory: "128Mi"
  #     cpu: "50m"

## Role Based Access
## Ref: https://kubernetes.io/docs/admin/authorization/rbac/
rbac:
  create: false
  role:
    ## Rules to create. It follows the role specification
    rules:
    - apiGroups:
      - ''
      resources:
      - services
      - endpoints
      - pods
      - pods/log
      - events
      verbs:
      - get
      - watch
      - list
    - apiGroups:
      - 'batch'
      resources:
      - jobs
      verbs:
      - get
      - watch
      - list
      - create
      - delete

networkpolicy: []
  # Allows all ingress and egress
  # - name: xray
  #   podSelector:
  #     matchLabels:
  #       app: xray
  #   egress:
  #     - {}
  #   ingress:
  #     - {}
  # Uncomment to allow only xray pods to communicate with postgresql (if postgresql.enabled is true)
  # - name: postgres
  #   podSelector:
  #     matchLabels:
  #       app.kubernetes.io/name: postgresql
  #   ingress:
  #   - from:
  #     - podSelector:
  #         matchLabels:
  #           app: xray
  # Uncomment to allow only xray pods to communicate with rabbitmq (if rabbitmq.enabled is true)
  # - name: rabbitmq
  #   podSelector:
  #     matchLabels:
  #       app.kubernetes.io/name: rabbitmq
  #   ingress:
  #   - from:
  #     - podSelector:
  #         matchLabels:
  #           app: xray

## Affinity rules
nodeSelector: {}
affinity: {}
tolerations: []

## Apply horizontal pod auto scaling on Xray pods
## Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 3
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 90
  ## Specify if using the keda hpa or regular basic hpa
  ## Note: keda should be installed on the target cluster
  ## Ref: https://keda.sh/docs/2.10/deploy/
  keda:
    enabled: false
    annotations: {}
    scaleUp:
      stabilizationWindowSeconds: 90
      policies:
        - type: Pods
          value: 3
          periodSeconds: 30
    scaleDown:
      stabilizationWindowSeconds: 90
      policies:
        - type: Pods
          value: 1
          periodSeconds: 30
    pollingInterval: 10
    cooldownPeriod: 10
    queues:
      - name: analysis
        value: "100"
      - name: index
        value: "100"
      - name: persist
        value: "100"
      - name: policyEnforcer
        value: "100"
      - name: impactAnalysis
        value: "100"

## Service Account
## Ref: https://kubernetes.io/docs/admin/service-accounts-admin/
##
serviceAccount:
  create: false
  ## The name of the ServiceAccount to use.
  ## If not set and create is true, a name is generated using the fullname template
  name:
  ## Service Account annotations
  annotations: {}
  ## Explicitly mounts the API credentials for the Service Account
  automountServiceAccountToken: true

## @param podSecurityContext.enabled enable the pod's Security Context
podSecurityContext:
  enabled: true
  runAsNonRoot: true
  runAsUser: 1035
  runAsGroup: 1035
  fsGroup: 1035
  # fsGroupChangePolicy: "Always"
  # seLinuxOptions: {}

## @param containerSecurityContext.enabled enable the container's Security Context
containerSecurityContext:
  enabled: true
  runAsNonRoot: true
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - NET_RAW


# PostgreSQL
## Please note bundled postgresql is not recommended for production use.
## Configuration values for the postgresql dependency
## ref: https://github.com/kubernetes/charts/blob/master/stable/postgresql/README.md
##
postgresql:
  enabled: true
  image:
    registry: releases-docker.jfrog.io
    repository: bitnami/postgresql
    tag: 15.6.0-debian-11-r16
  auth:
    username: "xray"
    password: ""
    database: "xraydb"
  primary:
    extendedConfiguration: |
      max_connections = 1500
      listen_addresses = '*'
    persistence:
      size: 300Gi
    service:
      ports:
        postgresql: 5432
    nodeSelector: {}
    affinity: {}
    tolerations: []
    ## @param primary.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
  readReplicas:
    nodeSelector: {}
    affinity: {}
    tolerations: []

## If NOT using the PostgreSQL in this chart (postgresql.enabled=false),
database:
  type: "postgresql"
  driver: "org.postgresql.Driver"
  ## If you would like this chart to create the secret containing the database url, user, password - use these below values
  url:
  user:
  password:
  ## When working with Azure managed PG you have to provide the actualUsername. The application will pick actualUsername and will use it in some cases where it is needed
  actualUsername:
  ## If you have existing Kubernetes secrets containing db credentials, use
  ## these values
  secrets: {}
  #  user:
  #    name: "xray-database-creds"
  #    key: "db-user"
  #  password:
  #    name: "xray-database-creds"
  #    key: "db-password"
  #  url:
  #    name: "xray-database-creds"
  #    key: "db-url"
  #  actualUsername:
  #    name: "xray-database-creds"
  #    key: "db-actualUsername"

# RabbitMQ
## Configuration values for the rabbitmq dependency
## ref: https://github.com/bitnami/charts/blob/master/bitnami/rabbitmq/README.md
##
rabbitmq:
  enabled: true
  ## Enable the flag if the feature flags in rabbitmq is enabled manually
  rabbitmqUpgradeReady: false
  replicaCount: 1
  rbac:
    create: true
  image:
    registry: releases-docker.jfrog.io
    repository: bitnami/rabbitmq
    tag: 3.13.7-debian-12-r5
  extraPlugins: "rabbitmq_management"

  auth:
    ## Enable encryption to rabbitmq
    ## ref: https://www.rabbitmq.com/ssl.html
    ## @param auth.tls.enabled Enable TLS support on RabbitMQ
    ## @param auth.tls.autoGenerated Generate automatically self-signed TLS certificates
    ## @param auth.tls.failIfNoPeerCert When set to true, TLS connection will be rejected if client fails to provide a certificate
    ## @param auth.tls.sslOptionsVerify Should [peer verification](https://www.rabbitmq.com/ssl.html#peer-verification) be enabled?
    ## @param auth.tls.sslOptionsPassword.enabled Enable usage of password for private Key
    ## @param auth.tls.sslOptionsPassword.existingSecret Name of existing Secret containing the sslOptionsPassword
    ## @param auth.tls.sslOptionsPassword.key Enable Key referring to sslOptionsPassword in Secret specified in auth.tls.sslOptionsPassword.existingSecret
    ## @param auth.tls.sslOptionsPassword.password Use this string as Password. If set, auth.tls.sslOptionsPassword.existingSecret and auth.tls.sslOptionsPassword.key are ignored
    ## @param auth.tls.caCertificate Certificate Authority (CA) bundle content
    ## @param auth.tls.serverCertificate Server certificate content
    ## @param auth.tls.serverKey Server private key content
    ## @param auth.tls.existingSecret Existing secret with certificate content to RabbitMQ credentials
    ## @param auth.tls.existingSecretFullChain Whether or not the existing secret contains the full chain in the certificate (`tls.crt`). Will be used in place of `ca.cert` if `true`.
    ## @param auth.tls.overrideCaCertificate Existing secret with certificate content be mounted instead of the `ca.crt` coming from caCertificate or existingSecret/existingSecretFullChain.
    ##
    tls:
      enabled: false
      # By default TLS certs are autogenerated, if you wish to add your own certs, please set this to false.
      autoGenerated: true
      failIfNoPeerCert: false
      sslOptionsVerify: verify_peer
      failIfNoCert: false
      sslOptionsPassword:
        enabled: false
        existingSecret: ""
        key: ""
        password: ""

      caCertificate:
      serverCertificate:
      serverKey:

      # Rabbitmq tls-certs secret name, as by default it will have {{ .Release.Name }}-rabbitmq-certs.
      existingSecret:
      existingSecretFullChain: false
      overrideCaCertificate: ""
    username: guest
    password: password
    ## @param auth.securePassword Whether to set the RabbitMQ password securely. This is incompatible with loading external RabbitMQ definitions and 'true' when not setting the auth.password parameter.
    ## ref: https://github.com/bitnami/containers/tree/main/bitnami/rabbitmq#environment-variables
    securePassword: false
    ## Alternatively, you can use a pre-existing secret with a key called rabbitmq-password by specifying existingPasswordSecret
    # existingPasswordSecret: <name-of-existing-secret>
    erlangCookie: XRAYRABBITMQCLUSTER
    # existingErlangSecret: <name-of-existing-secret>
  # memoryHighWatermark:
  #   ## @param memoryHighWatermark.enabled Enable configuring Memory high watermark on RabbitMQ
  #   ##
  #   enabled: false
  #   ## @param memoryHighWatermark.type Memory high watermark type. Either `absolute` or `relative`
  #   ##
  #   type: "absolute"
  #   ## Memory high watermark value.
  #   ## @param memoryHighWatermark.value Memory high watermark value
  #   ## The default value of 0.4 stands for 40% of available RAM
  #   ## Note: the memory relative limit is applied to the resource.limits.memory to calculate the memory threshold
  #   ## You can also use an absolute value, e.g.: 256MB
  #   ##
  #   value: 700MB
  # resources:
  #   requests:
  #     memory: "512Mi"
  #     cpu: "500m"
  #   limits:
  #     memory: "1Gi"
  #     cpu: "1"
  ## RabbitMQ maximum available scheduler threads and online scheduler threads. By default it will create a thread per CPU detected, with the following parameters you can tune it manually.
  maxAvailableSchedulers: null
  onlineSchedulers: null
  ## To support upgrade from 3.8.x to 3.11.x , featureFlags are needed
  ## ref: https://blog.rabbitmq.com/posts/2022/07/required-feature-flags-in-rabbitmq-3.11/
  featureFlags: drop_unroutable_metric,empty_basic_get_metric,implicit_default_bindings,maintenance_mode_status,quorum_queue,stream_queue,user_limits,virtual_host_metadata
  ## Additional environment variables to set
  ## ref: https://github.com/bitnami/charts/tree/master/bitnami/rabbitmq#adding-extra-environment-variables
  extraEnvVars:
    - name: RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS
      value: "+S 2:2 +sbwt none +sbwtdcpu none +sbwtdio none"
  service:
    ports:
      amqp: 5672
      amqpTls: 5671
      manager: 15672
  external:
    username:
    password:
    url:
    erlangCookie:
    secrets: {}
    #  username:
    #    name: "xray-rabbitmq-creds"
    #    key: "username"
    #  password:
    #    name: "xray-rabbitmq-creds"
    #    key: "password"
    #  url:
    #    name: "xray-rabbitmq-creds"
    #    key: "url"
  persistence:
    enabled: true
    accessMode: ReadWriteOnce
    size: 20Gi
  ## Load Definitions - https://www.rabbitmq.com/management.html#load-definitions
  # ref : https://github.com/bitnami/charts/tree/master/bitnami/rabbitmq#load-definitions
  extraSecretsPrependReleaseName: true
  extraSecrets:
    load-definition:
      load_definition.json: |
        {
          "permissions": [
            {
              "user": "{{ .Values.auth.username }}",
              "vhost": "/",
              "configure": ".*",
              "write": ".*",
              "read": ".*"
            },
            {
              "user": "{{ .Values.auth.username }}",
              "vhost": "{{ .Values.global.xray.rabbitmq.haQuorum.vhost }}",
              "configure": ".*",
              "write": ".*",
              "read": ".*"
            }
          ],
          "users": [
            {
              "name": "{{ .Values.auth.username }}",
              "password": "{{ .Values.auth.password }}",
              "tags": "administrator"
            }
          ],
          "vhosts": [
            {
              "name": "/"
            },
            {
              "name": "{{ .Values.global.xray.rabbitmq.haQuorum.vhost }}"
            }
          ],
          "policies": [
            {{- if not .Values.global.xray.rabbitmq.haQuorum.enabled }}
            {
              "name": "ha-all",
              "apply-to": "all",
              "pattern": ".*",
              "vhost": "/",
              "definition": {
                "ha-mode": "all",
                "ha-sync-mode": "automatic"
              }
            }
            {{- end }}
          ]
        }
  loadDefinition:
    enabled: true
    existingSecret: '{{ .Release.Name }}-load-definition'
  nodeSelector: {}
  tolerations: []
  affinity: {}
  containerSecurityContext:
    enabled: true
    allowPrivilegeEscalation: false
  ## Upgrade of rabbitmq from 3.8.x to 3.11.x needs the feature flags to be enabled.
  ## Ref: (https://blog.rabbitmq.com/posts/2022/07/required-feature-flags-in-rabbitmq-3.11/
  ## migration enable will perform `rabbitmqctl enable_feature_flag all` command on the existing rabbitmq before starting the upgrade
  migration:
    ## Migration is required to be performed only once hence this option can be disabled once the feature flags are enabled in rabbitmq.
    enabled: true
    ## Another uses of migration hook are:
    ## - Deleting StatefulSet for allowing updating certain fields that require it:
    ##   Changing podManagementPolicy OrderedReady -> Parallel requires deleting stateful set
    ## - Deleting ha-all mirror policy on migrating to Quorum Queues
    deleteStatefulSetToAllowFieldUpdate:
      enabled: false
    removeHaPolicyOnMigrationToHaQuorum:
      enabled: false
    image:
      registry: releases-docker.jfrog.io
      repository: bitnami/kubectl
      tag: 1.32.0
    ## Service account for the pre-upgrade hook to perform rabbitmq migration
    serviceAccount:
      create: true
      ## The name of the ServiceAccount to use.
      ## If not set and create is true, a name is generated using the fullname template
      name:
      ## Explicitly mounts the API credentials for the Service Account
      ## Service Account annotations
      annotations: {}
      automountServiceAccountToken: true
      rbac:
        create: true
        role:
          ## Rules to create. It follows the role specification
          rules:
            - apiGroups:
                - ""
              resources:
                - pods/exec
                - pods
              verbs:
                - create
                - get
                - list
            - apiGroups:
                - "apps"
              resources:
                - statefulsets
              verbs:
                - get
                - list
                - delete


  # This is automatically set based on rabbitmqTLS enabled flag.
  extraConfiguration: |-
    management.listener.ssl = {{ template "xray.rabbitmq.isManagementListenerTlsEnabledInContext" . }}

  initContainers: |
    {{- if and .Values.global.xray.rabbitmq.haQuorum.enabled .Values.global.xray.rabbitmq.haQuorum.waitForPreviousPodsOnInitialStartup }}
    - name: "wait-for-previous-pods"
      image: "{{ template "rabbitmq.image" . }}"
      imagePullPolicy: {{ .Values.image.pullPolicy | quote }}
      env:
        - name: RABBITMQ_ERL_COOKIE
          valueFrom:
            secretKeyRef:
              name: {{ template "rabbitmq.secretErlangName" . }}
              key: rabbitmq-erlang-cookie
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: MY_POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: K8S_SERVICE_NAME
          value: {{ printf "%s-%s" (include "common.names.fullname" .) (default "headless" .Values.servicenameOverride) }}
        {{- if (eq "hostname" .Values.clustering.addressType) }}
        - name: RABBITMQ_NODE_NAME
          value: "rabbit@$(MY_POD_NAME).$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.{{ .Values.clusterDomain }}"
        - name: K8S_HOSTNAME_SUFFIX
          value: ".$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.{{ .Values.clusterDomain }}"
        {{- else }}
        - name: RABBITMQ_NODE_NAME
          value: "rabbit@$(MY_POD_NAME)"
        {{- end }}
        - name: RABBITMQ_MNESIA_DIR
          value: "{{ .Values.persistence.mountPath }}/$(RABBITMQ_NODE_NAME)"
      command:
        - /bin/bash
      args:
        - -ecx
        - |
          echo $HOSTNAME
          if [[ $HOSTNAME == *-0 ]]; then
             exit 0
          fi  
          if [ -d "$RABBITMQ_MNESIA_DIR" ]; then
             exit 0
          fi
    
          # wait for zero pod to start running and accept requests
          zero_pod_name=$(echo $MY_POD_NAME | sed -E "s/-[[:digit:]]$/-0/")
          zero_pod_node_name=$(echo "$RABBITMQ_NODE_NAME" | sed -E "s/^rabbit@$MY_POD_NAME/rabbit@$zero_pod_name/")
          maxIterations=60
          i=1
          while true; do
              rabbitmq-diagnostics -q check_running -n $zero_pod_node_name --longnames --erlang-cookie $RABBITMQ_ERL_COOKIE && \
              rabbitmq-diagnostics -q check_local_alarms -n $zero_pod_node_name --longnames --erlang-cookie $RABBITMQ_ERL_COOKIE && \
              break || sleep 5;
              if [ "$i" == "$maxIterations" ]; then exit 1; fi
              i=$((i+1))
          done;   
    
          # node x waits for x previous nodes to join cluster (since node number is zero based)
          nodeSerialNum=$(echo "$MY_POD_NAME" | grep -o "[0-9]*$")
          timeoutSeconds=180
          rabbitmqctl --erlang-cookie $RABBITMQ_ERL_COOKIE \
                      --node $zero_pod_node_name --longnames  \
                      await_online_nodes $nodeSerialNum \
                      --timeout $timeoutSeconds || exit 1
      {{- if .Values.containerSecurityContext.enabled }}
      securityContext: {{- omit .Values.containerSecurityContext "enabled" | toYaml | nindent 12 }}
      {{- end }}
      volumeMounts:
        - name: data
          mountPath: {{ .Values.persistence.mountPath }}
          {{- if .Values.persistence.subPath }}
          subPath: {{ .Values.persistence.subPath }}
          {{- end }}
    {{- end }}

# Common Xray settings
common:
  ## Note that by default we use appVersion to get image tag
  # xrayVersion:

  # Spread Xray pods evenly across your nodes or some other topology
  topologySpreadConstraints: []
    # - maxSkew: 1
    #   topologyKey: kubernetes.io/hostname
    #   whenUnsatisfiable: DoNotSchedule
    #   labelSelector:
    #     matchLabels:
    #       app: '{{ template "xray.name" . }}'
    #       role: '{{ template "xray.name" . }}'
    #       release: "{{ .Release.Name }}"

  # Xray configuration to be written to xray_config.yaml
  xrayConfig:
    stdOutEnabled: true
    indexAllBuilds: false
    support-router: true

  # Use rabbitmq connection config from environment variables.
  # If false, then connection details should be set directly in system.yaml (systemYaml section).
  # When using external rabbitmq, set this to false
  rabbitmq:
    connectionConfigFromEnvironment: true
    waitForReplicasQuorumOnStartup: true

  ## Custom command to run before Xray startup. Runs BEFORE any microservice-specific preStartCommand
  preStartCommand:

  ## Add custom volumes
  # If .Values.xray.unifiedSecretInstallation is true then secret name should be '{{ template "xray.name" . }}-unified-secret'.
  customVolumes: |
  #  - name: custom-script
  #    configMap:
  #      name: custom-script

  ## Add custom volumesMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: /scripts/script.sh
  #    subPath: script.sh

  # Add any list of configmaps to Xray
  configMaps: |
  #  posthook-start.sh: |-
  #    echo "This is a post start script"
  #  posthook-end.sh: |-
  #    echo "This is a post end script"

  ## Add custom init containers execution before predefined init containers
  customInitContainersBegin: |
  #  - name: "custom-setup"
  #    image: {{ include "xray.getImageInfoByValue" (list . "initContainers") }}
  #    imagePullPolicy: "{{ .Values.initContainers.image.pullPolicy }}"
  #    {{- if .Values.containerSecurityContext.enabled }}
  #    securityContext: {{- tpl (omit .Values.containerSecurityContext "enabled" | toYaml) . | nindent 10 }}
  #    {{- end }}
  #    command:
  #      - 'sh'
  #      - '-c'
  #      - 'touch {{ .Values.xray.persistence.mountPath }}/example-custom-setup'
  #    volumeMounts:
  #      - mountPath: "{{ .Values.xray.persistence.mountPath }}"
  #        name: data-volume

  ## Add custom init containers execution after predefined init containers
  customInitContainers: |
  #  - name: "custom-systemyaml-setup"
  #    image: "{{ include "xray.getImageInfoByValue" (list . "initContainers") }}"
  #    imagePullPolicy: "{{ .Values.initContainers.image.pullPolicy }}"
  #    {{- if .Values.containerSecurityContext.enabled }}
  #    securityContext: {{- tpl (omit .Values.containerSecurityContext "enabled" | toYaml) . | nindent 10 }}
  #    {{- end }}
  #    command:
  #      - 'sh'
  #      - '-c'
  #      - 'wget -O {{ .Values.xray.persistence.mountPath }}/etc/system.yaml https://<repo-url>/systemyaml'
  #    volumeMounts:
  #      - mountPath: "{{ .Values.xray.persistence.mountPath }}"
  #        name: data-volume

  ## Add custom sidecar containers
  # - The provided example uses a custom volume (customVolumes)
  # - The provided example shows running container as root (id 0)
  customSidecarContainers: |
  #  - name: "sidecar-list-etc"
  #    image: {{ include "xray.getImageInfoByValue" (list . "initContainers") }}
  #    imagePullPolicy: {{ .Values.initContainers.image.pullPolicy }}
  #    {{- if .Values.containerSecurityContext.enabled }}
  #    securityContext: {{- tpl (omit .Values.containerSecurityContext "enabled" | toYaml) . | nindent 10 }}
  #    {{- end }}
  #    command:
  #      - 'sh'
  #      - '-c'
  #      - >
  #       while true; do echo "running in sidecar"; sleep 2; done
  #    volumeMounts:
  #      - mountPath: "{{ .Values.xray.persistence.mountPath }}"
  #        name: data-volume
  #    resources:
  #      requests:
  #        memory: "32Mi"
  #        cpu: "50m"
  #      limits:
  #        memory: "128Mi"
  #        cpu: "100m"

  # If .Values.xray.unifiedSecretInstallation is true then secret name should be '{{ template "xray.name" . }}-unified-secret'.
  customSecrets:
  #  - name: custom-secret
  #    key: custom-secret.yaml
  #    data: >
  #      custom_secret_config:
  #        parameter1: value1
  #        parameter2: value2
  #  - name: custom-secret2
  #    key: custom-secret2.config
  #    data: |
  #      here the custom secret 2 config

  persistence:
    enabled: true
    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    # existingClaim:

    accessMode: ReadWriteOnce
    ## Container storage limit if persistence.enabled: false
    ## Otherwise PVC size
    size: 50Gi
    ## server data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

  ## @param extraEnvVars Extra environment variables to add to xray containers
  ## E.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: BAR
  ##
  extraEnvVars:

analysis:
  name: xray-analysis
  ## Note that by default we use appVersion to get image tag/version
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/xray-analysis
    # tag:
  internalPort: 7000
  externalPort: 7000
  annotations: {}
  extraEnvVars:

  # Add lifecycle hooks for the analysis pod
  lifecycle: {}
    # postStart:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
    # preStop:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the preStart handler > /usr/share/message"]

  ## Add custom volumesMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: /scripts/script.sh
  #    subPath: script.sh

  livenessProbe:
    enabled: true
    config: |
      httpGet:
        path: /api/v1/system/liveness
        port: {{ .Values.analysis.internalPort }}
      initialDelaySeconds: {{ if semverCompare "<v1.20.0-0" .Capabilities.KubeVersion.Version }}90{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}
      failureThreshold: 3
      successThreshold: 1

  startupProbe:
    enabled: true
    config: |
      httpGet:
        path: /api/v1/system/readiness
        port: {{ .Values.analysis.internalPort }}
      initialDelaySeconds: {{ .Values.probes.xrayStartup.initialDelaySeconds }}
      failureThreshold: 30
      periodSeconds: 5
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}

  ## Custom command to run before Xray Analysis startup. Runs AFTER the common.preStartCommand
  preStartCommand:

  resources: {}
  #  requests:
  #    memory: "1Gi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "1"

sbom:
  enabled: false
  name: xray-sbom
  ## Note that by default we use appVersion to get image tag/version
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/xray-sbom
  internalPort: 7006
  externalPort: 7006
  annotations: {}
  extraEnvVars:

  # Add lifecycle hooks for the indexer pod
  lifecycle: {}
    # postStart:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
    # preStop:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the preStart handler > /usr/share/message"]

  ## Add custom volumesMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: /scripts/script.sh
  #    subPath: script.sh

  livenessProbe:
    enabled: true
    config: |
      exec:
        command:
          - sh
          - -c
          - curl -s -k --fail --max-time {{ .Values.probes.timeoutSeconds }} http://localhost:{{ .Values.sbom.internalPort }}/api/v1/system/liveness
      initialDelaySeconds: {{ if semverCompare "<v1.20.0-0" .Capabilities.KubeVersion.Version }}90{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}
      failureThreshold: 3
      successThreshold: 1

  startupProbe:
    enabled: true
    config: |
      exec:
        command:
          - sh
          - -c
          - curl -s -k --fail --max-time {{ .Values.probes.timeoutSeconds }} http://localhost:{{ .Values.sbom.internalPort }}/api/v1/system/readiness
      initialDelaySeconds: {{ .Values.probes.xrayStartup.initialDelaySeconds }}
      failureThreshold: 30
      periodSeconds: 5
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}

  ## Custom command to run before Xray Indexer startup. Runs AFTER the common.preStartCommand
  preStartCommand:

  resources: {}
  #  requests:
  #    memory: "1Gi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "1"

panoramic:
  enabled: false
  name: xray-panoramic
  ## Note that by default we use appVersion to get image tag/version
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/xray-panoramic
  internalPort: 7007
  externalPort: 7007
  annotations: {}
  extraEnvVars:

  # Add lifecycle hooks for the indexer pod
  lifecycle: {}
    # postStart:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
    # preStop:
    #   exec:
  #     command: ["/bin/sh", "-c", "echo Hello from the preStart handler > /usr/share/message"]

  ## Add custom volumesMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: /scripts/script.sh
  #    subPath: script.sh

  livenessProbe:
    enabled: true
    config: |
      exec:
        command:
          - sh
          - -c
          - curl -s -k --fail --max-time {{ .Values.probes.timeoutSeconds }} http://localhost:{{ .Values.panoramic.internalPort }}/api/v1/system/liveness
      initialDelaySeconds: {{ if semverCompare "<v1.20.0-0" .Capabilities.KubeVersion.Version }}90{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}
      failureThreshold: 3
      successThreshold: 1

  startupProbe:
    enabled: true
    config: |
      exec:
        command:
          - sh
          - -c
          - curl -s -k --fail --max-time {{ .Values.probes.timeoutSeconds }} http://localhost:{{ .Values.panoramic.internalPort }}/api/v1/system/readiness
      initialDelaySeconds: 30
      failureThreshold: 30
      periodSeconds: 5
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}
  ## Custom command to run before Xray Indexer startup. Runs AFTER the common.preStartCommand
  preStartCommand:

  resources: {}
  #  requests:
  #    memory: "1Gi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "1"

policyenforcer:
  name: xray-policyenforcer
  ## Note that by default we use appVersion to get image tag/version
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/xray-policyenforcer
  internalPort: 7009
  externalPort: 7009
  annotations: {}
  extraEnvVars:

  # Add lifecycle hooks for the indexer pod
  lifecycle: {}
    # postStart:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
    # preStop:
    #   exec:
  #     command: ["/bin/sh", "-c", "echo Hello from the preStart handler > /usr/share/message"]

  ## Add custom volumesMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: /scripts/script.sh
  #    subPath: script.sh

  livenessProbe:
    enabled: true
    config: |
      exec:
        command:
          - sh
          - -c
          - curl -s -k --fail --max-time {{ .Values.probes.timeoutSeconds }} http://localhost:{{ .Values.policyenforcer.internalPort }}/api/v1/system/liveness
      initialDelaySeconds: {{ if semverCompare "<v1.20.0-0" .Capabilities.KubeVersion.Version }}90{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}
      failureThreshold: 3
      successThreshold: 1

  startupProbe:
    enabled: true
    config: |
      exec:
        command:
          - sh
          - -c
          - curl -s -k --fail --max-time {{ .Values.probes.timeoutSeconds }} http://localhost:{{ .Values.policyenforcer.internalPort }}/api/v1/system/readiness
      initialDelaySeconds: 30
      failureThreshold: 30
      periodSeconds: 5
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}
  ## Custom command to run before Xray Indexer startup. Runs AFTER the common.preStartCommand
  preStartCommand:

  resources: {}
  #  requests:
  #    memory: "1Gi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "1" 


indexer:
  name: xray-indexer
  ## Note that by default we use appVersion to get image tag/version
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/xray-indexer
    # tag:
  internalPort: 7002
  externalPort: 7002
  annotations: {}
  extraEnvVars:

  # Add lifecycle hooks for the indexer pod
  lifecycle: {}
    # postStart:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
    # preStop:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the preStart handler > /usr/share/message"]

  ## Add custom volumesMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: /scripts/script.sh
  #    subPath: script.sh

  livenessProbe:
    enabled: true
    config: |
      httpGet:
        path: /api/v1/system/liveness
        port: {{ .Values.indexer.internalPort }}
      initialDelaySeconds: {{ if semverCompare "<v1.20.0-0" .Capabilities.KubeVersion.Version }}90{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}
      failureThreshold: 3
      successThreshold: 1

  startupProbe:
    enabled: true
    config: |
      httpGet:
        path: /api/v1/system/readiness
        port: {{ .Values.indexer.internalPort }}
      initialDelaySeconds: {{ .Values.probes.xrayStartup.initialDelaySeconds }}
      failureThreshold: 30
      periodSeconds: 5
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}

  ## Custom command to run before Xray Indexer startup. Runs AFTER the common.preStartCommand
  preStartCommand:

  resources: {}
  #  requests:
  #    memory: "1Gi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "1"

persist:
  name: xray-persist
  ## Note that by default we use appVersion to get image tag/version
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/xray-persist
    # tag:
  internalPort: 7003
  externalPort: 7003
  annotations: {}
  extraEnvVars:

  # Add lifecycle hooks for the persist pod
  lifecycle: {}
    # postStart:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
    # preStop:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the preStart handler > /usr/share/message"]

  ## Add custom volumesMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: /scripts/script.sh
  #    subPath: script.sh

  livenessProbe:
    enabled: true
    config: |
      httpGet:
        path: /api/v1/system/liveness
        port: {{ .Values.persist.internalPort }}
      initialDelaySeconds: {{ if semverCompare "<v1.20.0-0" .Capabilities.KubeVersion.Version }}90{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}
      failureThreshold: 3
      successThreshold: 1

  startupProbe:
    enabled: true
    config: |
      httpGet:
        path: /api/v1/system/readiness
        port: {{ .Values.persist.internalPort }}
      initialDelaySeconds: {{ .Values.probes.xrayStartup.initialDelaySeconds }}
      failureThreshold: 30
      periodSeconds: 5
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}

  ## Custom command to run before Xray Persist startup. Runs AFTER the common.preStartCommand
  preStartCommand:

  resources: {}
  #  requests:
  #    memory: "1Gi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "1"

server:
  name: xray-server
  ## Note that by default we use appVersion to get image tag/version
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/xray-server
    # tag:
  internalPort: 8000
  externalPort: 80
  annotations: {}
  extraEnvVars:
  # Add lifecycle hooks for the insight pods
  lifecycle: {}
    # postStart:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
    # preStop:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the preStart handler > /usr/share/message"]

  # mailServer: ""
  # indexAllBuilds: false

  ## Add custom volumesMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: /scripts/script.sh
  #    subPath: script.sh

  service:
    type: ClusterIP
    name: xray
    annotations: {}
    ## Provide additional spec to xray service
    ## Example:
    ## additionalSpec: |
    ##   customKey: customVal
    ##
    additionalSpec: |

  statefulset:
    annotations: {}

  livenessProbe:
    enabled: true
    config: |
      httpGet:
        path: /api/v1/system/liveness
        port: {{ .Values.server.internalPort }}
      initialDelaySeconds: {{ if semverCompare "<v1.20.0-0" .Capabilities.KubeVersion.Version }}90{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}
      failureThreshold: 3
      successThreshold: 1

  startupProbe:
    enabled: true
    config: |
      httpGet:
        path: /api/v1/system/readiness
        port: {{ .Values.server.internalPort }}
      initialDelaySeconds: {{ .Values.probes.xrayStartup.initialDelaySeconds }}
      failureThreshold: 30
      periodSeconds: 5
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}

  ## Custom command to run before Xray Server startup. Runs AFTER the common.preStartCommand
  preStartCommand:

  resources: {}
  #  requests:
  #    memory: "1Gi"
  #    cpu: "100m"
  #  limits:
  #    memory: "2Gi"
  #    cpu: "1"

## JAS periodic health check
jas:
  healthcheck:
    enabled: false
contextualAnalysis:
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/xray-jas-contextual-analysis

exposures:
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/xray-jas-exposures

router:
  name: router
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/router
    tag: 7.149.1
    imagePullPolicy: IfNotPresent
  serviceRegistry:
    ## Service registry (Access) TLS verification skipped if enabled.
    ## Set to true in case access cert is self signed and routertlsEnabled flag is true.
    insecure: false
  internalPort: 8082
  externalPort: 8082
  tlsEnabled: false
  resources: {}
  #  requests:
  #    memory: "100Mi"
  #    cpu: "100m"
  #  limits:
  #    memory: "1Gi"
  #    cpu: "1"
  extraEnvVars:
  # Add lifecycle hooks for the router pod
  lifecycle: {}
    # postStart:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
    # preStop:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the preStart handler > /usr/share/message"]

  annotations: {}

  ## Add custom volumesMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: /scripts/script.sh
  #    subPath: script.sh

  livenessProbe:
    enabled: true
    config: |
      httpGet:
        path: /router/api/v1/system/liveness
        port: {{ .Values.router.internalPort }}
        scheme: {{ include "xray.scheme" . | upper }}
      initialDelaySeconds: {{ if semverCompare "<v1.20.0-0" .Capabilities.KubeVersion.Version }}90{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}
      failureThreshold: 5
      successThreshold: 1

  readinessProbe:
    enabled: true
    config: | 
      httpGet:
        path: /router/api/v1/system/readiness
        port: {{ .Values.router.internalPort }}
        scheme: {{ include "xray.scheme" . | upper}}
      initialDelaySeconds: {{ if semverCompare "<v1.20.0-0" .Capabilities.KubeVersion.Version }}60{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}
      failureThreshold: 5
      successThreshold: 1

  startupProbe:
    enabled: true
    config: |
      httpGet:
        path: /router/api/v1/system/readiness
        port: {{ .Values.router.internalPort }}
        scheme: {{ include "xray.scheme" . | upper }}
      initialDelaySeconds: 30
      failureThreshold: 30
      periodSeconds: 5
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}

  persistence:
    mountPath: "/var/opt/jfrog/router"

  # Add any of the loggers to a sidecar if you want to be able to see them with kubectl logs or a log collector in your k8s cluster
  loggers: []

observability:
  name: observability
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/observability
    tag: 1.31.11
    imagePullPolicy: IfNotPresent
  internalPort: 8036
  resources: {}
  #  requests:
  #    memory: "100Mi"
  #    cpu: "100m"
  #  limits:
  #    memory: "1Gi"
  #    cpu: "1"

  # Add lifecycle hooks for the observability pod
  lifecycle: {}
    # postStart:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
    # preStop:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the preStart handler > /usr/share/message"]
  extraEnvVars:

  livenessProbe:
    enabled: true
    config: |
      httpGet:
        path: observability/api/v1/system/liveness
        port: 8082
        scheme: {{ include "xray.scheme" . | upper}}
      initialDelaySeconds: {{ if semverCompare "<v1.20.0-0" .Capabilities.KubeVersion.Version }}90{{ else }}0{{ end }}
      failureThreshold: 5
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}
      periodSeconds: 10
      successThreshold: 1

  startupProbe:
    enabled: true
    config: |
      httpGet:
        path: observability/api/v1/system/readiness
        port: 8082
        scheme: {{ include "xray.scheme" . | upper}}
      initialDelaySeconds: 30
      failureThreshold: 90
      periodSeconds: 5
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}

  persistence:
    mountPath: "/var/opt/jfrog/observability"

# Filebeat Sidecar container
## The provided filebeat configuration is for Xray logs. It assumes you have a logstash installed and configured properly.
filebeat:
  enabled: false
  name: xray-filebeat
  image:
    repository: "docker.elastic.co/beats/filebeat"
    version: 7.16.2
  logstashUrl: "logstash:5044"

  annotations: {}

  terminationGracePeriod: 10

  livenessProbe:
    exec:
      command:
        - sh
        - -c
        - |
          #!/usr/bin/env bash -e
          curl --fail 127.0.0.1:5066
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5

  readinessProbe:
    exec:
      command:
        - sh
        - -c
        - |
          #!/usr/bin/env bash -e
          filebeat test output
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5

  resources: {}
  #  requests:
  #    memory: "100Mi"
  #    cpu: "100m"
  #  limits:
  #    memory: "100Mi"
  #    cpu: "100m"

  filebeatYml: |
    logging.level: info
    path.data: {{ .Values.xray.persistence.mountPath }}/log/filebeat
    name: xray-filebeat
    queue.spool:
      file:
        permissions: 0760
    filebeat.inputs:
    - type: log
      enabled: true
      close_eof: ${CLOSE:false}
      paths:
         - {{ .Values.xray.persistence.mountPath }}/log/*.log
      fields:
        service: "jfxr"
        log_type: "xray"
    output:
      logstash:
         hosts: ["{{ .Values.filebeat.logstashUrl }}"]

## Allows to add additional kubernetes resources
## Use --- as a separator between multiple resources
## For an example, refer - https://github.com/jfrog/log-analytics-prometheus/blob/master/xray-values.yaml
additionalResources: |

# Adding entries to a Pod's /etc/hosts file
# For an example, refer - https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases
hostAliases: []
#  - ip: "127.0.0.1"
#    hostnames:
#      - "foo.local"
#      - "bar.local"
#  - ip: "10.1.2.3"
#    hostnames:
#      - "foo.remote"
#      - "bar.remote"

## Specify common probes parameters
probes:
  timeoutSeconds: 5
  xrayStartup:
    initialDelaySeconds: 30

## To limit the amount of jobs created by xray execution service
quota:
  enabled: true
  jobCount: 100

###################################################################################
## At present, this feature is not available for onprem installations.
## Separate Xray into distinct pods
## (Enabling this setting will divide the Xray pod into two deployments: xray-server and xray-ipa)
splitXraytoSeparateDeployments:
 enabled: false
 ## To prevent downtime (both the statefulset pod and deployment pod are kept together, with gradual upgrade set to false, which can turn off statefulsets in subsequent upgrades)
 gradualUpgrade: false
replicaCountServer: 2
## Apply horizontal pod auto scaling on Xray server pods
## Only applicable when (splitXraytoSeparateDeployments.enabled) is set to true
## Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
autoscalingServer:
 enabled: false
 minReplicas: 2
 maxReplicas: 3
 targetCPUUtilizationPercentage: 70
 targetMemoryUtilizationPercentage: 90
 ## Specify if using the keda hpa or regular basic hpa
 ## Note: keda should be installed on the target cluster
 ## Ref: https://keda.sh/docs/2.10/deploy/
 keda:
   enabled: false
   annotations: {}
   scaleUp:
     stabilizationWindowSeconds: 90
     policies:
       - type: Pods
         value: 3
         periodSeconds: 30
   scaleDown:
     stabilizationWindowSeconds: 90
     policies:
       - type: Pods
         value: 1
         periodSeconds: 30
   pollingInterval: 10
   cooldownPeriod: 10
   queues: []
## Apply horizontal pod auto scaling on Xray ipa pods
## Only applicable when (splitXraytoSeparateDeployments.enabled) is set to true
## Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
autoscalingIpa:
 enabled: false
 minReplicas: 1
 maxReplicas: 3
 targetCPUUtilizationPercentage: 70
 targetMemoryUtilizationPercentage: 90
 ## Specify if using the keda hpa or regular basic hpa
 ## Note: keda should be installed on the target cluster
 ## Ref: https://keda.sh/docs/2.10/deploy/
 keda:
   enabled: false
   annotations: {}
   scaleUp:
     stabilizationWindowSeconds: 90
     policies:
       - type: Pods
         value: 3
         periodSeconds: 30
   scaleDown:
     stabilizationWindowSeconds: 90
     policies:
       - type: Pods
         value: 1
         periodSeconds: 30
   pollingInterval: 10
   cooldownPeriod: 10
   queues:
     - name: analysis
       value: "100"
     - name: index
       value: "100"
     - name: persist
       value: "100"
     - name: impactAnalysis
       value: "100"
     - name: policyEnforcer
       value: "100"

ipa:
  affinity: {}
  ## Only used if "affinity" is empty
  podAntiAffinity:
    ## Valid values are "soft" or "hard"; any other value indicates no anti-affinity
    type: "soft"
    topologyKey: "kubernetes.io/hostname"
###################################################################################


# This will install catalog chart as a dependency chart in xray.
catalog:
  customInitContainers: |
    {{- if and (.Values.createCatalogDb.enabled) (not .Values.database.secrets.user) (not .Values.database.secrets.url) (not .Values.database.secrets.password) -}}
    {{- if contains (printf "%s-postgresql" .Release.Name) (tpl .Values.database.url .) -}}
    - name: "wait-for-catalog-database"
      image: {{ .Values.createCatalogDb.image }}
      imagePullPolicy: {{ .Values.initContainers.image.pullPolicy | quote }}
      {{- if .Values.containerSecurityContext.enabled }}
      securityContext:
        {{- toYaml (omit .Values.containerSecurityContext "enabled") | nindent 12 }}
      {{- end }}
      {{- if .Values.initContainers.resources }}
      resources: {{- toYaml .Values.initContainers.resources | nindent 12 }}
      {{- end }}
      command:
      - '/bin/bash'
      - '-c'
      - |
        echo "Waiting for PostgreSQL and catalogdb to become available"
        ready=false
        start_time=$(date +%s)
        while ! $ready; do 
            echo "Checking PostgreSQL connection and database availability..."
            psql "postgres://{{ .Values.database.user }}:{{ .Values.database.password }}@{{ .Release.Name }}-postgresql:5432/catalogdb?sslmode=disable" -c "\q" 2>/dev/null
            exit_status=$?
            if [[ $exit_status -eq 0 ]]; then
                ready=true
                echo "catalogdb database is available"
            else
                echo "Database not ready, retrying..."
                current_time=$(date +%s)
                elapsed_time=$(( current_time - start_time ))
                if [[ $elapsed_time -ge 600 ]]; then
                    echo "Timeout reached: PostgreSQL did not become available within 10 minutes."
                    exit 1
                fi
            fi
            sleep 1
        done
    {{- end -}}
    {{- end -}}
  enabled: false
  ## Catalog db creation in xray bundled postgresql. If set to true, then the catalog db will be created in xray bundled postgresql.
  createCatalogDb:
    enabled: true
    image: releases-docker.jfrog.io/postgres:15.6-alpine
  ## Change database connection details to external database. Bundled postgresql is not recomended for production use.
  ## When using bundled postgresql, provide the same credentials as used by xray. 
  ## Xray post upgrade hook will create the catalog db in the bundled postgresql.
  ## Catalog customInitContainers will wait for the database to be available before starting the catalog. (Only applicable for bundled postgresql)
  database:
    url: "postgres://{{ .Release.Name }}-postgresql:5432/catalogdb?sslmode=disable"
    user: xray
    password: ""