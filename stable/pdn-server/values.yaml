# Default values for pdn-server.
# This is a YAML-formatted file.
# Beware when changing values here. You should know what you are doing!
# Access the values with {{ .Values.key.subkey }}

global:
  # imageRegistry:
  # imagePullSecrets:
  #   - myRegistryKeySecretName
  ## Chart.AppVersion can be overidden using global.versions.pdnServer or image tags
  ## Note: Order of preference is 1) global.versions 2) image tags 3) Chart.AppVersion
  versions: {}
    # pdnServer:
    # router:
  # jfrogUrl:
  # joinKey:
  # masterKey:
  # joinKeySecretName:
  # masterKeySecretName:

  ## Note: tags customInitContainersBegin,customInitContainers,customVolumes,customVolumeMounts,customSidecarContainers can be used both from global and application level simultaneously
  # customVolumes: |

  # customVolumeMounts: |

  # customInitContainers: |

  # customInitContainersBegin: |

  # customSidecarContainers: |

  ## certificates added to this secret will be copied to $JFROG_HOME/pdnserver/var/etc/security/keys/trusted directory
  customCertificates:
    enabled: false
    # certificateSecretName:

  ## Applies to pdn server pods
  nodeSelector: {}

## String to partially override pdn-server.fullname template (will maintain the release name)
##
# nameOverride:

## String to fully override pdn-server.fullname template
##
# fullnameOverride:

initContainerImage: releases-docker.jfrog.io/jfrog/ubi-minimal:8.5-204

# Init containers
initContainers:
  resources:
    requests:
      memory: "50Mi"
      cpu: "10m"
    limits:
      memory: "1Gi"
      cpu: "1"

# For supporting pulling from private registries
imagePullSecrets:
  # - myRegistryKeySecretName

## PDN Server systemYaml override
## This is for advanced usecases where users wants to provide their own systemYaml for configuring pdn server
## Refer - https://www.jfrog.com/confluence/display/JFROG/PdnServer+System+YAML
## Note: This will override existing (default) .Values.pdnServer.systemYaml in values.yaml
## Alternatively, systemYaml can be overidden via customInitContainers using external sources like vaults, external repositories etc. Please refer customInitContainer section below for an example.
## Note: Order of preference is 1) customInitContainers 2) systemYamlOverride existingSecret 3) default systemYaml in values.yaml
systemYamlOverride:
## You can use a pre-existing secret by specifying existingSecret
  existingSecret:
## The dataKey should be the name of the secret data key created.
  dataKey:

# For HA
replicaCount: 1

## Role Based Access Control
## Ref: https://kubernetes.io/docs/admin/authorization/rbac/
rbac:
  create: false
  role:
    ## Rules to create. It follows the role specification
    rules:
    - apiGroups:
      - ''
      resources:
      - services
      - endpoints
      - pods
      verbs:
      - get
      - watch
      - list

networkpolicy: []
  # Allows all ingress and egress
  # - name: pdnServer
  #   podSelector:
  #     matchLabels:
  #       app: pdn-server
  #   egress:
  #     - {}
  #   ingress:
  #     - {}

## Service Account
## Ref: https://kubernetes.io/docs/admin/service-accounts-admin/
##
serviceAccount:
  create: false
  ## The name of the ServiceAccount to use.
  ## If not set and create is true, a name is generated using the fullname template
  name:
  ## Explicitly mounts the API credentials for the Service Account
  automountServiceAccountToken: false

pdnServer:
  name: pdn-server
  ## Note that by default we use appVersion to get image tag/version
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/pdn/server
    # tag:
    imagePullPolicy: IfNotPresent

  # Create a priority class for the pdnserver pod or use an existing one
  # NOTE - Maximum allowed value of a user defined priority is 1000000000
  priorityClass:
    create: false
    value: 1000000000
    ## Override default name
    # name:
    ## Use an existing priority class
    # existingPriorityClass:

  labels: {}

  ingress:
    enabled: false
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    path: /
    className: ""
    hosts:
      - chart-example.local

    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  port: 8095
  internalPort: 8094
  httpPort: 8092
  insecurePort: 8093
  ## Pdn server requires a unique master key
  ## You can generate one with the command:
  ## 'openssl rand -hex 32'
  ## Pass it to helm with '--set pdnServer.masterKey=${MASTER_KEY}'
  ## IMPORTANT: You should NOT use the example masterKey for a production deployment!
  masterKey: BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB
  ## Alternatively, you can use a pre-existing secret with a key called master-key by specifying masterKeySecretName
  # masterKeySecretName:

  joinKey: EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE
  ## Alternatively, you can use a pre-existing secret with a key called join-key by specifying joinKeySecretName
  # joinKeySecretName:

  ## certificates added to this secret will be copied to $JFROG_HOME/pdnserver/var/etc/security/keys/trusted directory
  customCertificates:
    enabled: false
    # certificateSecretName:

  ## If false, all service console logs will not redirect to a common console.log
  consoleLog: false
  ## Artifactory URL . Mandatory
  jfrogUrl:

  ## SelfAddress . Mandatory
  selfAddress: localhost:8095

  uid: 1055
  gid: 1055
  # fsGroupChangePolicy: "Always"

  ## Custom command to run before pdnserver startup. Runs BEFORE any microservice-specific preStartCommand
  preStartCommand:

  ## Add custom annotations for pipelines pods
  annotations: {}

  systemYaml: |
    router:
      serviceRegistry:
        insecure: {{ .Values.router.serviceRegistry.insecure }}
    shared:
      jfrogUrl: "{{ tpl (required "\n\npdnServer.jfrogUrl or global.jfrogUrl is required! This allows to connect to Artifactory.\nYou can copy the JFrog URL from Admin > Security > Settings" (include "pdn-server.jfrogUrl" .)) . }}"
    pdnServer:
      selfAddress: {{ .Values.pdnServer.selfAddress }}

  service:
    type: ClusterIP

  # Add custom secrets - secret per file
  customSecrets:
  #  - name: custom-secret
  #    key: custom-secret.yaml
  #    data: >
  #      custom_secret_config:
  #        parameter1: value1
  #        parameter2: value2
  #  - name: custom-secret2
  #    key: custom-secret2.config
  #    data: |
  #      here the custom secret 2 config

## Add custom volumes
  customVolumes: |
  #  - name: custom-script
  #    configMap:
  #      name: custom-script

    ## Add custom volumesMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: /scripts/script.sh
  #    subPath: script.sh

  # Add any list of configmaps to pdnserver
  configMaps: |
  #  posthook-start.sh: |-
  #    echo "This is a post start script"
  #  posthook-end.sh: |-
  #    echo "This is a post end script"

  ## Add custom init containers execution before predefined init containers
  customInitContainersBegin: |
  #  - name: "custom-setup"
  #    image: "{{ .Values.initContainerImage }}"
  #    imagePullPolicy: "{{ .Values.pdnServer.image.pullPolicy }}"
  #    securityContext:
  #      runAsNonRoot: true
  #      allowPrivilegeEscalation: false
  #      capabilities:
  #        drop:
  #          - NET_RAW
  #    command:
  #      - 'sh'
  #      - '-c'
  #      - 'touch {{ .Values.pdnServer.persistence.mountPath }}/example-custom-setup'
  #    volumeMounts:
  #      - mountPath: "{{ .Values.pdnServer.persistence.mountPath }}"
  #        name: pdnserver-data

  ## Add custom init containers execution after predefined init containers
  customInitContainers: |
  #  - name: "custom-systemyaml-setup"
  #    image: "{{ .Values.initContainerImage }}"
  #    imagePullPolicy: "{{ .Values.pdnServer.image.pullPolicy }}"
  #    securityContext:
  #      runAsNonRoot: true
  #      allowPrivilegeEscalation: false
  #      capabilities:
  #        drop:
  #          - NET_RAW
  #    command:
  #      - 'sh'
  #      - '-c'
  #      - 'curl -o {{ .Values.pdnServer.persistence.mountPath }}/etc/system.yaml https://<repo-url>/systemyaml'
  #    volumeMounts:
  #      - mountPath: "{{ .Values.pdnServer.persistence.mountPath }}"
  #        name: pdnserver-data

  ## Add custom sidecar containers
  # - The provided example uses a custom volume (customVolumes)
  customSidecarContainers: |
  #  - name: "sidecar-list-etc"
  #    image: "{{ .Values.initContainerImage }}"
  #    imagePullPolicy: "{{ .Values.imagePullPolicy }}"
  #    securityContext:
  #      runAsNonRoot: true
  #      allowPrivilegeEscalation: false
  #      capabilities:
  #        drop:
  #          - NET_RAW
  #    command:
  #      - 'sh'
  #      - '-c'
  #      - 'sh /scripts/script.sh'
  #    volumeMounts:
  #      - mountPath: "{{ .Values.pdnServer.persistence.mountPath }}"
  #        name: volume
  #      - mountPath: "/scripts/script.sh"
  #        name: custom-script
  #        subPath: script.sh
  #    resources:
  #      requests:
  #        memory: "32Mi"
  #        cpu: "50m"
  #      limits:
  #        memory: "128Mi"
  #        cpu: "100m"

  resources: {}
    # requests:
    #   memory: "2Gi"
    #   cpu: "1"
    # limits:
    #   memory: "4Gi"
    #   cpu: "2"

  livenessProbe:
    enabled: true
    config: |
      exec:
        command:
          - sh
          - -c
          - curl --fail -s --max-time {{ .Values.probes.timeoutSeconds }} http://localhost:{{ .Values.pdnServer.httpPort }}{{ include "pdn-server.livenessProbe" .}}
      initialDelaySeconds: {{ if semverCompare "<v1.20.0-0" .Capabilities.KubeVersion.Version }}90{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}
      failureThreshold: 5
      successThreshold: 1

  startupProbe:
    enabled: true
    config: |
      exec:
        command:
          - sh
          - -c
          - curl --fail -s --max-time {{ .Values.probes.timeoutSeconds }} http://localhost:{{ .Values.pdnServer.httpPort }}{{ include "pdn-server.readinessProbe" .}}
      initialDelaySeconds: 60
      failureThreshold: 60
      periodSeconds: 5
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}

  persistence:
    mountPath: "/var/opt/jfrog/pdnserver"

  # Add any of the loggers to a sidecar if you want to be able to see them with kubectl logs or a log collector in your k8s cluster
  loggers: []
  # - pdn-server-service.log
  # - pdn-server-request.log

  # Loggers containers resources
  loggersResources: {}
  #  requests:
  #    memory: "64Mi"
  #    cpu: "25m"
  #  limits:
  #    memory: "128Mi"
  #    cpu: "50m"

  nodeSelector: {}
  tolerations: []
  affinity: {}
  ## Only used if "affinity" is empty
  podAntiAffinity:
    ## Valid values are "soft" or "hard"; any other value indicates no anti-affinity
    type: "soft"
    topologyKey: "kubernetes.io/hostname"

router:
  name: router
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/router
    tag: 7.36.1
    imagePullPolicy: IfNotPresent
  serviceRegistry:
    ## Service registry (Access) TLS verification skipped if enabled
    insecure: false
  internalPort: 8082
  externalPort: 8082
  tlsEnabled: false
  resources: {}
  #  requests:
  #    memory: "100Mi"
  #    cpu: "100m"
  #  limits:
  #    memory: "1Gi"
  #    cpu: "1"

  ## Add custom volumesMounts
  customVolumeMounts: |
  #  - name: custom-script
  #    mountPath: /scripts/script.sh
  #    subPath: script.sh

  livenessProbe:
    enabled: true
    config: |
      exec:
        command:
          - sh
          - -c
          - curl -k -s --fail --max-time {{ .Values.probes.timeoutSeconds }} {{ include "pdn-server.scheme" . }}://localhost:{{ .Values.router.internalPort }}/router/api/v1/system/liveness
      initialDelaySeconds: {{ if semverCompare "<v1.20.0-0" .Capabilities.KubeVersion.Version }}90{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}
      failureThreshold: 5
      successThreshold: 1

  readinessProbe:
    enabled: true
    config: |
      exec:
        command:
          - sh
          - -c
          - curl -k -s --fail --max-time {{ .Values.probes.timeoutSeconds }} {{ include "pdn-server.scheme" . }}://localhost:{{ .Values.router.internalPort }}/router/api/v1/system/readiness
      initialDelaySeconds: {{ if semverCompare "<v1.20.0-0" .Capabilities.KubeVersion.Version }}60{{ else }}0{{ end }}
      periodSeconds: 10
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}
      failureThreshold: 5
      successThreshold: 1

  startupProbe:
    enabled: true
    config: |
      exec:
        command:
          - sh
          - -c
          - curl -k -s --fail --max-time {{ .Values.probes.timeoutSeconds }} {{ include "pdn-server.scheme" . }}://localhost:{{ .Values.router.internalPort }}/router/api/v1/system/readiness
      initialDelaySeconds: 30
      failureThreshold: 30
      periodSeconds: {{ .Values.probes.timeoutSeconds }}
      timeoutSeconds: 1

  persistence:
    accessMode: ReadWriteOnce
    mountPath: "/var/opt/jfrog/router"
    size: 5Gi

  # Add any of the loggers to a sidecar if you want to be able to see them with kubectl logs or a log collector in your k8s cluster
  loggers: []

  nodeSelector: {}
  tolerations: []
  affinity: {}

observability:
  name: observability
  image:
    registry: releases-docker.jfrog.io
    repository: jfrog/observability
    tag: 1.5.1
    imagePullPolicy: IfNotPresent
  internalPort: 8036
  resources: {}
  #  requests:
  #    memory: "100Mi"
  #    cpu: "100m"
  #  limits:
  #    memory: "1Gi"
  #    cpu: "1"

  livenessProbe:
    enabled: true
    config: |
      exec:
        command:
          - sh
          - -c
          - curl --fail -s --max-time {{ .Values.probes.timeoutSeconds }} http://localhost:{{ .Values.observability.internalPort }}/api/v1/system/liveness
      initialDelaySeconds: {{ if semverCompare "<v1.20.0-0" .Capabilities.KubeVersion.Version }}90{{ else }}0{{ end }}
      failureThreshold: 5
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}
      periodSeconds: 10
      successThreshold: 1

  startupProbe:
    enabled: true
    config: |
      exec:
        command:
          - sh
          - -c
          - curl --fail -s --max-time {{ .Values.probes.timeoutSeconds }} http://localhost:{{ .Values.observability.internalPort }}/api/v1/system/readiness
      initialDelaySeconds: 30
      failureThreshold: 90
      periodSeconds: 5
      timeoutSeconds: {{ .Values.probes.timeoutSeconds }}

  persistence:
    mountPath: "/var/opt/jfrog/observability"

# Filebeat Sidecar container
## The provided filebeat configuration is for Pdn server logs. It assumes you have a logstash installed and configured properly.
filebeat:
  enabled: false
  name: pdnserver-filebeat
  image:
    repository: docker.elastic.co/beats/filebeat
    version: 7.16.2
  logstashUrl: "logstash:5044"

  terminationGracePeriod: 10

  livenessProbe:
    exec:
      command:
        - sh
        - -c
        - |
          #!/usr/bin/env bash -e
          curl -s --fail 127.0.0.1:5066
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5

  readinessProbe:
    exec:
      command:
        - sh
        - -c
        - |
          #!/usr/bin/env bash -e
          filebeat test output
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5

  resources: {}
#    requests:
#      memory: "100Mi"
#      cpu: "100m"
#    limits:
#      memory: "100Mi"
#      cpu: "100m"

  filebeatYml: |
    logging.level: info
    path.data: {{ .Values.pdnServer.persistence.mountPath }}/log/filebeat
    name: pdnserver-filebeat
    queue.spool:
      file:
        permissions: 0760
    filebeat.inputs:
    - type: log
      enabled: true
      close_eof: ${CLOSE:false}
      paths:
         - {{ .Values.pdnServer.persistence.mountPath }}/log/*.log
      fields:
        service: "jftr"
        log_type: "pdnserver"
    output:
      logstash:
         hosts: ["{{ .Values.filebeat.logstashUrl }}"]

## Allows to add additional kubernetes resources
## Use --- as a separator between multiple resources
additionalResources: |

# Adding entries to a Pod's /etc/hosts file
# For an example, refer - https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases
hostAliases: []
#  - ip: "127.0.0.1"
#    hostnames:
#      - "foo.local"
#      - "bar.local"
#  - ip: "10.1.2.3"
#    hostnames:
#      - "foo.remote"
#      - "bar.remote"

## Specify common probes parameters
probes:
  timeoutSeconds: 5
